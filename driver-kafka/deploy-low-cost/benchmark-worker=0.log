13:47:14.665 [main] INFO log - Logging initialized @3731ms to org.eclipse.jetty.util.log.Slf4jLog
13:47:14.759 [main] INFO Server - jetty-9.4.42.v20210604; built: 2021-06-04T17:33:38.939Z; git: 5cd5e6d2375eeab146813b0de9f19eda6ab6e6cb; jvm 11.0.23+9-LTS
13:47:14.790 [main] INFO ContextHandler - Started o.e.j.s.ServletContextHandler@323e8306{/,null,AVAILABLE}
13:47:14.805 [main] INFO AbstractConnector - Started ServerConnector@733c423e{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
13:47:14.805 [main] INFO Server - Started @3875ms
13:47:14.807 [main] INFO PrometheusMetricsProvider - Started Prometheus stats endpoint at 0.0.0.0:8081
13:47:14.846 [main] INFO BenchmarkWorker - Starting benchmark with config: {
  "httpPort" : 8080,
  "statsPort" : 8081
}
13:47:14.891 [main] INFO Javalin - 
 _________________________________________
|        _                  _ _           |
|       | | __ ___   ____ _| (_)_ __      |
|    _  | |/ _` \ \ / / _` | | | '_ \     |
|   | |_| | (_| |\ V / (_| | | | | | |    |
|    \___/ \__,_| \_/ \__,_|_|_|_| |_|    |
|_________________________________________|
|                                         |
|    https://javalin.io/documentation     |
|_________________________________________|
13:47:14.894 [main] INFO Javalin - Starting Javalin ...
13:47:14.901 [main] INFO Server - jetty-9.4.42.v20210604; built: 2021-06-04T17:33:38.939Z; git: 5cd5e6d2375eeab146813b0de9f19eda6ab6e6cb; jvm 11.0.23+9-LTS
13:47:14.914 [main] INFO session - DefaultSessionIdManager workerName=node0
13:47:14.915 [main] INFO session - No SessionScavenger set, using defaults
13:47:14.916 [main] INFO session - node0 Scavenging every 600000ms
13:47:14.917 [main] INFO ContextHandler - Started i.j.e.j.@69b2f8e5{/,null,AVAILABLE}
13:47:14.918 [main] INFO ContextHandler - Started o.e.j.s.ServletContextHandler@a10c1b5{/,null,AVAILABLE}
13:47:14.919 [main] INFO AbstractConnector - Started ServerConnector@c3c4c1c{HTTP/1.1, (http/1.1)}{0.0.0.0:8080}
13:47:14.919 [main] INFO Server - Started @3989ms
13:47:14.919 [main] INFO EmbeddedServer - Jetty is listening on: [http://localhost:8080]
13:47:14.919 [main] INFO Javalin - Javalin has started \o/
13:47:14.990 [main] INFO InstanceWorkerStats - Instance worker stats initialized.
13:47:19.538 [main] INFO Benchmark - Using default worker file workers.yaml!
13:47:19.546 [main] INFO Benchmark - Reading workers list from workers.yaml
13:47:19.594 [main] INFO Benchmark - Starting benchmark with config: {
  "drivers" : [ "driver-kafka/kafka-experiment.yaml" ],
  "workers" : [ "http://10.0.0.175:8080", "http://10.0.0.217:8080", "http://10.0.0.152:8080" ],
  "workersFile" : "/opt/benchmark/workers.yaml",
  "tpcHFiles" : [ "workloads/tpc-h-q6-5000-300.yaml" ],
  "workloads" : [ "workloads/tpc-h-base-long.yaml" ],
  "output" : null
}
13:47:19.610 [main] INFO Benchmark - Workloads: {
  "tpc-h-base-long" : {
    "name" : "tpc-h",
    "topics" : 0,
    "partitionsPerTopic" : 1,
    "keyDistributor" : "NO_KEY",
    "messageSize" : 0,
    "useRandomizedPayloads" : false,
    "randomBytesRatio" : 0.0,
    "randomizedPayloadPoolSize" : 0,
    "payloadFile" : "",
    "subscriptionsPerTopic" : 1,
    "producersPerTopic" : 1,
    "consumerPerSubscription" : 1,
    "producerRate" : 10000000,
    "consumerBacklogSizeGB" : 0,
    "backlogDrainRatio" : 1.0,
    "testDurationMinutes" : 10,
    "warmupDurationMinutes" : 1
  }
}
13:47:19.620 [main] INFO Benchmark - TPC-H arguments: [ {
  "queryId" : "tpc-h-q6-5000-300",
  "query" : "ForecastingRevenueChange",
  "sourceDataS3FolderUri" : "s3://tpc-h-chunks/chunks-by-file-size/1400kb",
  "numberOfChunks" : 5000,
  "numberOfWorkers" : 300
} ]
13:47:19.660 [main] INFO InstanceWorkerStats - Instance worker stats initialized.
13:47:20.286 [main] INFO DistributedWorkersEnsemble - Workers list - producers: [http://10.0.0.175:8080,http://10.0.0.217:8080,http://10.0.0.152:8080]
13:47:20.287 [main] INFO DistributedWorkersEnsemble - Workers list - consumers: http://10.0.0.175:8080,http://10.0.0.217:8080,http://10.0.0.152:8080
13:47:20.290 [main] INFO Benchmark - --------------- WORKLOAD : tpc-h --- DRIVER : Kafka---------------
13:47:20.909 [qtp435803541-24] INFO LocalWorker - Driver: {
  "name" : "Kafka",
  "driverClass" : "io.openmessaging.benchmark.driver.kafka.KafkaBenchmarkDriver"
}
13:47:20.936 [qtp435803541-24] INFO AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 1200000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

13:47:20.991 [qtp435803541-24] INFO AppInfoParser - Kafka version: 3.6.1
13:47:20.991 [qtp435803541-24] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:20.991 [qtp435803541-24] INFO AppInfoParser - Kafka startTimeMs: 1717336040989
13:47:20.997 [main] INFO LocalWorker - Driver: {
  "name" : "Kafka",
  "driverClass" : "io.openmessaging.benchmark.driver.kafka.KafkaBenchmarkDriver"
}
13:47:21.030 [main] INFO AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 1200000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

13:47:21.105 [main] INFO AppInfoParser - Kafka version: 3.6.1
13:47:21.105 [main] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:21.106 [main] INFO AppInfoParser - Kafka startTimeMs: 1717336041103
13:47:21.123 [qtp435803541-29] INFO WorkerHandler - Received create topics request for topics: {
  "numberOfTopics" : 304,
  "numberOfPartitionsPerTopic" : 1
}
13:47:22.958 [qtp435803541-29] INFO LocalWorker - Created 304 topics in 1834.178517 ms
13:47:22.973 [main] INFO WorkloadGenerator - Created 304 topics in 1860.957006 ms
13:47:23.016 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-001-8cAUgF8-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-001-8cAUgF8
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.055 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.055 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.055 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043055
13:47:23.056 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-001-8cAUgF8-1, groupId=sub-001-8cAUgF8] Subscribed to topic(s): test-topic-0000001-8XkVdYY
13:47:23.058 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-004-qJr9NHU-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-004-qJr9NHU
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.068 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.068 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.068 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043068
13:47:23.069 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-004-qJr9NHU-2, groupId=sub-004-qJr9NHU] Subscribed to topic(s): test-topic-0000004-20ZHReU
13:47:23.069 [pool-6-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-001-8cAUgF8-1, groupId=sub-001-8cAUgF8] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.070 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-007-1XVEdnU-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-007-1XVEdnU
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.070 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-8cAUgF8-1, groupId=sub-001-8cAUgF8] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.072 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-8cAUgF8-1, groupId=sub-001-8cAUgF8] (Re-)joining group
13:47:23.076 [pool-7-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-004-qJr9NHU-2, groupId=sub-004-qJr9NHU] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.079 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.079 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.079 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043078
13:47:23.079 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-007-1XVEdnU-3, groupId=sub-007-1XVEdnU] Subscribed to topic(s): test-topic-0000007-0MzD9Ik
13:47:23.081 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-010-0wq8Jaw-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-010-0wq8Jaw
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.084 [pool-8-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-007-1XVEdnU-3, groupId=sub-007-1XVEdnU] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.085 [pool-8-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-1XVEdnU-3, groupId=sub-007-1XVEdnU] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.085 [pool-8-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-1XVEdnU-3, groupId=sub-007-1XVEdnU] (Re-)joining group
13:47:23.085 [pool-7-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-qJr9NHU-2, groupId=sub-004-qJr9NHU] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.087 [pool-7-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-qJr9NHU-2, groupId=sub-004-qJr9NHU] (Re-)joining group
13:47:23.087 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-8cAUgF8-1, groupId=sub-001-8cAUgF8] Request joining group due to: need to re-join with the given member-id: consumer-sub-001-8cAUgF8-1-869910e3-f648-4f81-b323-8ddf19638628
13:47:23.087 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-8cAUgF8-1, groupId=sub-001-8cAUgF8] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.088 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-8cAUgF8-1, groupId=sub-001-8cAUgF8] (Re-)joining group
13:47:23.089 [pool-8-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-1XVEdnU-3, groupId=sub-007-1XVEdnU] Request joining group due to: need to re-join with the given member-id: consumer-sub-007-1XVEdnU-3-e874697d-d29b-4559-bf2e-ab10ea1deac1
13:47:23.089 [pool-8-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-1XVEdnU-3, groupId=sub-007-1XVEdnU] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.089 [pool-8-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-1XVEdnU-3, groupId=sub-007-1XVEdnU] (Re-)joining group
13:47:23.090 [pool-7-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-qJr9NHU-2, groupId=sub-004-qJr9NHU] Request joining group due to: need to re-join with the given member-id: consumer-sub-004-qJr9NHU-2-10f24e90-59b2-4042-a108-8091b22f4022
13:47:23.090 [pool-7-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-qJr9NHU-2, groupId=sub-004-qJr9NHU] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.090 [pool-7-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-qJr9NHU-2, groupId=sub-004-qJr9NHU] (Re-)joining group
13:47:23.091 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.091 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.091 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043091
13:47:23.092 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-010-0wq8Jaw-4, groupId=sub-010-0wq8Jaw] Subscribed to topic(s): test-topic-0000010-NyjSQ9A
13:47:23.093 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-013-4EId-xE-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-013-4EId-xE
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.096 [pool-9-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-010-0wq8Jaw-4, groupId=sub-010-0wq8Jaw] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.097 [pool-9-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-0wq8Jaw-4, groupId=sub-010-0wq8Jaw] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.097 [pool-9-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-0wq8Jaw-4, groupId=sub-010-0wq8Jaw] (Re-)joining group
13:47:23.100 [pool-9-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-0wq8Jaw-4, groupId=sub-010-0wq8Jaw] Request joining group due to: need to re-join with the given member-id: consumer-sub-010-0wq8Jaw-4-1d286d60-eae3-434a-bf10-2a15be89d1ef
13:47:23.101 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.101 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.101 [pool-9-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-0wq8Jaw-4, groupId=sub-010-0wq8Jaw] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.101 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043100
13:47:23.101 [pool-9-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-0wq8Jaw-4, groupId=sub-010-0wq8Jaw] (Re-)joining group
13:47:23.101 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-013-4EId-xE-5, groupId=sub-013-4EId-xE] Subscribed to topic(s): test-topic-0000013-FjDWcwQ
13:47:23.102 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-016-IXOF7Yo-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-016-IXOF7Yo
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.105 [pool-10-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-013-4EId-xE-5, groupId=sub-013-4EId-xE] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.106 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-4EId-xE-5, groupId=sub-013-4EId-xE] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.107 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-4EId-xE-5, groupId=sub-013-4EId-xE] (Re-)joining group
13:47:23.110 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.110 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-4EId-xE-5, groupId=sub-013-4EId-xE] Request joining group due to: need to re-join with the given member-id: consumer-sub-013-4EId-xE-5-19c12c65-1824-4f5b-b0f7-87751d065275
13:47:23.110 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.110 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043110
13:47:23.110 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-4EId-xE-5, groupId=sub-013-4EId-xE] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.110 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-4EId-xE-5, groupId=sub-013-4EId-xE] (Re-)joining group
13:47:23.110 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-016-IXOF7Yo-6, groupId=sub-016-IXOF7Yo] Subscribed to topic(s): test-topic-0000016-sZJ3Jho
13:47:23.111 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-019-ka4JHhg-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-019-ka4JHhg
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.113 [pool-11-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-016-IXOF7Yo-6, groupId=sub-016-IXOF7Yo] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.114 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-IXOF7Yo-6, groupId=sub-016-IXOF7Yo] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.118 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.118 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.118 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043118
13:47:23.119 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-019-ka4JHhg-7, groupId=sub-019-ka4JHhg] Subscribed to topic(s): test-topic-0000019-KoBZyHg
13:47:23.120 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-IXOF7Yo-6, groupId=sub-016-IXOF7Yo] (Re-)joining group
13:47:23.120 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-022-c_9WhqM-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-022-c_9WhqM
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.122 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-IXOF7Yo-6, groupId=sub-016-IXOF7Yo] Request joining group due to: need to re-join with the given member-id: consumer-sub-016-IXOF7Yo-6-686f61ba-2433-44d6-8d7a-292102d0c9c6
13:47:23.122 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-IXOF7Yo-6, groupId=sub-016-IXOF7Yo] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.122 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-IXOF7Yo-6, groupId=sub-016-IXOF7Yo] (Re-)joining group
13:47:23.126 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.127 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.127 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043126
13:47:23.127 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-022-c_9WhqM-8, groupId=sub-022-c_9WhqM] Subscribed to topic(s): test-topic-0000022-SNsKA5U
13:47:23.128 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-025-KJTUWl8-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-025-KJTUWl8
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.132 [pool-12-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-019-ka4JHhg-7, groupId=sub-019-ka4JHhg] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.132 [pool-13-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-022-c_9WhqM-8, groupId=sub-022-c_9WhqM] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.132 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-ka4JHhg-7, groupId=sub-019-ka4JHhg] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.132 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-c_9WhqM-8, groupId=sub-022-c_9WhqM] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.133 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-ka4JHhg-7, groupId=sub-019-ka4JHhg] (Re-)joining group
13:47:23.133 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-c_9WhqM-8, groupId=sub-022-c_9WhqM] (Re-)joining group
13:47:23.135 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.135 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.135 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043135
13:47:23.136 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-025-KJTUWl8-9, groupId=sub-025-KJTUWl8] Subscribed to topic(s): test-topic-0000025-6uZz340
13:47:23.136 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-ka4JHhg-7, groupId=sub-019-ka4JHhg] Request joining group due to: need to re-join with the given member-id: consumer-sub-019-ka4JHhg-7-08485441-8eb0-490f-b320-476da927fe70
13:47:23.136 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-ka4JHhg-7, groupId=sub-019-ka4JHhg] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.136 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-ka4JHhg-7, groupId=sub-019-ka4JHhg] (Re-)joining group
13:47:23.136 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-c_9WhqM-8, groupId=sub-022-c_9WhqM] Request joining group due to: need to re-join with the given member-id: consumer-sub-022-c_9WhqM-8-0cfedd2c-5e4b-439c-89f5-c530f1fd75b0
13:47:23.137 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-c_9WhqM-8, groupId=sub-022-c_9WhqM] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.137 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-c_9WhqM-8, groupId=sub-022-c_9WhqM] (Re-)joining group
13:47:23.137 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-028-b0xyLAc-10
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-028-b0xyLAc
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.142 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.142 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.142 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043142
13:47:23.142 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-028-b0xyLAc-10, groupId=sub-028-b0xyLAc] Subscribed to topic(s): test-topic-0000028-3nLVSQs
13:47:23.143 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-031-f2tqkFU-11
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-031-f2tqkFU
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.144 [pool-14-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-025-KJTUWl8-9, groupId=sub-025-KJTUWl8] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.144 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-KJTUWl8-9, groupId=sub-025-KJTUWl8] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.145 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-KJTUWl8-9, groupId=sub-025-KJTUWl8] (Re-)joining group
13:47:23.146 [pool-15-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-028-b0xyLAc-10, groupId=sub-028-b0xyLAc] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.147 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-b0xyLAc-10, groupId=sub-028-b0xyLAc] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.147 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-b0xyLAc-10, groupId=sub-028-b0xyLAc] (Re-)joining group
13:47:23.148 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-KJTUWl8-9, groupId=sub-025-KJTUWl8] Request joining group due to: need to re-join with the given member-id: consumer-sub-025-KJTUWl8-9-f2738c01-4377-4b85-95e4-2f04cae6ec84
13:47:23.148 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-KJTUWl8-9, groupId=sub-025-KJTUWl8] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.149 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-KJTUWl8-9, groupId=sub-025-KJTUWl8] (Re-)joining group
13:47:23.149 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.149 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.149 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043149
13:47:23.150 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-031-f2tqkFU-11, groupId=sub-031-f2tqkFU] Subscribed to topic(s): test-topic-0000031-DjUOkAw
13:47:23.150 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-b0xyLAc-10, groupId=sub-028-b0xyLAc] Request joining group due to: need to re-join with the given member-id: consumer-sub-028-b0xyLAc-10-007bb323-4fcf-47ef-b092-03d27bf3667f
13:47:23.150 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-034-e9D4N40-12
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-034-e9D4N40
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.151 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-b0xyLAc-10, groupId=sub-028-b0xyLAc] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.151 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-b0xyLAc-10, groupId=sub-028-b0xyLAc] (Re-)joining group
13:47:23.153 [pool-16-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-031-f2tqkFU-11, groupId=sub-031-f2tqkFU] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.154 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-f2tqkFU-11, groupId=sub-031-f2tqkFU] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.154 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-f2tqkFU-11, groupId=sub-031-f2tqkFU] (Re-)joining group
13:47:23.155 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.155 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.155 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043155
13:47:23.156 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-034-e9D4N40-12, groupId=sub-034-e9D4N40] Subscribed to topic(s): test-topic-0000034-P1Rw2WM
13:47:23.156 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-037-KMpaHu8-13
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-037-KMpaHu8
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.157 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-f2tqkFU-11, groupId=sub-031-f2tqkFU] Request joining group due to: need to re-join with the given member-id: consumer-sub-031-f2tqkFU-11-dac8594c-a57b-4152-9ce6-7c7e5411443d
13:47:23.157 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-f2tqkFU-11, groupId=sub-031-f2tqkFU] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.157 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-f2tqkFU-11, groupId=sub-031-f2tqkFU] (Re-)joining group
13:47:23.160 [pool-17-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-034-e9D4N40-12, groupId=sub-034-e9D4N40] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.160 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-e9D4N40-12, groupId=sub-034-e9D4N40] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.161 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.161 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.162 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043161
13:47:23.162 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-037-KMpaHu8-13, groupId=sub-037-KMpaHu8] Subscribed to topic(s): test-topic-0000037-2r-FtRk
13:47:23.163 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-e9D4N40-12, groupId=sub-034-e9D4N40] (Re-)joining group
13:47:23.165 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-e9D4N40-12, groupId=sub-034-e9D4N40] Request joining group due to: need to re-join with the given member-id: consumer-sub-034-e9D4N40-12-42255493-6dae-40d8-800a-0abeea6f68c3
13:47:23.166 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-e9D4N40-12, groupId=sub-034-e9D4N40] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.166 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-e9D4N40-12, groupId=sub-034-e9D4N40] (Re-)joining group
13:47:23.166 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-040--rMXIGQ-14
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-040--rMXIGQ
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.168 [pool-18-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-037-KMpaHu8-13, groupId=sub-037-KMpaHu8] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.169 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-KMpaHu8-13, groupId=sub-037-KMpaHu8] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.169 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.169 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.169 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043169
13:47:23.169 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-KMpaHu8-13, groupId=sub-037-KMpaHu8] (Re-)joining group
13:47:23.170 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-040--rMXIGQ-14, groupId=sub-040--rMXIGQ] Subscribed to topic(s): test-topic-0000040-ctRrJ_E
13:47:23.171 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-043-ydmoIdM-15
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-043-ydmoIdM
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.172 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-KMpaHu8-13, groupId=sub-037-KMpaHu8] Request joining group due to: need to re-join with the given member-id: consumer-sub-037-KMpaHu8-13-8467a1ea-0a48-4993-8497-f15521f1a240
13:47:23.172 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-KMpaHu8-13, groupId=sub-037-KMpaHu8] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.172 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-KMpaHu8-13, groupId=sub-037-KMpaHu8] (Re-)joining group
13:47:23.173 [pool-19-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-040--rMXIGQ-14, groupId=sub-040--rMXIGQ] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.173 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040--rMXIGQ-14, groupId=sub-040--rMXIGQ] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.174 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.174 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.174 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043174
13:47:23.174 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040--rMXIGQ-14, groupId=sub-040--rMXIGQ] (Re-)joining group
13:47:23.174 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-043-ydmoIdM-15, groupId=sub-043-ydmoIdM] Subscribed to topic(s): test-topic-0000043-CI9z7bU
13:47:23.176 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-046-y_Q8I9c-16
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-046-y_Q8I9c
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.176 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040--rMXIGQ-14, groupId=sub-040--rMXIGQ] Request joining group due to: need to re-join with the given member-id: consumer-sub-040--rMXIGQ-14-85be1785-b452-4f52-93e8-61d0ecb6f84b
13:47:23.177 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040--rMXIGQ-14, groupId=sub-040--rMXIGQ] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.177 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040--rMXIGQ-14, groupId=sub-040--rMXIGQ] (Re-)joining group
13:47:23.178 [pool-20-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-043-ydmoIdM-15, groupId=sub-043-ydmoIdM] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.179 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-ydmoIdM-15, groupId=sub-043-ydmoIdM] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.180 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-ydmoIdM-15, groupId=sub-043-ydmoIdM] (Re-)joining group
13:47:23.182 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.182 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-ydmoIdM-15, groupId=sub-043-ydmoIdM] Request joining group due to: need to re-join with the given member-id: consumer-sub-043-ydmoIdM-15-d1883fd7-194e-4cd3-a303-c4c2f26c1e3d
13:47:23.182 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.182 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043182
13:47:23.182 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-ydmoIdM-15, groupId=sub-043-ydmoIdM] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.183 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-ydmoIdM-15, groupId=sub-043-ydmoIdM] (Re-)joining group
13:47:23.183 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-046-y_Q8I9c-16, groupId=sub-046-y_Q8I9c] Subscribed to topic(s): test-topic-0000046-P0FMkh4
13:47:23.184 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-049-tL_8fMg-17
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-049-tL_8fMg
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.186 [pool-21-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-046-y_Q8I9c-16, groupId=sub-046-y_Q8I9c] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.186 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-y_Q8I9c-16, groupId=sub-046-y_Q8I9c] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.188 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.188 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.188 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043188
13:47:23.188 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-049-tL_8fMg-17, groupId=sub-049-tL_8fMg] Subscribed to topic(s): test-topic-0000049-9ea7a3Q
13:47:23.189 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-y_Q8I9c-16, groupId=sub-046-y_Q8I9c] (Re-)joining group
13:47:23.190 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-052-__A3iWA-18
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-052-__A3iWA
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.191 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-y_Q8I9c-16, groupId=sub-046-y_Q8I9c] Request joining group due to: need to re-join with the given member-id: consumer-sub-046-y_Q8I9c-16-e00d26a5-5f3d-4533-8d8a-b7b781258498
13:47:23.191 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-y_Q8I9c-16, groupId=sub-046-y_Q8I9c] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.192 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-y_Q8I9c-16, groupId=sub-046-y_Q8I9c] (Re-)joining group
13:47:23.192 [pool-22-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-049-tL_8fMg-17, groupId=sub-049-tL_8fMg] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.192 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-tL_8fMg-17, groupId=sub-049-tL_8fMg] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.192 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-tL_8fMg-17, groupId=sub-049-tL_8fMg] (Re-)joining group
13:47:23.194 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.194 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.194 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043194
13:47:23.194 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-052-__A3iWA-18, groupId=sub-052-__A3iWA] Subscribed to topic(s): test-topic-0000052-AfpVojk
13:47:23.195 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-tL_8fMg-17, groupId=sub-049-tL_8fMg] Request joining group due to: need to re-join with the given member-id: consumer-sub-049-tL_8fMg-17-e365320a-1af4-406c-8052-0b043620df87
13:47:23.195 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-055-yjFnWF4-19
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-055-yjFnWF4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.195 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-tL_8fMg-17, groupId=sub-049-tL_8fMg] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.195 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-tL_8fMg-17, groupId=sub-049-tL_8fMg] (Re-)joining group
13:47:23.198 [pool-23-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-052-__A3iWA-18, groupId=sub-052-__A3iWA] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.198 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-__A3iWA-18, groupId=sub-052-__A3iWA] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.199 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.199 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-__A3iWA-18, groupId=sub-052-__A3iWA] (Re-)joining group
13:47:23.199 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.199 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043199
13:47:23.199 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-055-yjFnWF4-19, groupId=sub-055-yjFnWF4] Subscribed to topic(s): test-topic-0000055-C3trC3o
13:47:23.201 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-058-2rRMNfE-20
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-058-2rRMNfE
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.201 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-__A3iWA-18, groupId=sub-052-__A3iWA] Request joining group due to: need to re-join with the given member-id: consumer-sub-052-__A3iWA-18-023a3ae4-84f7-4464-b66f-54b25253c60d
13:47:23.201 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-__A3iWA-18, groupId=sub-052-__A3iWA] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.202 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-__A3iWA-18, groupId=sub-052-__A3iWA] (Re-)joining group
13:47:23.204 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.204 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.205 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043204
13:47:23.205 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-058-2rRMNfE-20, groupId=sub-058-2rRMNfE] Subscribed to topic(s): test-topic-0000058-ru5jUZw
13:47:23.205 [pool-24-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-055-yjFnWF4-19, groupId=sub-055-yjFnWF4] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.206 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-055-yjFnWF4-19, groupId=sub-055-yjFnWF4] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.206 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-061-krTjLto-21
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-061-krTjLto
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.207 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-055-yjFnWF4-19, groupId=sub-055-yjFnWF4] (Re-)joining group
13:47:23.209 [pool-25-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-058-2rRMNfE-20, groupId=sub-058-2rRMNfE] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.209 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-058-2rRMNfE-20, groupId=sub-058-2rRMNfE] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.209 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.209 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-055-yjFnWF4-19, groupId=sub-055-yjFnWF4] Request joining group due to: need to re-join with the given member-id: consumer-sub-055-yjFnWF4-19-824ba906-339a-4328-8e5e-b3f9c2ddcf0b
13:47:23.209 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.210 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043209
13:47:23.210 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-055-yjFnWF4-19, groupId=sub-055-yjFnWF4] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.210 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-061-krTjLto-21, groupId=sub-061-krTjLto] Subscribed to topic(s): test-topic-0000061-btSl6bw
13:47:23.210 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-055-yjFnWF4-19, groupId=sub-055-yjFnWF4] (Re-)joining group
13:47:23.210 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-058-2rRMNfE-20, groupId=sub-058-2rRMNfE] (Re-)joining group
13:47:23.212 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-064-DKhien4-22
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-064-DKhien4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.213 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-058-2rRMNfE-20, groupId=sub-058-2rRMNfE] Request joining group due to: need to re-join with the given member-id: consumer-sub-058-2rRMNfE-20-bb065645-f6e7-450e-945e-2f362ae23030
13:47:23.213 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-058-2rRMNfE-20, groupId=sub-058-2rRMNfE] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.213 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-058-2rRMNfE-20, groupId=sub-058-2rRMNfE] (Re-)joining group
13:47:23.214 [pool-26-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-061-krTjLto-21, groupId=sub-061-krTjLto] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.215 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-061-krTjLto-21, groupId=sub-061-krTjLto] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.215 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-061-krTjLto-21, groupId=sub-061-krTjLto] (Re-)joining group
13:47:23.217 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-061-krTjLto-21, groupId=sub-061-krTjLto] Request joining group due to: need to re-join with the given member-id: consumer-sub-061-krTjLto-21-0d680477-53cd-4d29-93d5-fd962ebd8fee
13:47:23.218 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-061-krTjLto-21, groupId=sub-061-krTjLto] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.218 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-061-krTjLto-21, groupId=sub-061-krTjLto] (Re-)joining group
13:47:23.221 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.221 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.221 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043221
13:47:23.222 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-064-DKhien4-22, groupId=sub-064-DKhien4] Subscribed to topic(s): test-topic-0000064-JLHPfAM
13:47:23.223 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-067-lT7d7GQ-23
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-067-lT7d7GQ
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.225 [pool-27-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-064-DKhien4-22, groupId=sub-064-DKhien4] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.225 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-064-DKhien4-22, groupId=sub-064-DKhien4] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.226 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.226 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.226 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043226
13:47:23.227 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-067-lT7d7GQ-23, groupId=sub-067-lT7d7GQ] Subscribed to topic(s): test-topic-0000067-6rhS1nY
13:47:23.228 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-064-DKhien4-22, groupId=sub-064-DKhien4] (Re-)joining group
13:47:23.230 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-064-DKhien4-22, groupId=sub-064-DKhien4] Request joining group due to: need to re-join with the given member-id: consumer-sub-064-DKhien4-22-d9f9d40c-393f-49f6-b1a1-93b15d1ace31
13:47:23.230 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-064-DKhien4-22, groupId=sub-064-DKhien4] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.230 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-064-DKhien4-22, groupId=sub-064-DKhien4] (Re-)joining group
13:47:23.232 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-070-MXbUqc8-24
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-070-MXbUqc8
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.234 [pool-28-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-067-lT7d7GQ-23, groupId=sub-067-lT7d7GQ] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.235 [pool-28-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-067-lT7d7GQ-23, groupId=sub-067-lT7d7GQ] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.236 [pool-28-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-067-lT7d7GQ-23, groupId=sub-067-lT7d7GQ] (Re-)joining group
13:47:23.236 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.236 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.236 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043236
13:47:23.236 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-070-MXbUqc8-24, groupId=sub-070-MXbUqc8] Subscribed to topic(s): test-topic-0000070-toSKqSk
13:47:23.237 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-073-_TxsaHQ-25
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-073-_TxsaHQ
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.238 [pool-28-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-067-lT7d7GQ-23, groupId=sub-067-lT7d7GQ] Request joining group due to: need to re-join with the given member-id: consumer-sub-067-lT7d7GQ-23-93ecb203-40f4-4810-9080-5863470e37c6
13:47:23.238 [pool-28-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-067-lT7d7GQ-23, groupId=sub-067-lT7d7GQ] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.239 [pool-28-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-067-lT7d7GQ-23, groupId=sub-067-lT7d7GQ] (Re-)joining group
13:47:23.241 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.241 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.241 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043241
13:47:23.241 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-073-_TxsaHQ-25, groupId=sub-073-_TxsaHQ] Subscribed to topic(s): test-topic-0000073-HZ6O7Og
13:47:23.244 [pool-29-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-070-MXbUqc8-24, groupId=sub-070-MXbUqc8] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.244 [pool-29-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-070-MXbUqc8-24, groupId=sub-070-MXbUqc8] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.245 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-076-h05AUYU-26
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-076-h05AUYU
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.246 [pool-29-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-070-MXbUqc8-24, groupId=sub-070-MXbUqc8] (Re-)joining group
13:47:23.249 [pool-29-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-070-MXbUqc8-24, groupId=sub-070-MXbUqc8] Request joining group due to: need to re-join with the given member-id: consumer-sub-070-MXbUqc8-24-8c2e0162-ed88-487b-b7b5-3a775056d576
13:47:23.249 [pool-29-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-070-MXbUqc8-24, groupId=sub-070-MXbUqc8] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.249 [pool-29-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-070-MXbUqc8-24, groupId=sub-070-MXbUqc8] (Re-)joining group
13:47:23.250 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.250 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.250 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043248
13:47:23.250 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-076-h05AUYU-26, groupId=sub-076-h05AUYU] Subscribed to topic(s): test-topic-0000076-9xcr8KU
13:47:23.251 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-079-bjUU63o-27
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-079-bjUU63o
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.252 [pool-30-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-073-_TxsaHQ-25, groupId=sub-073-_TxsaHQ] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.252 [pool-30-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-073-_TxsaHQ-25, groupId=sub-073-_TxsaHQ] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.253 [pool-30-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-073-_TxsaHQ-25, groupId=sub-073-_TxsaHQ] (Re-)joining group
13:47:23.253 [pool-31-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-076-h05AUYU-26, groupId=sub-076-h05AUYU] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.253 [pool-31-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-076-h05AUYU-26, groupId=sub-076-h05AUYU] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.258 [pool-30-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-073-_TxsaHQ-25, groupId=sub-073-_TxsaHQ] Request joining group due to: need to re-join with the given member-id: consumer-sub-073-_TxsaHQ-25-0a2b51ad-32bb-47dd-895a-a6fe22a579ae
13:47:23.258 [pool-30-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-073-_TxsaHQ-25, groupId=sub-073-_TxsaHQ] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.258 [pool-30-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-073-_TxsaHQ-25, groupId=sub-073-_TxsaHQ] (Re-)joining group
13:47:23.259 [pool-31-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-076-h05AUYU-26, groupId=sub-076-h05AUYU] (Re-)joining group
13:47:23.259 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.259 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.259 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043259
13:47:23.260 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-079-bjUU63o-27, groupId=sub-079-bjUU63o] Subscribed to topic(s): test-topic-0000079-4XRk71U
13:47:23.261 [pool-31-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-076-h05AUYU-26, groupId=sub-076-h05AUYU] Request joining group due to: need to re-join with the given member-id: consumer-sub-076-h05AUYU-26-cceb5020-b8ee-4fdc-b543-5137506aa175
13:47:23.261 [pool-31-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-076-h05AUYU-26, groupId=sub-076-h05AUYU] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.261 [pool-31-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-076-h05AUYU-26, groupId=sub-076-h05AUYU] (Re-)joining group
13:47:23.267 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-082-7tZMVGo-28
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-082-7tZMVGo
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.269 [pool-32-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-079-bjUU63o-27, groupId=sub-079-bjUU63o] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.270 [pool-32-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-079-bjUU63o-27, groupId=sub-079-bjUU63o] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.271 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.271 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.271 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043271
13:47:23.272 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-082-7tZMVGo-28, groupId=sub-082-7tZMVGo] Subscribed to topic(s): test-topic-0000082-x9QjlwQ
13:47:23.274 [pool-32-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-079-bjUU63o-27, groupId=sub-079-bjUU63o] (Re-)joining group
13:47:23.275 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-085-JIGyKs0-29
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-085-JIGyKs0
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.277 [pool-32-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-079-bjUU63o-27, groupId=sub-079-bjUU63o] Request joining group due to: need to re-join with the given member-id: consumer-sub-079-bjUU63o-27-f227535e-77b2-423e-a588-01305190c02c
13:47:23.277 [pool-32-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-079-bjUU63o-27, groupId=sub-079-bjUU63o] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.277 [pool-32-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-079-bjUU63o-27, groupId=sub-079-bjUU63o] (Re-)joining group
13:47:23.277 [pool-33-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-082-7tZMVGo-28, groupId=sub-082-7tZMVGo] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.278 [pool-33-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-082-7tZMVGo-28, groupId=sub-082-7tZMVGo] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.278 [pool-33-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-082-7tZMVGo-28, groupId=sub-082-7tZMVGo] (Re-)joining group
13:47:23.279 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.279 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.279 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043279
13:47:23.279 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-085-JIGyKs0-29, groupId=sub-085-JIGyKs0] Subscribed to topic(s): test-topic-0000085-_VhFHj0
13:47:23.281 [pool-33-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-082-7tZMVGo-28, groupId=sub-082-7tZMVGo] Request joining group due to: need to re-join with the given member-id: consumer-sub-082-7tZMVGo-28-5272bd4b-f9b5-4e98-9790-db7d4994ba62
13:47:23.281 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-088-Jm7oJ_0-30
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-088-Jm7oJ_0
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.281 [pool-33-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-082-7tZMVGo-28, groupId=sub-082-7tZMVGo] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.281 [pool-33-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-082-7tZMVGo-28, groupId=sub-082-7tZMVGo] (Re-)joining group
13:47:23.283 [pool-34-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-085-JIGyKs0-29, groupId=sub-085-JIGyKs0] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.283 [pool-34-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-085-JIGyKs0-29, groupId=sub-085-JIGyKs0] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.284 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.284 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.284 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043284
13:47:23.284 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-088-Jm7oJ_0-30, groupId=sub-088-Jm7oJ_0] Subscribed to topic(s): test-topic-0000088-m_LR2ic
13:47:23.284 [pool-34-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-085-JIGyKs0-29, groupId=sub-085-JIGyKs0] (Re-)joining group
13:47:23.286 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-091-Tt1W9dc-31
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-091-Tt1W9dc
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.287 [pool-34-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-085-JIGyKs0-29, groupId=sub-085-JIGyKs0] Request joining group due to: need to re-join with the given member-id: consumer-sub-085-JIGyKs0-29-7ba0f02c-c67c-4106-bb04-8bb77f11904a
13:47:23.287 [pool-34-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-085-JIGyKs0-29, groupId=sub-085-JIGyKs0] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.287 [pool-34-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-085-JIGyKs0-29, groupId=sub-085-JIGyKs0] (Re-)joining group
13:47:23.289 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.289 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.289 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043289
13:47:23.289 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-091-Tt1W9dc-31, groupId=sub-091-Tt1W9dc] Subscribed to topic(s): test-topic-0000091-DPFMLNo
13:47:23.292 [pool-35-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-088-Jm7oJ_0-30, groupId=sub-088-Jm7oJ_0] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.293 [pool-35-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-088-Jm7oJ_0-30, groupId=sub-088-Jm7oJ_0] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.294 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-094-RJlVLD4-32
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-094-RJlVLD4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.297 [pool-35-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-088-Jm7oJ_0-30, groupId=sub-088-Jm7oJ_0] (Re-)joining group
13:47:23.297 [pool-36-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-091-Tt1W9dc-31, groupId=sub-091-Tt1W9dc] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.297 [pool-36-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-091-Tt1W9dc-31, groupId=sub-091-Tt1W9dc] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.298 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.298 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.298 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043297
13:47:23.298 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-094-RJlVLD4-32, groupId=sub-094-RJlVLD4] Subscribed to topic(s): test-topic-0000094-EbfdF94
13:47:23.298 [pool-36-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-091-Tt1W9dc-31, groupId=sub-091-Tt1W9dc] (Re-)joining group
13:47:23.299 [pool-35-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-088-Jm7oJ_0-30, groupId=sub-088-Jm7oJ_0] Request joining group due to: need to re-join with the given member-id: consumer-sub-088-Jm7oJ_0-30-b6bdc197-f24c-4e21-9622-cccee8a49c30
13:47:23.299 [pool-35-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-088-Jm7oJ_0-30, groupId=sub-088-Jm7oJ_0] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.299 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-097-qcwhv34-33
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-097-qcwhv34
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.299 [pool-35-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-088-Jm7oJ_0-30, groupId=sub-088-Jm7oJ_0] (Re-)joining group
13:47:23.301 [pool-36-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-091-Tt1W9dc-31, groupId=sub-091-Tt1W9dc] Request joining group due to: need to re-join with the given member-id: consumer-sub-091-Tt1W9dc-31-6623427b-7a19-462e-8027-78db3bf84408
13:47:23.301 [pool-36-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-091-Tt1W9dc-31, groupId=sub-091-Tt1W9dc] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.301 [pool-36-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-091-Tt1W9dc-31, groupId=sub-091-Tt1W9dc] (Re-)joining group
13:47:23.303 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.303 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.303 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043302
13:47:23.303 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-097-qcwhv34-33, groupId=sub-097-qcwhv34] Subscribed to topic(s): test-topic-0000097-rYrnvdk
13:47:23.304 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-100-dbigFbc-34
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-100-dbigFbc
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.305 [pool-37-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-094-RJlVLD4-32, groupId=sub-094-RJlVLD4] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.306 [pool-37-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-094-RJlVLD4-32, groupId=sub-094-RJlVLD4] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.306 [pool-37-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-094-RJlVLD4-32, groupId=sub-094-RJlVLD4] (Re-)joining group
13:47:23.310 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.310 [pool-38-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-097-qcwhv34-33, groupId=sub-097-qcwhv34] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.310 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.310 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043310
13:47:23.311 [pool-38-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-097-qcwhv34-33, groupId=sub-097-qcwhv34] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.311 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-100-dbigFbc-34, groupId=sub-100-dbigFbc] Subscribed to topic(s): test-topic-0000100-tJC42sY
13:47:23.311 [pool-37-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-094-RJlVLD4-32, groupId=sub-094-RJlVLD4] Request joining group due to: need to re-join with the given member-id: consumer-sub-094-RJlVLD4-32-9a0c0626-4a1e-43e3-8170-e322d7b6262e
13:47:23.311 [pool-38-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-097-qcwhv34-33, groupId=sub-097-qcwhv34] (Re-)joining group
13:47:23.311 [pool-37-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-094-RJlVLD4-32, groupId=sub-094-RJlVLD4] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.312 [pool-37-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-094-RJlVLD4-32, groupId=sub-094-RJlVLD4] (Re-)joining group
13:47:23.312 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-103-YQZE89U-35
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-103-YQZE89U
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.313 [pool-38-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-097-qcwhv34-33, groupId=sub-097-qcwhv34] Request joining group due to: need to re-join with the given member-id: consumer-sub-097-qcwhv34-33-df5dd98a-898e-4271-8e2e-7d49bab8c194
13:47:23.314 [pool-38-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-097-qcwhv34-33, groupId=sub-097-qcwhv34] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.314 [pool-38-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-097-qcwhv34-33, groupId=sub-097-qcwhv34] (Re-)joining group
13:47:23.315 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.315 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.315 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043315
13:47:23.315 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-103-YQZE89U-35, groupId=sub-103-YQZE89U] Subscribed to topic(s): test-topic-0000103-7T4hNlo
13:47:23.316 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-106-914gNsw-36
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-106-914gNsw
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.319 [pool-39-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-100-dbigFbc-34, groupId=sub-100-dbigFbc] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.319 [pool-39-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-100-dbigFbc-34, groupId=sub-100-dbigFbc] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.320 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.320 [pool-40-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-103-YQZE89U-35, groupId=sub-103-YQZE89U] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.320 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.320 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043320
13:47:23.320 [pool-40-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-103-YQZE89U-35, groupId=sub-103-YQZE89U] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.320 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-106-914gNsw-36, groupId=sub-106-914gNsw] Subscribed to topic(s): test-topic-0000106-DPglyWA
13:47:23.321 [pool-39-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-100-dbigFbc-34, groupId=sub-100-dbigFbc] (Re-)joining group
13:47:23.322 [pool-40-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-103-YQZE89U-35, groupId=sub-103-YQZE89U] (Re-)joining group
13:47:23.322 [pool-39-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-100-dbigFbc-34, groupId=sub-100-dbigFbc] Request joining group due to: need to re-join with the given member-id: consumer-sub-100-dbigFbc-34-e3316b80-68d7-4134-a409-69c0ca7e3e95
13:47:23.322 [pool-39-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-100-dbigFbc-34, groupId=sub-100-dbigFbc] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.323 [pool-39-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-100-dbigFbc-34, groupId=sub-100-dbigFbc] (Re-)joining group
13:47:23.323 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-109-JW4VXwQ-37
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-109-JW4VXwQ
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.323 [pool-40-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-103-YQZE89U-35, groupId=sub-103-YQZE89U] Request joining group due to: need to re-join with the given member-id: consumer-sub-103-YQZE89U-35-fd1eb4e0-ce28-4ad3-ad4d-53a52ab4e128
13:47:23.324 [pool-40-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-103-YQZE89U-35, groupId=sub-103-YQZE89U] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.324 [pool-40-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-103-YQZE89U-35, groupId=sub-103-YQZE89U] (Re-)joining group
13:47:23.325 [pool-41-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-106-914gNsw-36, groupId=sub-106-914gNsw] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.326 [pool-41-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-106-914gNsw-36, groupId=sub-106-914gNsw] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.326 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.326 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.326 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043326
13:47:23.327 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-109-JW4VXwQ-37, groupId=sub-109-JW4VXwQ] Subscribed to topic(s): test-topic-0000109-8qoD6Cs
13:47:23.329 [pool-41-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-106-914gNsw-36, groupId=sub-106-914gNsw] (Re-)joining group
13:47:23.330 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-112-VeqiZ0Q-38
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-112-VeqiZ0Q
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.330 [pool-41-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-106-914gNsw-36, groupId=sub-106-914gNsw] Request joining group due to: need to re-join with the given member-id: consumer-sub-106-914gNsw-36-d7c02a5c-53c6-487d-8336-b9171d8d2194
13:47:23.331 [pool-41-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-106-914gNsw-36, groupId=sub-106-914gNsw] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.331 [pool-41-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-106-914gNsw-36, groupId=sub-106-914gNsw] (Re-)joining group
13:47:23.332 [pool-42-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-109-JW4VXwQ-37, groupId=sub-109-JW4VXwQ] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.332 [pool-42-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-109-JW4VXwQ-37, groupId=sub-109-JW4VXwQ] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.332 [pool-42-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-109-JW4VXwQ-37, groupId=sub-109-JW4VXwQ] (Re-)joining group
13:47:23.333 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.333 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.334 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043333
13:47:23.334 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-112-VeqiZ0Q-38, groupId=sub-112-VeqiZ0Q] Subscribed to topic(s): test-topic-0000112-6kLoZnM
13:47:23.334 [pool-42-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-109-JW4VXwQ-37, groupId=sub-109-JW4VXwQ] Request joining group due to: need to re-join with the given member-id: consumer-sub-109-JW4VXwQ-37-3c284ab9-79a4-42f5-8f75-cf3d3b23c29a
13:47:23.334 [pool-42-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-109-JW4VXwQ-37, groupId=sub-109-JW4VXwQ] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.334 [pool-42-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-109-JW4VXwQ-37, groupId=sub-109-JW4VXwQ] (Re-)joining group
13:47:23.335 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-115-iVExwXU-39
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-115-iVExwXU
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.337 [pool-43-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-112-VeqiZ0Q-38, groupId=sub-112-VeqiZ0Q] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.337 [pool-43-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-112-VeqiZ0Q-38, groupId=sub-112-VeqiZ0Q] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.337 [pool-43-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-112-VeqiZ0Q-38, groupId=sub-112-VeqiZ0Q] (Re-)joining group
13:47:23.339 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.339 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.339 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043338
13:47:23.339 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-115-iVExwXU-39, groupId=sub-115-iVExwXU] Subscribed to topic(s): test-topic-0000115-6QgytUA
13:47:23.339 [pool-43-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-112-VeqiZ0Q-38, groupId=sub-112-VeqiZ0Q] Request joining group due to: need to re-join with the given member-id: consumer-sub-112-VeqiZ0Q-38-e5932f3b-da7b-41a2-ae31-318d5f31ad5c
13:47:23.339 [pool-43-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-112-VeqiZ0Q-38, groupId=sub-112-VeqiZ0Q] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.339 [pool-43-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-112-VeqiZ0Q-38, groupId=sub-112-VeqiZ0Q] (Re-)joining group
13:47:23.340 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-118-4zQp7Rc-40
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-118-4zQp7Rc
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.342 [pool-44-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-115-iVExwXU-39, groupId=sub-115-iVExwXU] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.342 [pool-44-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-115-iVExwXU-39, groupId=sub-115-iVExwXU] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.342 [pool-44-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-115-iVExwXU-39, groupId=sub-115-iVExwXU] (Re-)joining group
13:47:23.343 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.343 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.344 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043343
13:47:23.344 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-118-4zQp7Rc-40, groupId=sub-118-4zQp7Rc] Subscribed to topic(s): test-topic-0000118-8tUi_6Q
13:47:23.344 [pool-44-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-115-iVExwXU-39, groupId=sub-115-iVExwXU] Request joining group due to: need to re-join with the given member-id: consumer-sub-115-iVExwXU-39-da19d54e-b3ef-47f7-86e6-508ef96c112f
13:47:23.345 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-121-oObYq0E-41
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-121-oObYq0E
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.345 [pool-44-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-115-iVExwXU-39, groupId=sub-115-iVExwXU] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.345 [pool-44-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-115-iVExwXU-39, groupId=sub-115-iVExwXU] (Re-)joining group
13:47:23.346 [pool-45-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-118-4zQp7Rc-40, groupId=sub-118-4zQp7Rc] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.346 [pool-45-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-118-4zQp7Rc-40, groupId=sub-118-4zQp7Rc] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.347 [pool-45-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-118-4zQp7Rc-40, groupId=sub-118-4zQp7Rc] (Re-)joining group
13:47:23.347 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.347 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.348 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043347
13:47:23.348 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-121-oObYq0E-41, groupId=sub-121-oObYq0E] Subscribed to topic(s): test-topic-0000121-MbWpUXY
13:47:23.348 [pool-45-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-118-4zQp7Rc-40, groupId=sub-118-4zQp7Rc] Request joining group due to: need to re-join with the given member-id: consumer-sub-118-4zQp7Rc-40-4d4b2b18-538d-4869-b948-c702223e740c
13:47:23.349 [pool-45-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-118-4zQp7Rc-40, groupId=sub-118-4zQp7Rc] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.349 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-124-en_Hg3o-42
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-124-en_Hg3o
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.349 [pool-45-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-118-4zQp7Rc-40, groupId=sub-118-4zQp7Rc] (Re-)joining group
13:47:23.350 [pool-46-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-121-oObYq0E-41, groupId=sub-121-oObYq0E] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.351 [pool-46-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-121-oObYq0E-41, groupId=sub-121-oObYq0E] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.351 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.351 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.351 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043351
13:47:23.351 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-124-en_Hg3o-42, groupId=sub-124-en_Hg3o] Subscribed to topic(s): test-topic-0000124-amAHj1M
13:47:23.354 [pool-46-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-121-oObYq0E-41, groupId=sub-121-oObYq0E] (Re-)joining group
13:47:23.355 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-127-2kfMsto-43
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-127-2kfMsto
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.356 [pool-46-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-121-oObYq0E-41, groupId=sub-121-oObYq0E] Request joining group due to: need to re-join with the given member-id: consumer-sub-121-oObYq0E-41-005298f7-b253-4951-b524-43462d09a2b0
13:47:23.356 [pool-46-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-121-oObYq0E-41, groupId=sub-121-oObYq0E] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.356 [pool-46-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-121-oObYq0E-41, groupId=sub-121-oObYq0E] (Re-)joining group
13:47:23.357 [pool-47-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-124-en_Hg3o-42, groupId=sub-124-en_Hg3o] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.357 [pool-47-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-124-en_Hg3o-42, groupId=sub-124-en_Hg3o] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.358 [pool-47-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-124-en_Hg3o-42, groupId=sub-124-en_Hg3o] (Re-)joining group
13:47:23.358 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.358 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.358 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043358
13:47:23.359 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-127-2kfMsto-43, groupId=sub-127-2kfMsto] Subscribed to topic(s): test-topic-0000127-yWtvaYk
13:47:23.359 [pool-47-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-124-en_Hg3o-42, groupId=sub-124-en_Hg3o] Request joining group due to: need to re-join with the given member-id: consumer-sub-124-en_Hg3o-42-5fc124b0-7f6e-4f66-a323-074ef035875b
13:47:23.359 [pool-47-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-124-en_Hg3o-42, groupId=sub-124-en_Hg3o] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.359 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-130-dO66nCk-44
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-130-dO66nCk
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.359 [pool-47-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-124-en_Hg3o-42, groupId=sub-124-en_Hg3o] (Re-)joining group
13:47:23.361 [pool-48-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-127-2kfMsto-43, groupId=sub-127-2kfMsto] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.361 [pool-48-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-127-2kfMsto-43, groupId=sub-127-2kfMsto] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.362 [pool-48-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-127-2kfMsto-43, groupId=sub-127-2kfMsto] (Re-)joining group
13:47:23.362 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.363 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.363 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043362
13:47:23.363 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-130-dO66nCk-44, groupId=sub-130-dO66nCk] Subscribed to topic(s): test-topic-0000130-Qg40Jrk
13:47:23.363 [pool-48-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-127-2kfMsto-43, groupId=sub-127-2kfMsto] Request joining group due to: need to re-join with the given member-id: consumer-sub-127-2kfMsto-43-1c3781bf-41cf-4419-a262-b5234f50fb73
13:47:23.363 [pool-48-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-127-2kfMsto-43, groupId=sub-127-2kfMsto] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.363 [pool-48-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-127-2kfMsto-43, groupId=sub-127-2kfMsto] (Re-)joining group
13:47:23.364 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-133-G2mVtvo-45
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-133-G2mVtvo
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.365 [pool-49-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-130-dO66nCk-44, groupId=sub-130-dO66nCk] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.366 [pool-49-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-130-dO66nCk-44, groupId=sub-130-dO66nCk] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.366 [pool-49-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-130-dO66nCk-44, groupId=sub-130-dO66nCk] (Re-)joining group
13:47:23.367 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.367 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.367 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043367
13:47:23.367 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-133-G2mVtvo-45, groupId=sub-133-G2mVtvo] Subscribed to topic(s): test-topic-0000133-98k_qFM
13:47:23.368 [pool-49-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-130-dO66nCk-44, groupId=sub-130-dO66nCk] Request joining group due to: need to re-join with the given member-id: consumer-sub-130-dO66nCk-44-2959ab19-f592-49ef-a403-a547a7284a9b
13:47:23.368 [pool-49-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-130-dO66nCk-44, groupId=sub-130-dO66nCk] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.368 [pool-49-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-130-dO66nCk-44, groupId=sub-130-dO66nCk] (Re-)joining group
13:47:23.368 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-136-6zUzaPQ-46
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-136-6zUzaPQ
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.369 [pool-50-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-133-G2mVtvo-45, groupId=sub-133-G2mVtvo] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.370 [pool-50-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-133-G2mVtvo-45, groupId=sub-133-G2mVtvo] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.370 [pool-50-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-133-G2mVtvo-45, groupId=sub-133-G2mVtvo] (Re-)joining group
13:47:23.371 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.371 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.371 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043371
13:47:23.372 [pool-50-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-133-G2mVtvo-45, groupId=sub-133-G2mVtvo] Request joining group due to: need to re-join with the given member-id: consumer-sub-133-G2mVtvo-45-fcf70cf4-ce4b-4040-82a9-b740e1fc8175
13:47:23.372 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-136-6zUzaPQ-46, groupId=sub-136-6zUzaPQ] Subscribed to topic(s): test-topic-0000136-Y9wEdEA
13:47:23.372 [pool-50-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-133-G2mVtvo-45, groupId=sub-133-G2mVtvo] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.372 [pool-50-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-133-G2mVtvo-45, groupId=sub-133-G2mVtvo] (Re-)joining group
13:47:23.373 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-139-HsIn88s-47
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-139-HsIn88s
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.375 [pool-51-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-136-6zUzaPQ-46, groupId=sub-136-6zUzaPQ] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.375 [pool-51-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-136-6zUzaPQ-46, groupId=sub-136-6zUzaPQ] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.375 [pool-51-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-136-6zUzaPQ-46, groupId=sub-136-6zUzaPQ] (Re-)joining group
13:47:23.376 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.376 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.376 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043376
13:47:23.376 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-139-HsIn88s-47, groupId=sub-139-HsIn88s] Subscribed to topic(s): test-topic-0000139-OuBTmUg
13:47:23.377 [pool-51-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-136-6zUzaPQ-46, groupId=sub-136-6zUzaPQ] Request joining group due to: need to re-join with the given member-id: consumer-sub-136-6zUzaPQ-46-d6e8672a-a7b3-4aef-b2d9-821f5f78d093
13:47:23.377 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-142-LVH9jo4-48
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-142-LVH9jo4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.377 [pool-51-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-136-6zUzaPQ-46, groupId=sub-136-6zUzaPQ] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.377 [pool-51-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-136-6zUzaPQ-46, groupId=sub-136-6zUzaPQ] (Re-)joining group
13:47:23.378 [pool-52-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-139-HsIn88s-47, groupId=sub-139-HsIn88s] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.379 [pool-52-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-139-HsIn88s-47, groupId=sub-139-HsIn88s] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.380 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.380 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.380 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043380
13:47:23.380 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-142-LVH9jo4-48, groupId=sub-142-LVH9jo4] Subscribed to topic(s): test-topic-0000142-PXtC-SI
13:47:23.382 [pool-52-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-139-HsIn88s-47, groupId=sub-139-HsIn88s] (Re-)joining group
13:47:23.383 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-145-Car66lg-49
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-145-Car66lg
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.383 [pool-52-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-139-HsIn88s-47, groupId=sub-139-HsIn88s] Request joining group due to: need to re-join with the given member-id: consumer-sub-139-HsIn88s-47-1e76a18f-4fd4-4f59-8f05-071f85da7d1a
13:47:23.383 [pool-52-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-139-HsIn88s-47, groupId=sub-139-HsIn88s] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.384 [pool-52-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-139-HsIn88s-47, groupId=sub-139-HsIn88s] (Re-)joining group
13:47:23.385 [pool-53-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-142-LVH9jo4-48, groupId=sub-142-LVH9jo4] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.386 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.386 [pool-53-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-142-LVH9jo4-48, groupId=sub-142-LVH9jo4] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.386 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.386 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043386
13:47:23.386 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-145-Car66lg-49, groupId=sub-145-Car66lg] Subscribed to topic(s): test-topic-0000145-c-Awvi4
13:47:23.386 [pool-53-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-142-LVH9jo4-48, groupId=sub-142-LVH9jo4] (Re-)joining group
13:47:23.387 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-148-g2FltJE-50
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-148-g2FltJE
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.388 [pool-53-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-142-LVH9jo4-48, groupId=sub-142-LVH9jo4] Request joining group due to: need to re-join with the given member-id: consumer-sub-142-LVH9jo4-48-287abba6-1191-4823-b235-730c1eed8d89
13:47:23.388 [pool-53-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-142-LVH9jo4-48, groupId=sub-142-LVH9jo4] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.388 [pool-53-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-142-LVH9jo4-48, groupId=sub-142-LVH9jo4] (Re-)joining group
13:47:23.389 [pool-54-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-145-Car66lg-49, groupId=sub-145-Car66lg] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.389 [pool-54-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-145-Car66lg-49, groupId=sub-145-Car66lg] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.397 [pool-54-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-145-Car66lg-49, groupId=sub-145-Car66lg] (Re-)joining group
13:47:23.398 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.398 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.398 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043398
13:47:23.398 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-148-g2FltJE-50, groupId=sub-148-g2FltJE] Subscribed to topic(s): test-topic-0000148-jGtN0UQ
13:47:23.399 [pool-54-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-145-Car66lg-49, groupId=sub-145-Car66lg] Request joining group due to: need to re-join with the given member-id: consumer-sub-145-Car66lg-49-94d64ef4-42d0-4a9f-9016-13d66945953f
13:47:23.399 [pool-54-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-145-Car66lg-49, groupId=sub-145-Car66lg] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.399 [pool-54-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-145-Car66lg-49, groupId=sub-145-Car66lg] (Re-)joining group
13:47:23.399 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-151-UaCj-zQ-51
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-151-UaCj-zQ
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.401 [pool-55-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-148-g2FltJE-50, groupId=sub-148-g2FltJE] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.401 [pool-55-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-148-g2FltJE-50, groupId=sub-148-g2FltJE] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.402 [pool-55-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-148-g2FltJE-50, groupId=sub-148-g2FltJE] (Re-)joining group
13:47:23.402 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.403 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.403 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043402
13:47:23.403 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-151-UaCj-zQ-51, groupId=sub-151-UaCj-zQ] Subscribed to topic(s): test-topic-0000151-TzSMYyE
13:47:23.404 [pool-55-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-148-g2FltJE-50, groupId=sub-148-g2FltJE] Request joining group due to: need to re-join with the given member-id: consumer-sub-148-g2FltJE-50-6f8572e0-1cb0-484a-8c22-d2bbbcfb8f6d
13:47:23.404 [pool-55-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-148-g2FltJE-50, groupId=sub-148-g2FltJE] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.404 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-154-JBCLsNU-52
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-154-JBCLsNU
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.404 [pool-55-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-148-g2FltJE-50, groupId=sub-148-g2FltJE] (Re-)joining group
13:47:23.406 [pool-56-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-151-UaCj-zQ-51, groupId=sub-151-UaCj-zQ] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.406 [pool-56-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-151-UaCj-zQ-51, groupId=sub-151-UaCj-zQ] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.407 [pool-56-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-151-UaCj-zQ-51, groupId=sub-151-UaCj-zQ] (Re-)joining group
13:47:23.407 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.407 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.408 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043407
13:47:23.408 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-154-JBCLsNU-52, groupId=sub-154-JBCLsNU] Subscribed to topic(s): test-topic-0000154-akFkJ1U
13:47:23.408 [pool-56-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-151-UaCj-zQ-51, groupId=sub-151-UaCj-zQ] Request joining group due to: need to re-join with the given member-id: consumer-sub-151-UaCj-zQ-51-f6cd4f25-df95-4a2c-bfb6-91f0daf41224
13:47:23.409 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-157-z_6wOmk-53
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-157-z_6wOmk
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.409 [pool-56-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-151-UaCj-zQ-51, groupId=sub-151-UaCj-zQ] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.409 [pool-56-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-151-UaCj-zQ-51, groupId=sub-151-UaCj-zQ] (Re-)joining group
13:47:23.410 [pool-57-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-154-JBCLsNU-52, groupId=sub-154-JBCLsNU] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.411 [pool-57-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-154-JBCLsNU-52, groupId=sub-154-JBCLsNU] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.412 [pool-57-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-154-JBCLsNU-52, groupId=sub-154-JBCLsNU] (Re-)joining group
13:47:23.412 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.412 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.412 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043412
13:47:23.412 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-157-z_6wOmk-53, groupId=sub-157-z_6wOmk] Subscribed to topic(s): test-topic-0000157--IfLOsc
13:47:23.413 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-160-dA9F2fM-54
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-160-dA9F2fM
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.414 [pool-57-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-154-JBCLsNU-52, groupId=sub-154-JBCLsNU] Request joining group due to: need to re-join with the given member-id: consumer-sub-154-JBCLsNU-52-1615aa8c-82d7-4315-ad42-2c3439812c89
13:47:23.414 [pool-57-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-154-JBCLsNU-52, groupId=sub-154-JBCLsNU] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.414 [pool-57-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-154-JBCLsNU-52, groupId=sub-154-JBCLsNU] (Re-)joining group
13:47:23.415 [pool-58-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-157-z_6wOmk-53, groupId=sub-157-z_6wOmk] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.415 [pool-58-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-157-z_6wOmk-53, groupId=sub-157-z_6wOmk] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.416 [pool-58-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-157-z_6wOmk-53, groupId=sub-157-z_6wOmk] (Re-)joining group
13:47:23.417 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.417 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.417 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043416
13:47:23.417 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-160-dA9F2fM-54, groupId=sub-160-dA9F2fM] Subscribed to topic(s): test-topic-0000160-mjkwCiM
13:47:23.417 [pool-58-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-157-z_6wOmk-53, groupId=sub-157-z_6wOmk] Request joining group due to: need to re-join with the given member-id: consumer-sub-157-z_6wOmk-53-1bc5d14d-6ab7-4ea7-ba0d-f5b412529a91
13:47:23.418 [pool-58-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-157-z_6wOmk-53, groupId=sub-157-z_6wOmk] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.418 [pool-58-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-157-z_6wOmk-53, groupId=sub-157-z_6wOmk] (Re-)joining group
13:47:23.418 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-163-JNFcgqM-55
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-163-JNFcgqM
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.420 [pool-59-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-160-dA9F2fM-54, groupId=sub-160-dA9F2fM] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.420 [pool-59-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-160-dA9F2fM-54, groupId=sub-160-dA9F2fM] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.420 [pool-59-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-160-dA9F2fM-54, groupId=sub-160-dA9F2fM] (Re-)joining group
13:47:23.421 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.421 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.421 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043421
13:47:23.421 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-163-JNFcgqM-55, groupId=sub-163-JNFcgqM] Subscribed to topic(s): test-topic-0000163-_ZTSuc8
13:47:23.422 [pool-59-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-160-dA9F2fM-54, groupId=sub-160-dA9F2fM] Request joining group due to: need to re-join with the given member-id: consumer-sub-160-dA9F2fM-54-e31b8255-2633-4fa9-a6dc-a4de36f2b4a0
13:47:23.422 [pool-59-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-160-dA9F2fM-54, groupId=sub-160-dA9F2fM] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.423 [pool-59-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-160-dA9F2fM-54, groupId=sub-160-dA9F2fM] (Re-)joining group
13:47:23.423 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-166-gpfNMxQ-56
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-166-gpfNMxQ
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.424 [pool-60-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-163-JNFcgqM-55, groupId=sub-163-JNFcgqM] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.425 [pool-60-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-163-JNFcgqM-55, groupId=sub-163-JNFcgqM] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.425 [pool-60-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-163-JNFcgqM-55, groupId=sub-163-JNFcgqM] (Re-)joining group
13:47:23.426 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.426 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.426 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043426
13:47:23.426 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-166-gpfNMxQ-56, groupId=sub-166-gpfNMxQ] Subscribed to topic(s): test-topic-0000166-FI2baz0
13:47:23.427 [pool-60-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-163-JNFcgqM-55, groupId=sub-163-JNFcgqM] Request joining group due to: need to re-join with the given member-id: consumer-sub-163-JNFcgqM-55-900fb8cb-b26f-4321-98b1-6f2a8b40dfac
13:47:23.427 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-169-Ss3iap4-57
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-169-Ss3iap4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.427 [pool-60-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-163-JNFcgqM-55, groupId=sub-163-JNFcgqM] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.427 [pool-60-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-163-JNFcgqM-55, groupId=sub-163-JNFcgqM] (Re-)joining group
13:47:23.428 [pool-61-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-166-gpfNMxQ-56, groupId=sub-166-gpfNMxQ] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.429 [pool-61-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-166-gpfNMxQ-56, groupId=sub-166-gpfNMxQ] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.430 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.430 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.430 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043430
13:47:23.430 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-169-Ss3iap4-57, groupId=sub-169-Ss3iap4] Subscribed to topic(s): test-topic-0000169-frnmqbA
13:47:23.431 [pool-61-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-166-gpfNMxQ-56, groupId=sub-166-gpfNMxQ] (Re-)joining group
13:47:23.432 [pool-61-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-166-gpfNMxQ-56, groupId=sub-166-gpfNMxQ] Request joining group due to: need to re-join with the given member-id: consumer-sub-166-gpfNMxQ-56-880d3cbe-b2fc-4c8e-911e-c1bc2a866786
13:47:23.432 [pool-61-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-166-gpfNMxQ-56, groupId=sub-166-gpfNMxQ] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.432 [pool-61-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-166-gpfNMxQ-56, groupId=sub-166-gpfNMxQ] (Re-)joining group
13:47:23.433 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-172-wfkKd_8-58
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-172-wfkKd_8
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.436 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.436 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.436 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043436
13:47:23.436 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-172-wfkKd_8-58, groupId=sub-172-wfkKd_8] Subscribed to topic(s): test-topic-0000172-zC5ZI88
13:47:23.437 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-175-e6kTYM0-59
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-175-e6kTYM0
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.440 [pool-63-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-172-wfkKd_8-58, groupId=sub-172-wfkKd_8] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.440 [pool-62-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-169-Ss3iap4-57, groupId=sub-169-Ss3iap4] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.440 [pool-63-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-172-wfkKd_8-58, groupId=sub-172-wfkKd_8] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.440 [pool-62-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-169-Ss3iap4-57, groupId=sub-169-Ss3iap4] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.440 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.440 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.440 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043440
13:47:23.441 [pool-63-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-172-wfkKd_8-58, groupId=sub-172-wfkKd_8] (Re-)joining group
13:47:23.441 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-175-e6kTYM0-59, groupId=sub-175-e6kTYM0] Subscribed to topic(s): test-topic-0000175-nKeyHaA
13:47:23.441 [pool-62-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-169-Ss3iap4-57, groupId=sub-169-Ss3iap4] (Re-)joining group
13:47:23.442 [pool-63-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-172-wfkKd_8-58, groupId=sub-172-wfkKd_8] Request joining group due to: need to re-join with the given member-id: consumer-sub-172-wfkKd_8-58-d0dad028-7e14-4b24-bcd9-e8845ff04726
13:47:23.443 [pool-63-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-172-wfkKd_8-58, groupId=sub-172-wfkKd_8] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.443 [pool-63-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-172-wfkKd_8-58, groupId=sub-172-wfkKd_8] (Re-)joining group
13:47:23.443 [pool-62-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-169-Ss3iap4-57, groupId=sub-169-Ss3iap4] Request joining group due to: need to re-join with the given member-id: consumer-sub-169-Ss3iap4-57-a4c1ff45-9ea5-405e-942b-d60a96cace53
13:47:23.443 [pool-62-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-169-Ss3iap4-57, groupId=sub-169-Ss3iap4] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.443 [pool-62-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-169-Ss3iap4-57, groupId=sub-169-Ss3iap4] (Re-)joining group
13:47:23.443 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-178-Baic5GM-60
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-178-Baic5GM
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.444 [pool-64-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-175-e6kTYM0-59, groupId=sub-175-e6kTYM0] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.445 [pool-64-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-175-e6kTYM0-59, groupId=sub-175-e6kTYM0] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.445 [pool-64-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-175-e6kTYM0-59, groupId=sub-175-e6kTYM0] (Re-)joining group
13:47:23.446 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.446 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.446 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043446
13:47:23.447 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-178-Baic5GM-60, groupId=sub-178-Baic5GM] Subscribed to topic(s): test-topic-0000178-yN-12bs
13:47:23.447 [pool-64-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-175-e6kTYM0-59, groupId=sub-175-e6kTYM0] Request joining group due to: need to re-join with the given member-id: consumer-sub-175-e6kTYM0-59-c7c2b8d0-6dc5-47c4-bdbf-9f2e04ead312
13:47:23.448 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-181-SEtrYEw-61
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-181-SEtrYEw
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.448 [pool-64-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-175-e6kTYM0-59, groupId=sub-175-e6kTYM0] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.448 [pool-64-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-175-e6kTYM0-59, groupId=sub-175-e6kTYM0] (Re-)joining group
13:47:23.449 [pool-65-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-178-Baic5GM-60, groupId=sub-178-Baic5GM] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.449 [pool-65-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-178-Baic5GM-60, groupId=sub-178-Baic5GM] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.450 [pool-65-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-178-Baic5GM-60, groupId=sub-178-Baic5GM] (Re-)joining group
13:47:23.451 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.451 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.451 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043451
13:47:23.452 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-181-SEtrYEw-61, groupId=sub-181-SEtrYEw] Subscribed to topic(s): test-topic-0000181-ILkihtM
13:47:23.452 [pool-65-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-178-Baic5GM-60, groupId=sub-178-Baic5GM] Request joining group due to: need to re-join with the given member-id: consumer-sub-178-Baic5GM-60-64b95a95-0fda-4b00-ae5d-01f9e33cd8d6
13:47:23.452 [pool-65-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-178-Baic5GM-60, groupId=sub-178-Baic5GM] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.452 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-184-wUomEWE-62
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-184-wUomEWE
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.453 [pool-65-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-178-Baic5GM-60, groupId=sub-178-Baic5GM] (Re-)joining group
13:47:23.455 [pool-66-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-181-SEtrYEw-61, groupId=sub-181-SEtrYEw] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.455 [pool-66-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-181-SEtrYEw-61, groupId=sub-181-SEtrYEw] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.456 [pool-66-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-181-SEtrYEw-61, groupId=sub-181-SEtrYEw] (Re-)joining group
13:47:23.456 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.456 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.456 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043456
13:47:23.456 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-184-wUomEWE-62, groupId=sub-184-wUomEWE] Subscribed to topic(s): test-topic-0000184-YsWG3PQ
13:47:23.457 [pool-66-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-181-SEtrYEw-61, groupId=sub-181-SEtrYEw] Request joining group due to: need to re-join with the given member-id: consumer-sub-181-SEtrYEw-61-3343e373-0410-4584-8905-9c35f2a61c41
13:47:23.457 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-187-8Yu2BAk-63
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-187-8Yu2BAk
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.458 [pool-66-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-181-SEtrYEw-61, groupId=sub-181-SEtrYEw] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.458 [pool-66-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-181-SEtrYEw-61, groupId=sub-181-SEtrYEw] (Re-)joining group
13:47:23.459 [pool-67-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-184-wUomEWE-62, groupId=sub-184-wUomEWE] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.459 [pool-67-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-184-wUomEWE-62, groupId=sub-184-wUomEWE] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.460 [pool-67-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-184-wUomEWE-62, groupId=sub-184-wUomEWE] (Re-)joining group
13:47:23.461 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.461 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.461 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043461
13:47:23.461 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-187-8Yu2BAk-63, groupId=sub-187-8Yu2BAk] Subscribed to topic(s): test-topic-0000187-a7MsovY
13:47:23.462 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-190-gMLfOV4-64
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-190-gMLfOV4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.464 [pool-68-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-187-8Yu2BAk-63, groupId=sub-187-8Yu2BAk] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.465 [pool-68-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-187-8Yu2BAk-63, groupId=sub-187-8Yu2BAk] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.466 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.466 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.466 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043466
13:47:23.466 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-190-gMLfOV4-64, groupId=sub-190-gMLfOV4] Subscribed to topic(s): test-topic-0000190-2u2ROx8
13:47:23.462 [pool-67-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-184-wUomEWE-62, groupId=sub-184-wUomEWE] Request joining group due to: need to re-join with the given member-id: consumer-sub-184-wUomEWE-62-6d157b5e-ada9-4965-884c-5fbb9e9fbf96
13:47:23.467 [pool-67-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-184-wUomEWE-62, groupId=sub-184-wUomEWE] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.467 [pool-67-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-184-wUomEWE-62, groupId=sub-184-wUomEWE] (Re-)joining group
13:47:23.467 [pool-68-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-187-8Yu2BAk-63, groupId=sub-187-8Yu2BAk] (Re-)joining group
13:47:23.469 [pool-68-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-187-8Yu2BAk-63, groupId=sub-187-8Yu2BAk] Request joining group due to: need to re-join with the given member-id: consumer-sub-187-8Yu2BAk-63-e8fe87fa-8277-4f3b-8d93-61ffd14f3fdd
13:47:23.469 [pool-68-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-187-8Yu2BAk-63, groupId=sub-187-8Yu2BAk] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.469 [pool-68-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-187-8Yu2BAk-63, groupId=sub-187-8Yu2BAk] (Re-)joining group
13:47:23.469 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-193-C454ui4-65
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-193-C454ui4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.471 [pool-69-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-190-gMLfOV4-64, groupId=sub-190-gMLfOV4] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.471 [pool-69-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-190-gMLfOV4-64, groupId=sub-190-gMLfOV4] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.473 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.473 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.473 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043473
13:47:23.473 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-193-C454ui4-65, groupId=sub-193-C454ui4] Subscribed to topic(s): test-topic-0000193-ngfcLus
13:47:23.474 [pool-69-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-190-gMLfOV4-64, groupId=sub-190-gMLfOV4] (Re-)joining group
13:47:23.475 [pool-69-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-190-gMLfOV4-64, groupId=sub-190-gMLfOV4] Request joining group due to: need to re-join with the given member-id: consumer-sub-190-gMLfOV4-64-404fe1dd-d188-4f1b-9d0d-37aeccac64ff
13:47:23.475 [pool-69-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-190-gMLfOV4-64, groupId=sub-190-gMLfOV4] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.476 [pool-69-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-190-gMLfOV4-64, groupId=sub-190-gMLfOV4] (Re-)joining group
13:47:23.477 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-196-oTB6GQc-66
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-196-oTB6GQc
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.479 [pool-70-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-193-C454ui4-65, groupId=sub-193-C454ui4] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.479 [pool-70-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-193-C454ui4-65, groupId=sub-193-C454ui4] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.480 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.480 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.480 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043480
13:47:23.481 [pool-70-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-193-C454ui4-65, groupId=sub-193-C454ui4] (Re-)joining group
13:47:23.481 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-196-oTB6GQc-66, groupId=sub-196-oTB6GQc] Subscribed to topic(s): test-topic-0000196-IG_hwmw
13:47:23.482 [pool-70-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-193-C454ui4-65, groupId=sub-193-C454ui4] Request joining group due to: need to re-join with the given member-id: consumer-sub-193-C454ui4-65-3d9dabca-8916-4bb8-b626-3ebf50fdc09d
13:47:23.483 [pool-70-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-193-C454ui4-65, groupId=sub-193-C454ui4] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.483 [pool-70-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-193-C454ui4-65, groupId=sub-193-C454ui4] (Re-)joining group
13:47:23.494 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-199-UuD-INE-67
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-199-UuD-INE
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.496 [pool-71-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-196-oTB6GQc-66, groupId=sub-196-oTB6GQc] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.496 [pool-71-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-196-oTB6GQc-66, groupId=sub-196-oTB6GQc] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.497 [pool-71-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-196-oTB6GQc-66, groupId=sub-196-oTB6GQc] (Re-)joining group
13:47:23.498 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.498 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.498 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043498
13:47:23.498 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-199-UuD-INE-67, groupId=sub-199-UuD-INE] Subscribed to topic(s): test-topic-0000199-CtttN8Y
13:47:23.499 [pool-71-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-196-oTB6GQc-66, groupId=sub-196-oTB6GQc] Request joining group due to: need to re-join with the given member-id: consumer-sub-196-oTB6GQc-66-20f8562d-c1c2-45c0-a767-dcb8663c143b
13:47:23.499 [pool-71-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-196-oTB6GQc-66, groupId=sub-196-oTB6GQc] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.499 [pool-71-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-196-oTB6GQc-66, groupId=sub-196-oTB6GQc] (Re-)joining group
13:47:23.500 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-202-V2MvyKU-68
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-202-V2MvyKU
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.503 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.503 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.503 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043503
13:47:23.503 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-202-V2MvyKU-68, groupId=sub-202-V2MvyKU] Subscribed to topic(s): test-topic-0000202-3wtSyrE
13:47:23.505 [pool-72-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-199-UuD-INE-67, groupId=sub-199-UuD-INE] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.505 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-205-y-mu9t8-69
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-205-y-mu9t8
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.505 [pool-72-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-199-UuD-INE-67, groupId=sub-199-UuD-INE] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.506 [pool-72-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-199-UuD-INE-67, groupId=sub-199-UuD-INE] (Re-)joining group
13:47:23.509 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.509 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.509 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043509
13:47:23.510 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-205-y-mu9t8-69, groupId=sub-205-y-mu9t8] Subscribed to topic(s): test-topic-0000205-2mMmqQg
13:47:23.510 [pool-72-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-199-UuD-INE-67, groupId=sub-199-UuD-INE] Request joining group due to: need to re-join with the given member-id: consumer-sub-199-UuD-INE-67-0a0d9dfd-3cfc-42b2-982f-6fd1cb1ceb32
13:47:23.510 [pool-72-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-199-UuD-INE-67, groupId=sub-199-UuD-INE] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.510 [pool-72-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-199-UuD-INE-67, groupId=sub-199-UuD-INE] (Re-)joining group
13:47:23.511 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-208-4UCS5aM-70
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-208-4UCS5aM
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.513 [pool-74-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-205-y-mu9t8-69, groupId=sub-205-y-mu9t8] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.514 [pool-74-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-205-y-mu9t8-69, groupId=sub-205-y-mu9t8] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.514 [pool-74-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-205-y-mu9t8-69, groupId=sub-205-y-mu9t8] (Re-)joining group
13:47:23.514 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.514 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.514 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043514
13:47:23.515 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-208-4UCS5aM-70, groupId=sub-208-4UCS5aM] Subscribed to topic(s): test-topic-0000208-zYqakMk
13:47:23.516 [pool-73-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-202-V2MvyKU-68, groupId=sub-202-V2MvyKU] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.516 [pool-73-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-202-V2MvyKU-68, groupId=sub-202-V2MvyKU] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.516 [pool-74-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-205-y-mu9t8-69, groupId=sub-205-y-mu9t8] Request joining group due to: need to re-join with the given member-id: consumer-sub-205-y-mu9t8-69-4be45384-e0e7-4656-a99f-67e2cc376520
13:47:23.516 [pool-74-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-205-y-mu9t8-69, groupId=sub-205-y-mu9t8] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.516 [pool-74-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-205-y-mu9t8-69, groupId=sub-205-y-mu9t8] (Re-)joining group
13:47:23.517 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-211-HlvOMkY-71
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-211-HlvOMkY
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.519 [pool-73-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-202-V2MvyKU-68, groupId=sub-202-V2MvyKU] (Re-)joining group
13:47:23.520 [pool-73-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-202-V2MvyKU-68, groupId=sub-202-V2MvyKU] Request joining group due to: need to re-join with the given member-id: consumer-sub-202-V2MvyKU-68-4020617f-d32f-431f-aa42-9a8fe38dcb52
13:47:23.521 [pool-73-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-202-V2MvyKU-68, groupId=sub-202-V2MvyKU] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.521 [pool-73-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-202-V2MvyKU-68, groupId=sub-202-V2MvyKU] (Re-)joining group
13:47:23.521 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.521 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.521 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043521
13:47:23.521 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-211-HlvOMkY-71, groupId=sub-211-HlvOMkY] Subscribed to topic(s): test-topic-0000211-_2UT4Bs
13:47:23.523 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-214-Bg60hV0-72
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-214-Bg60hV0
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.525 [pool-76-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-211-HlvOMkY-71, groupId=sub-211-HlvOMkY] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.525 [pool-76-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-211-HlvOMkY-71, groupId=sub-211-HlvOMkY] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.526 [pool-76-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-211-HlvOMkY-71, groupId=sub-211-HlvOMkY] (Re-)joining group
13:47:23.527 [pool-75-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-208-4UCS5aM-70, groupId=sub-208-4UCS5aM] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.528 [pool-75-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-208-4UCS5aM-70, groupId=sub-208-4UCS5aM] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.528 [pool-75-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-208-4UCS5aM-70, groupId=sub-208-4UCS5aM] (Re-)joining group
13:47:23.528 [pool-76-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-211-HlvOMkY-71, groupId=sub-211-HlvOMkY] Request joining group due to: need to re-join with the given member-id: consumer-sub-211-HlvOMkY-71-fe4f25c7-5d70-4d58-8094-5cf15502ad23
13:47:23.528 [pool-76-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-211-HlvOMkY-71, groupId=sub-211-HlvOMkY] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.529 [pool-76-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-211-HlvOMkY-71, groupId=sub-211-HlvOMkY] (Re-)joining group
13:47:23.529 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.529 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.529 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043527
13:47:23.529 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-214-Bg60hV0-72, groupId=sub-214-Bg60hV0] Subscribed to topic(s): test-topic-0000214-MyxBVy0
13:47:23.530 [pool-75-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-208-4UCS5aM-70, groupId=sub-208-4UCS5aM] Request joining group due to: need to re-join with the given member-id: consumer-sub-208-4UCS5aM-70-aa8ad4bb-f6ba-4549-a1f8-36a428f68cd7
13:47:23.530 [pool-75-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-208-4UCS5aM-70, groupId=sub-208-4UCS5aM] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.530 [pool-75-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-208-4UCS5aM-70, groupId=sub-208-4UCS5aM] (Re-)joining group
13:47:23.530 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-217-MqEbvWM-73
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-217-MqEbvWM
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.531 [pool-77-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-214-Bg60hV0-72, groupId=sub-214-Bg60hV0] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.532 [pool-77-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-214-Bg60hV0-72, groupId=sub-214-Bg60hV0] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.533 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.533 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.533 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043533
13:47:23.533 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-217-MqEbvWM-73, groupId=sub-217-MqEbvWM] Subscribed to topic(s): test-topic-0000217-XNxlkQ4
13:47:23.533 [pool-77-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-214-Bg60hV0-72, groupId=sub-214-Bg60hV0] (Re-)joining group
13:47:23.535 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-220-1KuraSw-74
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-220-1KuraSw
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.537 [pool-77-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-214-Bg60hV0-72, groupId=sub-214-Bg60hV0] Request joining group due to: need to re-join with the given member-id: consumer-sub-214-Bg60hV0-72-f001fc31-3671-4897-a6d3-27a30b7e62ff
13:47:23.538 [pool-77-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-214-Bg60hV0-72, groupId=sub-214-Bg60hV0] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.538 [pool-77-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-214-Bg60hV0-72, groupId=sub-214-Bg60hV0] (Re-)joining group
13:47:23.538 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.538 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.538 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043538
13:47:23.538 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-220-1KuraSw-74, groupId=sub-220-1KuraSw] Subscribed to topic(s): test-topic-0000220-uOm4gtE
13:47:23.540 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-223-CpU6oas-75
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-223-CpU6oas
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.542 [pool-79-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-220-1KuraSw-74, groupId=sub-220-1KuraSw] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.542 [pool-79-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-220-1KuraSw-74, groupId=sub-220-1KuraSw] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.544 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.544 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.544 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043544
13:47:23.544 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-223-CpU6oas-75, groupId=sub-223-CpU6oas] Subscribed to topic(s): test-topic-0000223-NBqvIZI
13:47:23.544 [pool-79-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-220-1KuraSw-74, groupId=sub-220-1KuraSw] (Re-)joining group
13:47:23.545 [pool-78-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-217-MqEbvWM-73, groupId=sub-217-MqEbvWM] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.545 [pool-78-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-217-MqEbvWM-73, groupId=sub-217-MqEbvWM] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.545 [pool-78-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-217-MqEbvWM-73, groupId=sub-217-MqEbvWM] (Re-)joining group
13:47:23.546 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-226-mSjBVK8-76
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-226-mSjBVK8
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.547 [pool-79-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-220-1KuraSw-74, groupId=sub-220-1KuraSw] Request joining group due to: need to re-join with the given member-id: consumer-sub-220-1KuraSw-74-2ff2f1ee-1656-446c-a611-280d79b9030c
13:47:23.547 [pool-79-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-220-1KuraSw-74, groupId=sub-220-1KuraSw] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.547 [pool-79-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-220-1KuraSw-74, groupId=sub-220-1KuraSw] (Re-)joining group
13:47:23.547 [pool-78-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-217-MqEbvWM-73, groupId=sub-217-MqEbvWM] Request joining group due to: need to re-join with the given member-id: consumer-sub-217-MqEbvWM-73-a570aa6d-0402-45fe-a538-cc041722790c
13:47:23.548 [pool-78-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-217-MqEbvWM-73, groupId=sub-217-MqEbvWM] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.548 [pool-78-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-217-MqEbvWM-73, groupId=sub-217-MqEbvWM] (Re-)joining group
13:47:23.549 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.549 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.549 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043549
13:47:23.549 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-226-mSjBVK8-76, groupId=sub-226-mSjBVK8] Subscribed to topic(s): test-topic-0000226-cmxwGEA
13:47:23.551 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-229-4OSEzZ0-77
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-229-4OSEzZ0
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.551 [pool-80-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-223-CpU6oas-75, groupId=sub-223-CpU6oas] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.552 [pool-80-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-223-CpU6oas-75, groupId=sub-223-CpU6oas] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.553 [pool-80-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-223-CpU6oas-75, groupId=sub-223-CpU6oas] (Re-)joining group
13:47:23.554 [pool-80-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-223-CpU6oas-75, groupId=sub-223-CpU6oas] Request joining group due to: need to re-join with the given member-id: consumer-sub-223-CpU6oas-75-5ccd6af0-062a-4413-9451-d6a5a446cede
13:47:23.554 [pool-80-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-223-CpU6oas-75, groupId=sub-223-CpU6oas] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.555 [pool-80-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-223-CpU6oas-75, groupId=sub-223-CpU6oas] (Re-)joining group
13:47:23.555 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.555 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.555 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043555
13:47:23.555 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-229-4OSEzZ0-77, groupId=sub-229-4OSEzZ0] Subscribed to topic(s): test-topic-0000229-0fBbwTU
13:47:23.556 [pool-81-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-226-mSjBVK8-76, groupId=sub-226-mSjBVK8] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.557 [pool-81-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-226-mSjBVK8-76, groupId=sub-226-mSjBVK8] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.557 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-232-IkCxllw-78
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-232-IkCxllw
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.557 [pool-81-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-226-mSjBVK8-76, groupId=sub-226-mSjBVK8] (Re-)joining group
13:47:23.559 [pool-81-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-226-mSjBVK8-76, groupId=sub-226-mSjBVK8] Request joining group due to: need to re-join with the given member-id: consumer-sub-226-mSjBVK8-76-15eaa6fc-c98a-4663-b146-cf1ec74b533b
13:47:23.559 [pool-81-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-226-mSjBVK8-76, groupId=sub-226-mSjBVK8] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.559 [pool-81-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-226-mSjBVK8-76, groupId=sub-226-mSjBVK8] (Re-)joining group
13:47:23.559 [pool-82-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-229-4OSEzZ0-77, groupId=sub-229-4OSEzZ0] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.560 [pool-82-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-229-4OSEzZ0-77, groupId=sub-229-4OSEzZ0] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.560 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.560 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.560 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043560
13:47:23.560 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-232-IkCxllw-78, groupId=sub-232-IkCxllw] Subscribed to topic(s): test-topic-0000232-L-U4evs
13:47:23.561 [pool-82-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-229-4OSEzZ0-77, groupId=sub-229-4OSEzZ0] (Re-)joining group
13:47:23.563 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-235-gPJ0DOc-79
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-235-gPJ0DOc
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.565 [pool-82-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-229-4OSEzZ0-77, groupId=sub-229-4OSEzZ0] Request joining group due to: need to re-join with the given member-id: consumer-sub-229-4OSEzZ0-77-d1f4b258-f440-49c9-acf9-93f414791f3c
13:47:23.565 [pool-82-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-229-4OSEzZ0-77, groupId=sub-229-4OSEzZ0] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.565 [pool-82-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-229-4OSEzZ0-77, groupId=sub-229-4OSEzZ0] (Re-)joining group
13:47:23.565 [pool-83-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-232-IkCxllw-78, groupId=sub-232-IkCxllw] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.565 [pool-83-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-232-IkCxllw-78, groupId=sub-232-IkCxllw] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.566 [pool-83-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-232-IkCxllw-78, groupId=sub-232-IkCxllw] (Re-)joining group
13:47:23.566 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.567 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.567 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043566
13:47:23.567 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-235-gPJ0DOc-79, groupId=sub-235-gPJ0DOc] Subscribed to topic(s): test-topic-0000235-daWCzN0
13:47:23.568 [pool-83-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-232-IkCxllw-78, groupId=sub-232-IkCxllw] Request joining group due to: need to re-join with the given member-id: consumer-sub-232-IkCxllw-78-048784b8-bf8d-4bc3-8938-7d9c1b3115eb
13:47:23.568 [pool-83-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-232-IkCxllw-78, groupId=sub-232-IkCxllw] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.568 [pool-83-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-232-IkCxllw-78, groupId=sub-232-IkCxllw] (Re-)joining group
13:47:23.568 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-238-M6fph0Y-80
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-238-M6fph0Y
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.571 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.571 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.571 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043571
13:47:23.571 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-238-M6fph0Y-80, groupId=sub-238-M6fph0Y] Subscribed to topic(s): test-topic-0000238-ATNTxQ8
13:47:23.572 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-241-5yLlWtg-81
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-241-5yLlWtg
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.574 [pool-85-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-238-M6fph0Y-80, groupId=sub-238-M6fph0Y] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.574 [pool-85-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-238-M6fph0Y-80, groupId=sub-238-M6fph0Y] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.575 [pool-84-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-235-gPJ0DOc-79, groupId=sub-235-gPJ0DOc] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.575 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.576 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.576 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043575
13:47:23.576 [pool-84-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-235-gPJ0DOc-79, groupId=sub-235-gPJ0DOc] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.576 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-241-5yLlWtg-81, groupId=sub-241-5yLlWtg] Subscribed to topic(s): test-topic-0000241-hsO-5Ig
13:47:23.577 [pool-85-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-238-M6fph0Y-80, groupId=sub-238-M6fph0Y] (Re-)joining group
13:47:23.580 [pool-84-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-235-gPJ0DOc-79, groupId=sub-235-gPJ0DOc] (Re-)joining group
13:47:23.579 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-244-Rz6I8TE-82
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-244-Rz6I8TE
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.581 [pool-84-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-235-gPJ0DOc-79, groupId=sub-235-gPJ0DOc] Request joining group due to: need to re-join with the given member-id: consumer-sub-235-gPJ0DOc-79-d0ccadd6-d1b9-4767-8c08-ef1cf9555634
13:47:23.581 [pool-84-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-235-gPJ0DOc-79, groupId=sub-235-gPJ0DOc] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.581 [pool-84-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-235-gPJ0DOc-79, groupId=sub-235-gPJ0DOc] (Re-)joining group
13:47:23.582 [pool-86-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-241-5yLlWtg-81, groupId=sub-241-5yLlWtg] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.582 [pool-86-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-241-5yLlWtg-81, groupId=sub-241-5yLlWtg] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.582 [pool-86-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-241-5yLlWtg-81, groupId=sub-241-5yLlWtg] (Re-)joining group
13:47:23.584 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.584 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.584 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043584
13:47:23.584 [pool-86-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-241-5yLlWtg-81, groupId=sub-241-5yLlWtg] Request joining group due to: need to re-join with the given member-id: consumer-sub-241-5yLlWtg-81-d29c9c83-a7a6-484a-9079-6d9ca60314c7
13:47:23.584 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-244-Rz6I8TE-82, groupId=sub-244-Rz6I8TE] Subscribed to topic(s): test-topic-0000244-CATmwfU
13:47:23.584 [pool-86-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-241-5yLlWtg-81, groupId=sub-241-5yLlWtg] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.584 [pool-86-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-241-5yLlWtg-81, groupId=sub-241-5yLlWtg] (Re-)joining group
13:47:23.585 [pool-85-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-238-M6fph0Y-80, groupId=sub-238-M6fph0Y] Request joining group due to: need to re-join with the given member-id: consumer-sub-238-M6fph0Y-80-d1a89b48-fa96-4a13-8b3c-067b46fb24a5
13:47:23.585 [pool-85-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-238-M6fph0Y-80, groupId=sub-238-M6fph0Y] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.585 [pool-85-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-238-M6fph0Y-80, groupId=sub-238-M6fph0Y] (Re-)joining group
13:47:23.586 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-247-HNv8ZAY-83
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-247-HNv8ZAY
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.588 [pool-87-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-244-Rz6I8TE-82, groupId=sub-244-Rz6I8TE] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.588 [pool-87-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-244-Rz6I8TE-82, groupId=sub-244-Rz6I8TE] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.589 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.589 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.589 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043589
13:47:23.590 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-247-HNv8ZAY-83, groupId=sub-247-HNv8ZAY] Subscribed to topic(s): test-topic-0000247-9BaoGEM
13:47:23.590 [pool-87-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-244-Rz6I8TE-82, groupId=sub-244-Rz6I8TE] (Re-)joining group
13:47:23.590 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-250-ioeMh9Q-84
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-250-ioeMh9Q
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.591 [pool-87-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-244-Rz6I8TE-82, groupId=sub-244-Rz6I8TE] Request joining group due to: need to re-join with the given member-id: consumer-sub-244-Rz6I8TE-82-ca019eff-81b3-482a-b392-0ab2bc3c4e50
13:47:23.592 [pool-87-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-244-Rz6I8TE-82, groupId=sub-244-Rz6I8TE] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.592 [pool-87-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-244-Rz6I8TE-82, groupId=sub-244-Rz6I8TE] (Re-)joining group
13:47:23.593 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.593 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.594 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043593
13:47:23.594 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-250-ioeMh9Q-84, groupId=sub-250-ioeMh9Q] Subscribed to topic(s): test-topic-0000250-hNzff68
13:47:23.595 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-253-TqYfbUA-85
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-253-TqYfbUA
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.596 [pool-88-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-247-HNv8ZAY-83, groupId=sub-247-HNv8ZAY] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.597 [pool-88-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-247-HNv8ZAY-83, groupId=sub-247-HNv8ZAY] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.598 [pool-88-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-247-HNv8ZAY-83, groupId=sub-247-HNv8ZAY] (Re-)joining group
13:47:23.598 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.598 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.598 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043598
13:47:23.598 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-253-TqYfbUA-85, groupId=sub-253-TqYfbUA] Subscribed to topic(s): test-topic-0000253-71qwhGg
13:47:23.600 [pool-88-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-247-HNv8ZAY-83, groupId=sub-247-HNv8ZAY] Request joining group due to: need to re-join with the given member-id: consumer-sub-247-HNv8ZAY-83-188c0ec5-6e3d-4e3b-90d5-582ab05b219c
13:47:23.600 [pool-88-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-247-HNv8ZAY-83, groupId=sub-247-HNv8ZAY] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.600 [pool-88-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-247-HNv8ZAY-83, groupId=sub-247-HNv8ZAY] (Re-)joining group
13:47:23.601 [pool-89-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-250-ioeMh9Q-84, groupId=sub-250-ioeMh9Q] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.602 [pool-89-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-250-ioeMh9Q-84, groupId=sub-250-ioeMh9Q] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.602 [pool-89-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-250-ioeMh9Q-84, groupId=sub-250-ioeMh9Q] (Re-)joining group
13:47:23.602 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-256-E_xLa20-86
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-256-E_xLa20
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.604 [pool-89-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-250-ioeMh9Q-84, groupId=sub-250-ioeMh9Q] Request joining group due to: need to re-join with the given member-id: consumer-sub-250-ioeMh9Q-84-e8af7339-16b8-48c2-ae14-d60dc2dbf685
13:47:23.604 [pool-89-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-250-ioeMh9Q-84, groupId=sub-250-ioeMh9Q] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.604 [pool-89-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-250-ioeMh9Q-84, groupId=sub-250-ioeMh9Q] (Re-)joining group
13:47:23.605 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.605 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.605 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043605
13:47:23.605 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-256-E_xLa20-86, groupId=sub-256-E_xLa20] Subscribed to topic(s): test-topic-0000256-GzCULh8
13:47:23.606 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-259-L0HKRLY-87
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-259-L0HKRLY
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.608 [pool-91-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-256-E_xLa20-86, groupId=sub-256-E_xLa20] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.608 [pool-91-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-256-E_xLa20-86, groupId=sub-256-E_xLa20] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.609 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.609 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.609 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043609
13:47:23.609 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-259-L0HKRLY-87, groupId=sub-259-L0HKRLY] Subscribed to topic(s): test-topic-0000259-x2gdERw
13:47:23.610 [pool-91-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-256-E_xLa20-86, groupId=sub-256-E_xLa20] (Re-)joining group
13:47:23.612 [pool-91-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-256-E_xLa20-86, groupId=sub-256-E_xLa20] Request joining group due to: need to re-join with the given member-id: consumer-sub-256-E_xLa20-86-cc3786d2-a15a-478e-ac9f-6e8e8b02aba0
13:47:23.612 [pool-91-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-256-E_xLa20-86, groupId=sub-256-E_xLa20] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.612 [pool-91-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-256-E_xLa20-86, groupId=sub-256-E_xLa20] (Re-)joining group
13:47:23.612 [pool-90-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-253-TqYfbUA-85, groupId=sub-253-TqYfbUA] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.612 [pool-90-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-253-TqYfbUA-85, groupId=sub-253-TqYfbUA] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.613 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-262-ZxwXbos-88
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-262-ZxwXbos
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.614 [pool-90-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-253-TqYfbUA-85, groupId=sub-253-TqYfbUA] (Re-)joining group
13:47:23.615 [pool-90-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-253-TqYfbUA-85, groupId=sub-253-TqYfbUA] Request joining group due to: need to re-join with the given member-id: consumer-sub-253-TqYfbUA-85-ede6a8e9-93f3-4ca5-be10-daed3eccb01d
13:47:23.615 [pool-90-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-253-TqYfbUA-85, groupId=sub-253-TqYfbUA] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.615 [pool-90-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-253-TqYfbUA-85, groupId=sub-253-TqYfbUA] (Re-)joining group
13:47:23.616 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.616 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.616 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043616
13:47:23.616 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-262-ZxwXbos-88, groupId=sub-262-ZxwXbos] Subscribed to topic(s): test-topic-0000262-Z_n6Lqs
13:47:23.619 [pool-92-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-259-L0HKRLY-87, groupId=sub-259-L0HKRLY] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.619 [pool-92-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-259-L0HKRLY-87, groupId=sub-259-L0HKRLY] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.621 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-265-kIY8JGg-89
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-265-kIY8JGg
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.623 [pool-93-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-262-ZxwXbos-88, groupId=sub-262-ZxwXbos] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.623 [pool-93-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-262-ZxwXbos-88, groupId=sub-262-ZxwXbos] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.624 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.624 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.624 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043624
13:47:23.624 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-265-kIY8JGg-89, groupId=sub-265-kIY8JGg] Subscribed to topic(s): test-topic-0000265-JPOr-Ag
13:47:23.626 [pool-92-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-259-L0HKRLY-87, groupId=sub-259-L0HKRLY] (Re-)joining group
13:47:23.626 [pool-93-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-262-ZxwXbos-88, groupId=sub-262-ZxwXbos] (Re-)joining group
13:47:23.626 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-268-K1hWqxg-90
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-268-K1hWqxg
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.628 [pool-93-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-262-ZxwXbos-88, groupId=sub-262-ZxwXbos] Request joining group due to: need to re-join with the given member-id: consumer-sub-262-ZxwXbos-88-60890567-6e8b-4ddd-8a35-7f1cb18c5f16
13:47:23.629 [pool-93-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-262-ZxwXbos-88, groupId=sub-262-ZxwXbos] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.629 [pool-93-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-262-ZxwXbos-88, groupId=sub-262-ZxwXbos] (Re-)joining group
13:47:23.629 [pool-94-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-265-kIY8JGg-89, groupId=sub-265-kIY8JGg] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.629 [pool-94-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-265-kIY8JGg-89, groupId=sub-265-kIY8JGg] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.630 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.630 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.630 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043630
13:47:23.630 [pool-94-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-265-kIY8JGg-89, groupId=sub-265-kIY8JGg] (Re-)joining group
13:47:23.630 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-268-K1hWqxg-90, groupId=sub-268-K1hWqxg] Subscribed to topic(s): test-topic-0000268-xHueAw8
13:47:23.631 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-271-Pnle5Mw-91
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-271-Pnle5Mw
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.632 [pool-94-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-265-kIY8JGg-89, groupId=sub-265-kIY8JGg] Request joining group due to: need to re-join with the given member-id: consumer-sub-265-kIY8JGg-89-cf137a34-2e0d-4fe5-a3b3-8fb0296e0b2c
13:47:23.632 [pool-94-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-265-kIY8JGg-89, groupId=sub-265-kIY8JGg] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.632 [pool-94-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-265-kIY8JGg-89, groupId=sub-265-kIY8JGg] (Re-)joining group
13:47:23.632 [pool-95-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-268-K1hWqxg-90, groupId=sub-268-K1hWqxg] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.633 [pool-95-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-268-K1hWqxg-90, groupId=sub-268-K1hWqxg] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.633 [pool-92-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-259-L0HKRLY-87, groupId=sub-259-L0HKRLY] Request joining group due to: need to re-join with the given member-id: consumer-sub-259-L0HKRLY-87-22ae2025-a840-4f97-9dc0-4fbc0b67d32f
13:47:23.633 [pool-92-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-259-L0HKRLY-87, groupId=sub-259-L0HKRLY] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.633 [pool-92-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-259-L0HKRLY-87, groupId=sub-259-L0HKRLY] (Re-)joining group
13:47:23.633 [pool-95-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-268-K1hWqxg-90, groupId=sub-268-K1hWqxg] (Re-)joining group
13:47:23.634 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.635 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.635 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043634
13:47:23.635 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-271-Pnle5Mw-91, groupId=sub-271-Pnle5Mw] Subscribed to topic(s): test-topic-0000271-ZxlpV1c
13:47:23.636 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-274-KCl7VaY-92
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-274-KCl7VaY
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.636 [pool-95-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-268-K1hWqxg-90, groupId=sub-268-K1hWqxg] Request joining group due to: need to re-join with the given member-id: consumer-sub-268-K1hWqxg-90-c4528907-9065-47e8-b45c-cf053c5075af
13:47:23.636 [pool-95-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-268-K1hWqxg-90, groupId=sub-268-K1hWqxg] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.636 [pool-95-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-268-K1hWqxg-90, groupId=sub-268-K1hWqxg] (Re-)joining group
13:47:23.637 [pool-96-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-271-Pnle5Mw-91, groupId=sub-271-Pnle5Mw] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.637 [pool-96-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-271-Pnle5Mw-91, groupId=sub-271-Pnle5Mw] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.638 [pool-96-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-271-Pnle5Mw-91, groupId=sub-271-Pnle5Mw] (Re-)joining group
13:47:23.639 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.639 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.639 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043639
13:47:23.639 [pool-96-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-271-Pnle5Mw-91, groupId=sub-271-Pnle5Mw] Request joining group due to: need to re-join with the given member-id: consumer-sub-271-Pnle5Mw-91-0f426a3c-b2f0-416f-a229-0c4e8a41f002
13:47:23.640 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-274-KCl7VaY-92, groupId=sub-274-KCl7VaY] Subscribed to topic(s): test-topic-0000274-WBszb4M
13:47:23.640 [pool-96-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-271-Pnle5Mw-91, groupId=sub-271-Pnle5Mw] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.640 [pool-96-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-271-Pnle5Mw-91, groupId=sub-271-Pnle5Mw] (Re-)joining group
13:47:23.641 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-277-Hee5c8Q-93
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-277-Hee5c8Q
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.644 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.644 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.644 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043644
13:47:23.644 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-277-Hee5c8Q-93, groupId=sub-277-Hee5c8Q] Subscribed to topic(s): test-topic-0000277-YF-_KfA
13:47:23.645 [pool-97-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-274-KCl7VaY-92, groupId=sub-274-KCl7VaY] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.645 [pool-97-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-274-KCl7VaY-92, groupId=sub-274-KCl7VaY] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.645 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-280-hP0y3fw-94
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-280-hP0y3fw
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.646 [pool-97-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-274-KCl7VaY-92, groupId=sub-274-KCl7VaY] (Re-)joining group
13:47:23.647 [pool-98-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-277-Hee5c8Q-93, groupId=sub-277-Hee5c8Q] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.647 [pool-98-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-277-Hee5c8Q-93, groupId=sub-277-Hee5c8Q] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.648 [pool-98-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-277-Hee5c8Q-93, groupId=sub-277-Hee5c8Q] (Re-)joining group
13:47:23.648 [pool-97-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-274-KCl7VaY-92, groupId=sub-274-KCl7VaY] Request joining group due to: need to re-join with the given member-id: consumer-sub-274-KCl7VaY-92-21462b45-169e-45d9-8a12-989df464c399
13:47:23.648 [pool-97-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-274-KCl7VaY-92, groupId=sub-274-KCl7VaY] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.648 [pool-97-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-274-KCl7VaY-92, groupId=sub-274-KCl7VaY] (Re-)joining group
13:47:23.649 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.649 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.649 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043649
13:47:23.649 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-280-hP0y3fw-94, groupId=sub-280-hP0y3fw] Subscribed to topic(s): test-topic-0000280-Y3fRkQ4
13:47:23.649 [pool-98-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-277-Hee5c8Q-93, groupId=sub-277-Hee5c8Q] Request joining group due to: need to re-join with the given member-id: consumer-sub-277-Hee5c8Q-93-8a3c9717-9b57-488f-a3a4-8b7b762cc78f
13:47:23.650 [pool-98-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-277-Hee5c8Q-93, groupId=sub-277-Hee5c8Q] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.650 [pool-98-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-277-Hee5c8Q-93, groupId=sub-277-Hee5c8Q] (Re-)joining group
13:47:23.650 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-283-gmHp3L8-95
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-283-gmHp3L8
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.652 [pool-99-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-280-hP0y3fw-94, groupId=sub-280-hP0y3fw] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.652 [pool-99-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-280-hP0y3fw-94, groupId=sub-280-hP0y3fw] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.653 [pool-99-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-280-hP0y3fw-94, groupId=sub-280-hP0y3fw] (Re-)joining group
13:47:23.653 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.653 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.653 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043653
13:47:23.654 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-283-gmHp3L8-95, groupId=sub-283-gmHp3L8] Subscribed to topic(s): test-topic-0000283-MdXNdPY
13:47:23.654 [pool-99-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-280-hP0y3fw-94, groupId=sub-280-hP0y3fw] Request joining group due to: need to re-join with the given member-id: consumer-sub-280-hP0y3fw-94-537fd25b-dfe1-42b7-869e-bd68a7ec44e1
13:47:23.654 [pool-99-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-280-hP0y3fw-94, groupId=sub-280-hP0y3fw] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.654 [pool-99-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-280-hP0y3fw-94, groupId=sub-280-hP0y3fw] (Re-)joining group
13:47:23.654 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-286-P82WLeA-96
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-286-P82WLeA
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.657 [pool-100-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-283-gmHp3L8-95, groupId=sub-283-gmHp3L8] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.657 [pool-100-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-283-gmHp3L8-95, groupId=sub-283-gmHp3L8] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.657 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.657 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.657 [pool-100-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-283-gmHp3L8-95, groupId=sub-283-gmHp3L8] (Re-)joining group
13:47:23.657 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043657
13:47:23.658 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-286-P82WLeA-96, groupId=sub-286-P82WLeA] Subscribed to topic(s): test-topic-0000286-yTITFyk
13:47:23.659 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-289-1YbAHy4-97
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-289-1YbAHy4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.659 [pool-100-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-283-gmHp3L8-95, groupId=sub-283-gmHp3L8] Request joining group due to: need to re-join with the given member-id: consumer-sub-283-gmHp3L8-95-4daacc85-50bd-4071-8d32-a365a85cd6fa
13:47:23.659 [pool-100-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-283-gmHp3L8-95, groupId=sub-283-gmHp3L8] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.659 [pool-100-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-283-gmHp3L8-95, groupId=sub-283-gmHp3L8] (Re-)joining group
13:47:23.662 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.662 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.662 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043662
13:47:23.662 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-289-1YbAHy4-97, groupId=sub-289-1YbAHy4] Subscribed to topic(s): test-topic-0000289-gnsmK30
13:47:23.663 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-292-7XbbrSY-98
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-292-7XbbrSY
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.663 [pool-101-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-286-P82WLeA-96, groupId=sub-286-P82WLeA] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.663 [pool-101-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-286-P82WLeA-96, groupId=sub-286-P82WLeA] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.665 [pool-101-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-286-P82WLeA-96, groupId=sub-286-P82WLeA] (Re-)joining group
13:47:23.666 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.666 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.666 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043666
13:47:23.666 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-292-7XbbrSY-98, groupId=sub-292-7XbbrSY] Subscribed to topic(s): test-topic-0000292-3RYn65c
13:47:23.667 [pool-101-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-286-P82WLeA-96, groupId=sub-286-P82WLeA] Request joining group due to: need to re-join with the given member-id: consumer-sub-286-P82WLeA-96-57b8524d-1b78-4194-b9ba-da58afa866e7
13:47:23.667 [pool-102-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-289-1YbAHy4-97, groupId=sub-289-1YbAHy4] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.667 [pool-101-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-286-P82WLeA-96, groupId=sub-286-P82WLeA] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.667 [pool-101-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-286-P82WLeA-96, groupId=sub-286-P82WLeA] (Re-)joining group
13:47:23.667 [pool-102-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-289-1YbAHy4-97, groupId=sub-289-1YbAHy4] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:23.668 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-295-aNnwtyo-99
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-295-aNnwtyo
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.670 [pool-103-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-292-7XbbrSY-98, groupId=sub-292-7XbbrSY] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.670 [pool-103-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-292-7XbbrSY-98, groupId=sub-292-7XbbrSY] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.671 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.671 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.671 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043671
13:47:23.671 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-295-aNnwtyo-99, groupId=sub-295-aNnwtyo] Subscribed to topic(s): test-topic-0000295-5y9DMI4
13:47:23.671 [pool-102-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-289-1YbAHy4-97, groupId=sub-289-1YbAHy4] (Re-)joining group
13:47:23.672 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-298-sz-MSPk-100
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-298-sz-MSPk
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.673 [pool-102-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-289-1YbAHy4-97, groupId=sub-289-1YbAHy4] Request joining group due to: need to re-join with the given member-id: consumer-sub-289-1YbAHy4-97-41bbc808-089b-4b83-a29d-7037a11e21b4
13:47:23.673 [pool-102-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-289-1YbAHy4-97, groupId=sub-289-1YbAHy4] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.673 [pool-102-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-289-1YbAHy4-97, groupId=sub-289-1YbAHy4] (Re-)joining group
13:47:23.675 [pool-103-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-292-7XbbrSY-98, groupId=sub-292-7XbbrSY] (Re-)joining group
13:47:23.676 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.676 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.676 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043676
13:47:23.676 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-298-sz-MSPk-100, groupId=sub-298-sz-MSPk] Subscribed to topic(s): test-topic-0000298-cXyRfIQ
13:47:23.677 [qtp435803541-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-301-kp9JcWM-101
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-301-kp9JcWM
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:23.677 [pool-103-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-292-7XbbrSY-98, groupId=sub-292-7XbbrSY] Request joining group due to: need to re-join with the given member-id: consumer-sub-292-7XbbrSY-98-11471e5d-bb7e-4a18-a817-168eca2a6a2e
13:47:23.677 [pool-103-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-292-7XbbrSY-98, groupId=sub-292-7XbbrSY] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.677 [pool-103-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-292-7XbbrSY-98, groupId=sub-292-7XbbrSY] (Re-)joining group
13:47:23.680 [qtp435803541-28] INFO AppInfoParser - Kafka version: 3.6.1
13:47:23.680 [qtp435803541-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:23.680 [qtp435803541-28] INFO AppInfoParser - Kafka startTimeMs: 1717336043680
13:47:23.680 [qtp435803541-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-301-kp9JcWM-101, groupId=sub-301-kp9JcWM] Subscribed to topic(s): test-topic-0000301-j62eWV0
13:47:23.681 [pool-104-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-295-aNnwtyo-99, groupId=sub-295-aNnwtyo] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.682 [pool-104-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-295-aNnwtyo-99, groupId=sub-295-aNnwtyo] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.682 [pool-104-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-295-aNnwtyo-99, groupId=sub-295-aNnwtyo] (Re-)joining group
13:47:23.684 [pool-105-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-298-sz-MSPk-100, groupId=sub-298-sz-MSPk] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.684 [pool-105-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-298-sz-MSPk-100, groupId=sub-298-sz-MSPk] Discovered group coordinator 10.0.0.52:9092 (id: 2147483646 rack: null)
13:47:23.685 [pool-105-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-298-sz-MSPk-100, groupId=sub-298-sz-MSPk] (Re-)joining group
13:47:23.685 [pool-104-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-295-aNnwtyo-99, groupId=sub-295-aNnwtyo] Request joining group due to: need to re-join with the given member-id: consumer-sub-295-aNnwtyo-99-d2f9e3f6-5893-4c6d-99c7-315fa5a2be0f
13:47:23.685 [pool-104-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-295-aNnwtyo-99, groupId=sub-295-aNnwtyo] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.685 [pool-104-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-295-aNnwtyo-99, groupId=sub-295-aNnwtyo] (Re-)joining group
13:47:23.687 [pool-106-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-301-kp9JcWM-101, groupId=sub-301-kp9JcWM] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:23.687 [pool-105-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-298-sz-MSPk-100, groupId=sub-298-sz-MSPk] Request joining group due to: need to re-join with the given member-id: consumer-sub-298-sz-MSPk-100-fe99ab57-3f3b-4580-928e-35f4819cc7b8
13:47:23.687 [pool-106-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-301-kp9JcWM-101, groupId=sub-301-kp9JcWM] Discovered group coordinator 10.0.0.216:9092 (id: 2147483647 rack: null)
13:47:23.687 [pool-105-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-298-sz-MSPk-100, groupId=sub-298-sz-MSPk] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.687 [pool-105-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-298-sz-MSPk-100, groupId=sub-298-sz-MSPk] (Re-)joining group
13:47:23.687 [pool-106-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-301-kp9JcWM-101, groupId=sub-301-kp9JcWM] (Re-)joining group
13:47:23.689 [pool-106-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-301-kp9JcWM-101, groupId=sub-301-kp9JcWM] Request joining group due to: need to re-join with the given member-id: consumer-sub-301-kp9JcWM-101-2ad10c95-d5eb-43e5-9c4a-b66cc6f0f1ba
13:47:23.689 [pool-106-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-301-kp9JcWM-101, groupId=sub-301-kp9JcWM] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:23.689 [pool-106-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-301-kp9JcWM-101, groupId=sub-301-kp9JcWM] (Re-)joining group
13:47:23.694 [qtp435803541-28] INFO LocalWorker - Created 101 consumers in 687.31006 ms
13:47:25.176 [main] INFO WorkloadGenerator - Created 304 external consumers in 2188.866235 ms
13:47:25.187 [main] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-000-H7fWDVE-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-000-H7fWDVE
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:47:25.226 [main] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.226 [main] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.227 [main] INFO AppInfoParser - Kafka startTimeMs: 1717336045226
13:47:25.229 [main] INFO KafkaConsumer - [Consumer clientId=consumer-sub-000-H7fWDVE-1, groupId=sub-000-H7fWDVE] Subscribed to topic(s): test-topic-0000000-FGdYwL4
13:47:25.233 [main] INFO LocalWorker - Created 1 consumers in 56.115811 ms
13:47:25.244 [pool-6-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-000-H7fWDVE-1, groupId=sub-000-H7fWDVE] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.245 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000-H7fWDVE-1, groupId=sub-000-H7fWDVE] Discovered group coordinator 10.0.0.229:9092 (id: 2147483645 rack: null)
13:47:25.248 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000-H7fWDVE-1, groupId=sub-000-H7fWDVE] (Re-)joining group
13:47:25.255 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.258 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-1] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.262 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000-H7fWDVE-1, groupId=sub-000-H7fWDVE] Request joining group due to: need to re-join with the given member-id: consumer-sub-000-H7fWDVE-1-23576a1d-c3dc-422d-a9fa-c634917e05a4
13:47:25.262 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000-H7fWDVE-1, groupId=sub-000-H7fWDVE] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
13:47:25.263 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000-H7fWDVE-1, groupId=sub-000-H7fWDVE] (Re-)joining group
13:47:25.266 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
13:47:25.277 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.278 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.278 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.278 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045277
13:47:25.278 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.279 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-2] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.279 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-2] Instantiated an idempotent producer.
13:47:25.281 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.281 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.281 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.281 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045281
13:47:25.281 [kafka-producer-network-thread | producer-1] INFO Metadata - [Producer clientId=producer-1] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.282 [kafka-producer-network-thread | producer-1] INFO TransactionManager - [Producer clientId=producer-1] ProducerId set to 14104 with epoch 0
13:47:25.282 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.282 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-3] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.283 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-3] Instantiated an idempotent producer.
13:47:25.283 [kafka-producer-network-thread | producer-2] INFO Metadata - [Producer clientId=producer-2] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.283 [kafka-producer-network-thread | producer-2] INFO TransactionManager - [Producer clientId=producer-2] ProducerId set to 12203 with epoch 0
13:47:25.285 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.285 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.285 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.285 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045285
13:47:25.286 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.286 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-4] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.286 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-4] Instantiated an idempotent producer.
13:47:25.287 [kafka-producer-network-thread | producer-3] INFO Metadata - [Producer clientId=producer-3] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.287 [kafka-producer-network-thread | producer-3] INFO TransactionManager - [Producer clientId=producer-3] ProducerId set to 13203 with epoch 0
13:47:25.289 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.289 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.289 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.289 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045289
13:47:25.290 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.290 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-5] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.290 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-5] Instantiated an idempotent producer.
13:47:25.291 [kafka-producer-network-thread | producer-4] INFO Metadata - [Producer clientId=producer-4] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.291 [kafka-producer-network-thread | producer-4] INFO TransactionManager - [Producer clientId=producer-4] ProducerId set to 13205 with epoch 0
13:47:25.293 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.293 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.293 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.293 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045293
13:47:25.293 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.294 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-6] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.294 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-6] Instantiated an idempotent producer.
13:47:25.294 [kafka-producer-network-thread | producer-5] INFO Metadata - [Producer clientId=producer-5] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.294 [kafka-producer-network-thread | producer-5] INFO TransactionManager - [Producer clientId=producer-5] ProducerId set to 12206 with epoch 0
13:47:25.296 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.296 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.296 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.297 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045296
13:47:25.297 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.298 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-7] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.298 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-7] Instantiated an idempotent producer.
13:47:25.298 [kafka-producer-network-thread | producer-6] INFO Metadata - [Producer clientId=producer-6] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.298 [kafka-producer-network-thread | producer-6] INFO TransactionManager - [Producer clientId=producer-6] ProducerId set to 14108 with epoch 0
13:47:25.300 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.300 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.300 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.300 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045300
13:47:25.301 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.301 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-8] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.301 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-8] Instantiated an idempotent producer.
13:47:25.302 [kafka-producer-network-thread | producer-7] INFO Metadata - [Producer clientId=producer-7] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.302 [kafka-producer-network-thread | producer-7] INFO TransactionManager - [Producer clientId=producer-7] ProducerId set to 14109 with epoch 0
13:47:25.303 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.304 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.304 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.304 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045304
13:47:25.304 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.305 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-9] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.305 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-9] Instantiated an idempotent producer.
13:47:25.305 [kafka-producer-network-thread | producer-8] INFO Metadata - [Producer clientId=producer-8] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.306 [kafka-producer-network-thread | producer-8] INFO TransactionManager - [Producer clientId=producer-8] ProducerId set to 13208 with epoch 0
13:47:25.307 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.307 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.307 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.307 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045307
13:47:25.308 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.308 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-10] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.308 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-10] Instantiated an idempotent producer.
13:47:25.309 [kafka-producer-network-thread | producer-9] INFO Metadata - [Producer clientId=producer-9] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.309 [kafka-producer-network-thread | producer-9] INFO TransactionManager - [Producer clientId=producer-9] ProducerId set to 13209 with epoch 0
13:47:25.311 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.311 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.311 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.311 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045311
13:47:25.311 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.312 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-11] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.312 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-11] Instantiated an idempotent producer.
13:47:25.312 [kafka-producer-network-thread | producer-10] INFO Metadata - [Producer clientId=producer-10] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.312 [kafka-producer-network-thread | producer-10] INFO TransactionManager - [Producer clientId=producer-10] ProducerId set to 12210 with epoch 0
13:47:25.314 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.314 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.314 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.314 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045314
13:47:25.315 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.315 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-12] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.315 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-12] Instantiated an idempotent producer.
13:47:25.315 [kafka-producer-network-thread | producer-11] INFO Metadata - [Producer clientId=producer-11] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.316 [kafka-producer-network-thread | producer-11] INFO TransactionManager - [Producer clientId=producer-11] ProducerId set to 14115 with epoch 0
13:47:25.317 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.317 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.317 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.317 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045317
13:47:25.318 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-13
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.319 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-13] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.319 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-13] Instantiated an idempotent producer.
13:47:25.319 [kafka-producer-network-thread | producer-12] INFO Metadata - [Producer clientId=producer-12] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.319 [kafka-producer-network-thread | producer-12] INFO TransactionManager - [Producer clientId=producer-12] ProducerId set to 14116 with epoch 0
13:47:25.321 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.321 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.321 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.321 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045321
13:47:25.322 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-14
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.322 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-14] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.322 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-14] Instantiated an idempotent producer.
13:47:25.323 [kafka-producer-network-thread | producer-13] INFO Metadata - [Producer clientId=producer-13] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.323 [kafka-producer-network-thread | producer-13] INFO TransactionManager - [Producer clientId=producer-13] ProducerId set to 13215 with epoch 0
13:47:25.324 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.324 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.324 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.324 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045324
13:47:25.325 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-15
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.326 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-15] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.326 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-15] Instantiated an idempotent producer.
13:47:25.326 [kafka-producer-network-thread | producer-14] INFO Metadata - [Producer clientId=producer-14] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.326 [kafka-producer-network-thread | producer-14] INFO TransactionManager - [Producer clientId=producer-14] ProducerId set to 12212 with epoch 0
13:47:25.328 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.328 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.328 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.328 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045328
13:47:25.329 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-16
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.329 [kafka-producer-network-thread | producer-15] INFO Metadata - [Producer clientId=producer-15] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.330 [kafka-producer-network-thread | producer-15] INFO TransactionManager - [Producer clientId=producer-15] ProducerId set to 12214 with epoch 0
13:47:25.330 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-16] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.330 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-16] Instantiated an idempotent producer.
13:47:25.332 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.332 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.332 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.332 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045332
13:47:25.333 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-17
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.333 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-17] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.333 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-17] Instantiated an idempotent producer.
13:47:25.336 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.336 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.336 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.336 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045336
13:47:25.336 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-18
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.337 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-18] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.337 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-18] Instantiated an idempotent producer.
13:47:25.338 [kafka-producer-network-thread | producer-17] INFO Metadata - [Producer clientId=producer-17] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.338 [kafka-producer-network-thread | producer-16] INFO Metadata - [Producer clientId=producer-16] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.338 [kafka-producer-network-thread | producer-17] INFO TransactionManager - [Producer clientId=producer-17] ProducerId set to 13219 with epoch 0
13:47:25.338 [kafka-producer-network-thread | producer-16] INFO TransactionManager - [Producer clientId=producer-16] ProducerId set to 13220 with epoch 0
13:47:25.339 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.339 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.339 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.339 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045339
13:47:25.340 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-19
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.340 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-19] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.340 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-19] Instantiated an idempotent producer.
13:47:25.341 [kafka-producer-network-thread | producer-18] INFO Metadata - [Producer clientId=producer-18] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.341 [kafka-producer-network-thread | producer-18] INFO TransactionManager - [Producer clientId=producer-18] ProducerId set to 12216 with epoch 0
13:47:25.342 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.342 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.342 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.342 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045342
13:47:25.343 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-20
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.343 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-20] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.343 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-20] Instantiated an idempotent producer.
13:47:25.344 [kafka-producer-network-thread | producer-19] INFO Metadata - [Producer clientId=producer-19] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.344 [kafka-producer-network-thread | producer-19] INFO TransactionManager - [Producer clientId=producer-19] ProducerId set to 14123 with epoch 0
13:47:25.345 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.345 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.345 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.345 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045345
13:47:25.345 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-21
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.346 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-21] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.346 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-21] Instantiated an idempotent producer.
13:47:25.347 [kafka-producer-network-thread | producer-20] INFO Metadata - [Producer clientId=producer-20] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.347 [kafka-producer-network-thread | producer-20] INFO TransactionManager - [Producer clientId=producer-20] ProducerId set to 13223 with epoch 0
13:47:25.347 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.348 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.348 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.348 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045348
13:47:25.348 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-22
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.348 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-22] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.348 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-22] Instantiated an idempotent producer.
13:47:25.349 [kafka-producer-network-thread | producer-21] INFO Metadata - [Producer clientId=producer-21] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.349 [kafka-producer-network-thread | producer-21] INFO TransactionManager - [Producer clientId=producer-21] ProducerId set to 14124 with epoch 0
13:47:25.350 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.350 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.350 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.350 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045350
13:47:25.350 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-23
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.351 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-23] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.351 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-23] Instantiated an idempotent producer.
13:47:25.352 [kafka-producer-network-thread | producer-22] INFO Metadata - [Producer clientId=producer-22] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.352 [kafka-producer-network-thread | producer-22] INFO TransactionManager - [Producer clientId=producer-22] ProducerId set to 13226 with epoch 0
13:47:25.352 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.353 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.353 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.353 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045353
13:47:25.353 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-24
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.354 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-24] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.354 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-24] Instantiated an idempotent producer.
13:47:25.354 [kafka-producer-network-thread | producer-23] INFO Metadata - [Producer clientId=producer-23] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.355 [kafka-producer-network-thread | producer-23] INFO TransactionManager - [Producer clientId=producer-23] ProducerId set to 13228 with epoch 0
13:47:25.355 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.355 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.355 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.355 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045355
13:47:25.356 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-25
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.356 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-25] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.356 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-25] Instantiated an idempotent producer.
13:47:25.358 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.358 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.358 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.358 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045358
13:47:25.358 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-26
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.358 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-26] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.359 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-26] Instantiated an idempotent producer.
13:47:25.359 [kafka-producer-network-thread | producer-25] INFO Metadata - [Producer clientId=producer-25] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.359 [kafka-producer-network-thread | producer-25] INFO TransactionManager - [Producer clientId=producer-25] ProducerId set to 13231 with epoch 0
13:47:25.360 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.360 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.360 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.360 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045360
13:47:25.361 [kafka-producer-network-thread | producer-24] INFO Metadata - [Producer clientId=producer-24] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.361 [kafka-producer-network-thread | producer-24] INFO TransactionManager - [Producer clientId=producer-24] ProducerId set to 14125 with epoch 0
13:47:25.361 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-27
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.362 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-27] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.362 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-27] Instantiated an idempotent producer.
13:47:25.363 [kafka-producer-network-thread | producer-26] INFO Metadata - [Producer clientId=producer-26] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.363 [kafka-producer-network-thread | producer-26] INFO TransactionManager - [Producer clientId=producer-26] ProducerId set to 14126 with epoch 0
13:47:25.363 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.364 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.364 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.364 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045364
13:47:25.364 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-28
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.365 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-28] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.365 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-28] Instantiated an idempotent producer.
13:47:25.365 [kafka-producer-network-thread | producer-27] INFO Metadata - [Producer clientId=producer-27] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.365 [kafka-producer-network-thread | producer-27] INFO TransactionManager - [Producer clientId=producer-27] ProducerId set to 12220 with epoch 0
13:47:25.366 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.366 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.366 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.366 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045366
13:47:25.367 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-29
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.367 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-29] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.367 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-29] Instantiated an idempotent producer.
13:47:25.368 [kafka-producer-network-thread | producer-28] INFO Metadata - [Producer clientId=producer-28] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.368 [kafka-producer-network-thread | producer-28] INFO TransactionManager - [Producer clientId=producer-28] ProducerId set to 12221 with epoch 0
13:47:25.369 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.369 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.369 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.369 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045369
13:47:25.369 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-30
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.370 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-30] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.370 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-30] Instantiated an idempotent producer.
13:47:25.371 [kafka-producer-network-thread | producer-29] INFO Metadata - [Producer clientId=producer-29] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.371 [kafka-producer-network-thread | producer-29] INFO TransactionManager - [Producer clientId=producer-29] ProducerId set to 14129 with epoch 0
13:47:25.371 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.372 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.372 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.372 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045371
13:47:25.372 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-31
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.373 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-31] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.373 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-31] Instantiated an idempotent producer.
13:47:25.374 [kafka-producer-network-thread | producer-30] INFO Metadata - [Producer clientId=producer-30] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.374 [kafka-producer-network-thread | producer-30] INFO TransactionManager - [Producer clientId=producer-30] ProducerId set to 12223 with epoch 0
13:47:25.374 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.374 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.374 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.374 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045374
13:47:25.375 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-32
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.375 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-32] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.375 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-32] Instantiated an idempotent producer.
13:47:25.376 [kafka-producer-network-thread | producer-31] INFO Metadata - [Producer clientId=producer-31] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.376 [kafka-producer-network-thread | producer-31] INFO TransactionManager - [Producer clientId=producer-31] ProducerId set to 14131 with epoch 0
13:47:25.377 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.377 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.377 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.377 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045377
13:47:25.377 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-33
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.378 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-33] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.378 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-33] Instantiated an idempotent producer.
13:47:25.379 [kafka-producer-network-thread | producer-32] INFO Metadata - [Producer clientId=producer-32] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.379 [kafka-producer-network-thread | producer-32] INFO TransactionManager - [Producer clientId=producer-32] ProducerId set to 14132 with epoch 0
13:47:25.380 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.380 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.380 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.380 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045380
13:47:25.380 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-34
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.381 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-34] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.381 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-34] Instantiated an idempotent producer.
13:47:25.381 [kafka-producer-network-thread | producer-33] INFO Metadata - [Producer clientId=producer-33] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.381 [kafka-producer-network-thread | producer-33] INFO TransactionManager - [Producer clientId=producer-33] ProducerId set to 14133 with epoch 0
13:47:25.382 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.383 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.383 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.383 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045383
13:47:25.383 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-35
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.384 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-35] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.384 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-35] Instantiated an idempotent producer.
13:47:25.384 [kafka-producer-network-thread | producer-34] INFO Metadata - [Producer clientId=producer-34] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.385 [kafka-producer-network-thread | producer-34] INFO TransactionManager - [Producer clientId=producer-34] ProducerId set to 13237 with epoch 0
13:47:25.386 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.386 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.386 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.386 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045386
13:47:25.386 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-36
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.387 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-36] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.387 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-36] Instantiated an idempotent producer.
13:47:25.388 [kafka-producer-network-thread | producer-35] INFO Metadata - [Producer clientId=producer-35] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.388 [kafka-producer-network-thread | producer-35] INFO TransactionManager - [Producer clientId=producer-35] ProducerId set to 14137 with epoch 0
13:47:25.388 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.388 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.389 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.389 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045388
13:47:25.389 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-37
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.390 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-37] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.390 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-37] Instantiated an idempotent producer.
13:47:25.390 [kafka-producer-network-thread | producer-36] INFO Metadata - [Producer clientId=producer-36] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.390 [kafka-producer-network-thread | producer-36] INFO TransactionManager - [Producer clientId=producer-36] ProducerId set to 14138 with epoch 0
13:47:25.391 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.391 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.391 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.391 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045391
13:47:25.392 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-38
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.392 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-38] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.392 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-38] Instantiated an idempotent producer.
13:47:25.393 [kafka-producer-network-thread | producer-37] INFO Metadata - [Producer clientId=producer-37] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.393 [kafka-producer-network-thread | producer-37] INFO TransactionManager - [Producer clientId=producer-37] ProducerId set to 14141 with epoch 0
13:47:25.394 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.394 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.394 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.394 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045394
13:47:25.395 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-39
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.395 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-39] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.395 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-39] Instantiated an idempotent producer.
13:47:25.396 [kafka-producer-network-thread | producer-38] INFO Metadata - [Producer clientId=producer-38] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.396 [kafka-producer-network-thread | producer-38] INFO TransactionManager - [Producer clientId=producer-38] ProducerId set to 14142 with epoch 0
13:47:25.397 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.397 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.397 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.397 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045397
13:47:25.398 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-40
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.398 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-40] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.398 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-40] Instantiated an idempotent producer.
13:47:25.399 [kafka-producer-network-thread | producer-39] INFO Metadata - [Producer clientId=producer-39] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.399 [kafka-producer-network-thread | producer-39] INFO TransactionManager - [Producer clientId=producer-39] ProducerId set to 14143 with epoch 0
13:47:25.400 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.400 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.400 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.400 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045400
13:47:25.400 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-41
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.401 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-41] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.401 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-41] Instantiated an idempotent producer.
13:47:25.401 [kafka-producer-network-thread | producer-40] INFO Metadata - [Producer clientId=producer-40] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.402 [kafka-producer-network-thread | producer-40] INFO TransactionManager - [Producer clientId=producer-40] ProducerId set to 14144 with epoch 0
13:47:25.402 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.403 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.403 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.403 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045403
13:47:25.403 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-42
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.403 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-42] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.403 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-42] Instantiated an idempotent producer.
13:47:25.404 [kafka-producer-network-thread | producer-41] INFO Metadata - [Producer clientId=producer-41] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.404 [kafka-producer-network-thread | producer-41] INFO TransactionManager - [Producer clientId=producer-41] ProducerId set to 12228 with epoch 0
13:47:25.405 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.405 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.405 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.405 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045405
13:47:25.405 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-43
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.406 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-43] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.406 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-43] Instantiated an idempotent producer.
13:47:25.407 [kafka-producer-network-thread | producer-42] INFO Metadata - [Producer clientId=producer-42] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.407 [kafka-producer-network-thread | producer-42] INFO TransactionManager - [Producer clientId=producer-42] ProducerId set to 14147 with epoch 0
13:47:25.408 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.408 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.408 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.408 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045408
13:47:25.408 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-44
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.409 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-44] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.409 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-44] Instantiated an idempotent producer.
13:47:25.410 [kafka-producer-network-thread | producer-43] INFO Metadata - [Producer clientId=producer-43] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.410 [kafka-producer-network-thread | producer-43] INFO TransactionManager - [Producer clientId=producer-43] ProducerId set to 12231 with epoch 0
13:47:25.410 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.410 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.410 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.410 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045410
13:47:25.411 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-45
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.411 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-45] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.411 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-45] Instantiated an idempotent producer.
13:47:25.413 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.413 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.413 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.413 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045413
13:47:25.414 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-46
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.414 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-46] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.415 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-46] Instantiated an idempotent producer.
13:47:25.415 [kafka-producer-network-thread | producer-44] INFO Metadata - [Producer clientId=producer-44] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.415 [kafka-producer-network-thread | producer-44] INFO TransactionManager - [Producer clientId=producer-44] ProducerId set to 13245 with epoch 0
13:47:25.415 [kafka-producer-network-thread | producer-45] INFO Metadata - [Producer clientId=producer-45] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.415 [kafka-producer-network-thread | producer-45] INFO TransactionManager - [Producer clientId=producer-45] ProducerId set to 13246 with epoch 0
13:47:25.416 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.416 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.416 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.416 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045416
13:47:25.417 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-47
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.417 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-47] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.417 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-47] Instantiated an idempotent producer.
13:47:25.418 [kafka-producer-network-thread | producer-46] INFO Metadata - [Producer clientId=producer-46] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.418 [kafka-producer-network-thread | producer-46] INFO TransactionManager - [Producer clientId=producer-46] ProducerId set to 13249 with epoch 0
13:47:25.419 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.419 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.419 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.419 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045419
13:47:25.419 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-48
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.419 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-48] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.420 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-48] Instantiated an idempotent producer.
13:47:25.420 [kafka-producer-network-thread | producer-47] INFO Metadata - [Producer clientId=producer-47] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.420 [kafka-producer-network-thread | producer-47] INFO TransactionManager - [Producer clientId=producer-47] ProducerId set to 12234 with epoch 0
13:47:25.421 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.421 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.421 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.421 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045421
13:47:25.422 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-49
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.422 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-49] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.422 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-49] Instantiated an idempotent producer.
13:47:25.423 [kafka-producer-network-thread | producer-48] INFO Metadata - [Producer clientId=producer-48] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.423 [kafka-producer-network-thread | producer-48] INFO TransactionManager - [Producer clientId=producer-48] ProducerId set to 14151 with epoch 0
13:47:25.424 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.424 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.424 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.424 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045424
13:47:25.424 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-50
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.424 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-50] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.425 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-50] Instantiated an idempotent producer.
13:47:25.426 [kafka-producer-network-thread | producer-49] INFO Metadata - [Producer clientId=producer-49] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.426 [kafka-producer-network-thread | producer-49] INFO TransactionManager - [Producer clientId=producer-49] ProducerId set to 14152 with epoch 0
13:47:25.426 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.426 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.426 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.426 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045426
13:47:25.427 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-51
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.427 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-51] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.427 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-51] Instantiated an idempotent producer.
13:47:25.428 [kafka-producer-network-thread | producer-50] INFO Metadata - [Producer clientId=producer-50] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.428 [kafka-producer-network-thread | producer-50] INFO TransactionManager - [Producer clientId=producer-50] ProducerId set to 13252 with epoch 0
13:47:25.429 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.429 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.429 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.429 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045429
13:47:25.429 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-52
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.430 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-52] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.430 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-52] Instantiated an idempotent producer.
13:47:25.430 [kafka-producer-network-thread | producer-51] INFO Metadata - [Producer clientId=producer-51] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.430 [kafka-producer-network-thread | producer-51] INFO TransactionManager - [Producer clientId=producer-51] ProducerId set to 13253 with epoch 0
13:47:25.431 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.431 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.432 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.432 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045431
13:47:25.432 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-53
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.433 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-53] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.433 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-53] Instantiated an idempotent producer.
13:47:25.433 [kafka-producer-network-thread | producer-52] INFO Metadata - [Producer clientId=producer-52] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.434 [kafka-producer-network-thread | producer-52] INFO TransactionManager - [Producer clientId=producer-52] ProducerId set to 14154 with epoch 0
13:47:25.434 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.435 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.435 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.435 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045434
13:47:25.435 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-54
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.436 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-54] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.436 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-54] Instantiated an idempotent producer.
13:47:25.436 [kafka-producer-network-thread | producer-53] INFO Metadata - [Producer clientId=producer-53] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.436 [kafka-producer-network-thread | producer-53] INFO TransactionManager - [Producer clientId=producer-53] ProducerId set to 14156 with epoch 0
13:47:25.448 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.448 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.448 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.448 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045448
13:47:25.449 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-55
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.449 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-55] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.450 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-55] Instantiated an idempotent producer.
13:47:25.450 [kafka-producer-network-thread | producer-54] INFO Metadata - [Producer clientId=producer-54] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.450 [kafka-producer-network-thread | producer-54] INFO TransactionManager - [Producer clientId=producer-54] ProducerId set to 13258 with epoch 0
13:47:25.451 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.451 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.451 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.451 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045451
13:47:25.452 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-56
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.453 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-56] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.453 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-56] Instantiated an idempotent producer.
13:47:25.453 [kafka-producer-network-thread | producer-55] INFO Metadata - [Producer clientId=producer-55] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.454 [kafka-producer-network-thread | producer-55] INFO TransactionManager - [Producer clientId=producer-55] ProducerId set to 12242 with epoch 0
13:47:25.454 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.454 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.454 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.454 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045454
13:47:25.455 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-57
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.455 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-57] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.455 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-57] Instantiated an idempotent producer.
13:47:25.456 [kafka-producer-network-thread | producer-56] INFO Metadata - [Producer clientId=producer-56] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.456 [kafka-producer-network-thread | producer-56] INFO TransactionManager - [Producer clientId=producer-56] ProducerId set to 12243 with epoch 0
13:47:25.457 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.457 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.457 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.457 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045457
13:47:25.458 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-58
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.458 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-58] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.458 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-58] Instantiated an idempotent producer.
13:47:25.459 [kafka-producer-network-thread | producer-57] INFO Metadata - [Producer clientId=producer-57] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.459 [kafka-producer-network-thread | producer-57] INFO TransactionManager - [Producer clientId=producer-57] ProducerId set to 14160 with epoch 0
13:47:25.460 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.460 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.460 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.460 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045460
13:47:25.460 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-59
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.461 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-59] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.461 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-59] Instantiated an idempotent producer.
13:47:25.461 [kafka-producer-network-thread | producer-58] INFO Metadata - [Producer clientId=producer-58] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.462 [kafka-producer-network-thread | producer-58] INFO TransactionManager - [Producer clientId=producer-58] ProducerId set to 12245 with epoch 0
13:47:25.462 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.462 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.462 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.462 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045462
13:47:25.463 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-60
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.463 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-60] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.463 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-60] Instantiated an idempotent producer.
13:47:25.464 [kafka-producer-network-thread | producer-59] INFO Metadata - [Producer clientId=producer-59] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.464 [kafka-producer-network-thread | producer-59] INFO TransactionManager - [Producer clientId=producer-59] ProducerId set to 12246 with epoch 0
13:47:25.465 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.465 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.465 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.465 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045465
13:47:25.465 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-61
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.466 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-61] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.466 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-61] Instantiated an idempotent producer.
13:47:25.467 [kafka-producer-network-thread | producer-60] INFO Metadata - [Producer clientId=producer-60] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.467 [kafka-producer-network-thread | producer-60] INFO TransactionManager - [Producer clientId=producer-60] ProducerId set to 13262 with epoch 0
13:47:25.468 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.468 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.468 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.468 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045468
13:47:25.468 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-62
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.469 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-62] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.469 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-62] Instantiated an idempotent producer.
13:47:25.470 [kafka-producer-network-thread | producer-61] INFO Metadata - [Producer clientId=producer-61] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.470 [kafka-producer-network-thread | producer-61] INFO TransactionManager - [Producer clientId=producer-61] ProducerId set to 13263 with epoch 0
13:47:25.471 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.471 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.471 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.471 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045471
13:47:25.472 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-63
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.472 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-63] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.472 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-63] Instantiated an idempotent producer.
13:47:25.473 [kafka-producer-network-thread | producer-62] INFO Metadata - [Producer clientId=producer-62] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.473 [kafka-producer-network-thread | producer-62] INFO TransactionManager - [Producer clientId=producer-62] ProducerId set to 12251 with epoch 0
13:47:25.473 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.473 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.474 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.474 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045473
13:47:25.474 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-64
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.475 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-64] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.475 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-64] Instantiated an idempotent producer.
13:47:25.475 [kafka-producer-network-thread | producer-63] INFO Metadata - [Producer clientId=producer-63] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.476 [kafka-producer-network-thread | producer-63] INFO TransactionManager - [Producer clientId=producer-63] ProducerId set to 14167 with epoch 0
13:47:25.476 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.476 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.476 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.476 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045476
13:47:25.477 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-65
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.477 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-65] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.477 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-65] Instantiated an idempotent producer.
13:47:25.478 [kafka-producer-network-thread | producer-64] INFO Metadata - [Producer clientId=producer-64] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.478 [kafka-producer-network-thread | producer-64] INFO TransactionManager - [Producer clientId=producer-64] ProducerId set to 13265 with epoch 0
13:47:25.479 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.479 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.479 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.479 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045479
13:47:25.480 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-66
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.480 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-66] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.480 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-66] Instantiated an idempotent producer.
13:47:25.481 [kafka-producer-network-thread | producer-65] INFO Metadata - [Producer clientId=producer-65] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.481 [kafka-producer-network-thread | producer-65] INFO TransactionManager - [Producer clientId=producer-65] ProducerId set to 12255 with epoch 0
13:47:25.482 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.482 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.482 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.482 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045482
13:47:25.482 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-67
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.483 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-67] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.483 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-67] Instantiated an idempotent producer.
13:47:25.483 [kafka-producer-network-thread | producer-66] INFO Metadata - [Producer clientId=producer-66] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.483 [kafka-producer-network-thread | producer-66] INFO TransactionManager - [Producer clientId=producer-66] ProducerId set to 14168 with epoch 0
13:47:25.484 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.484 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.484 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.484 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045484
13:47:25.485 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-68
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.485 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-68] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.485 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-68] Instantiated an idempotent producer.
13:47:25.486 [kafka-producer-network-thread | producer-67] INFO Metadata - [Producer clientId=producer-67] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.486 [kafka-producer-network-thread | producer-67] INFO TransactionManager - [Producer clientId=producer-67] ProducerId set to 12258 with epoch 0
13:47:25.487 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.487 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.487 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.487 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045487
13:47:25.488 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-69
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.488 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-69] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.488 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-69] Instantiated an idempotent producer.
13:47:25.489 [kafka-producer-network-thread | producer-68] INFO Metadata - [Producer clientId=producer-68] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.489 [kafka-producer-network-thread | producer-68] INFO TransactionManager - [Producer clientId=producer-68] ProducerId set to 13266 with epoch 0
13:47:25.490 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.490 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.490 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.490 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045490
13:47:25.491 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-70
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.491 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-70] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.491 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-70] Instantiated an idempotent producer.
13:47:25.492 [kafka-producer-network-thread | producer-69] INFO Metadata - [Producer clientId=producer-69] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.492 [kafka-producer-network-thread | producer-69] INFO TransactionManager - [Producer clientId=producer-69] ProducerId set to 14171 with epoch 0
13:47:25.493 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.493 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.493 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.493 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045493
13:47:25.493 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-71
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.494 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-71] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.494 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-71] Instantiated an idempotent producer.
13:47:25.495 [kafka-producer-network-thread | producer-70] INFO Metadata - [Producer clientId=producer-70] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.495 [kafka-producer-network-thread | producer-70] INFO TransactionManager - [Producer clientId=producer-70] ProducerId set to 13268 with epoch 0
13:47:25.496 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.496 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.496 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.496 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045496
13:47:25.496 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-72
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.497 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-72] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.497 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-72] Instantiated an idempotent producer.
13:47:25.498 [kafka-producer-network-thread | producer-71] INFO Metadata - [Producer clientId=producer-71] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.498 [kafka-producer-network-thread | producer-71] INFO TransactionManager - [Producer clientId=producer-71] ProducerId set to 14173 with epoch 0
13:47:25.499 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.499 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.499 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.499 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045499
13:47:25.500 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-73
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.500 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-73] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.500 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-73] Instantiated an idempotent producer.
13:47:25.501 [kafka-producer-network-thread | producer-72] INFO Metadata - [Producer clientId=producer-72] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.501 [kafka-producer-network-thread | producer-72] INFO TransactionManager - [Producer clientId=producer-72] ProducerId set to 13269 with epoch 0
13:47:25.502 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.502 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.502 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.502 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045502
13:47:25.503 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-74
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.503 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-74] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.503 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-74] Instantiated an idempotent producer.
13:47:25.504 [kafka-producer-network-thread | producer-73] INFO Metadata - [Producer clientId=producer-73] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.504 [kafka-producer-network-thread | producer-73] INFO TransactionManager - [Producer clientId=producer-73] ProducerId set to 14176 with epoch 0
13:47:25.505 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.505 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.505 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.505 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045505
13:47:25.506 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-75
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.506 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-75] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.506 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-75] Instantiated an idempotent producer.
13:47:25.507 [kafka-producer-network-thread | producer-74] INFO Metadata - [Producer clientId=producer-74] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.507 [kafka-producer-network-thread | producer-74] INFO TransactionManager - [Producer clientId=producer-74] ProducerId set to 12262 with epoch 0
13:47:25.508 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.508 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.508 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.508 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045508
13:47:25.509 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-76
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.509 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-76] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.509 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-76] Instantiated an idempotent producer.
13:47:25.510 [kafka-producer-network-thread | producer-75] INFO Metadata - [Producer clientId=producer-75] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.510 [kafka-producer-network-thread | producer-75] INFO TransactionManager - [Producer clientId=producer-75] ProducerId set to 12264 with epoch 0
13:47:25.511 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.511 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.511 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.511 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045511
13:47:25.512 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-77
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.512 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-77] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.512 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-77] Instantiated an idempotent producer.
13:47:25.513 [kafka-producer-network-thread | producer-76] INFO Metadata - [Producer clientId=producer-76] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.513 [kafka-producer-network-thread | producer-76] INFO TransactionManager - [Producer clientId=producer-76] ProducerId set to 13274 with epoch 0
13:47:25.518 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.518 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.518 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.518 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045518
13:47:25.518 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-78
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.519 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-78] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.519 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-78] Instantiated an idempotent producer.
13:47:25.520 [kafka-producer-network-thread | producer-77] INFO Metadata - [Producer clientId=producer-77] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.520 [kafka-producer-network-thread | producer-77] INFO TransactionManager - [Producer clientId=producer-77] ProducerId set to 13277 with epoch 0
13:47:25.521 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.521 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.521 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.521 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045521
13:47:25.522 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-79
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.522 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-79] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.522 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-79] Instantiated an idempotent producer.
13:47:25.523 [kafka-producer-network-thread | producer-78] INFO Metadata - [Producer clientId=producer-78] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.523 [kafka-producer-network-thread | producer-78] INFO TransactionManager - [Producer clientId=producer-78] ProducerId set to 12268 with epoch 0
13:47:25.524 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.524 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.524 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.525 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045524
13:47:25.525 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-80
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.526 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-80] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.526 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-80] Instantiated an idempotent producer.
13:47:25.526 [kafka-producer-network-thread | producer-79] INFO Metadata - [Producer clientId=producer-79] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.526 [kafka-producer-network-thread | producer-79] INFO TransactionManager - [Producer clientId=producer-79] ProducerId set to 12271 with epoch 0
13:47:25.528 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.528 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.528 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.528 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045528
13:47:25.528 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-81
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.529 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-81] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.529 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-81] Instantiated an idempotent producer.
13:47:25.530 [kafka-producer-network-thread | producer-80] INFO Metadata - [Producer clientId=producer-80] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.530 [kafka-producer-network-thread | producer-80] INFO TransactionManager - [Producer clientId=producer-80] ProducerId set to 14179 with epoch 0
13:47:25.531 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.531 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.531 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.531 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045531
13:47:25.531 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-82
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.532 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-82] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.532 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-82] Instantiated an idempotent producer.
13:47:25.533 [kafka-producer-network-thread | producer-81] INFO Metadata - [Producer clientId=producer-81] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.533 [kafka-producer-network-thread | producer-81] INFO TransactionManager - [Producer clientId=producer-81] ProducerId set to 12272 with epoch 0
13:47:25.534 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.534 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.534 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.534 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045534
13:47:25.534 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-83
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.535 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-83] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.535 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-83] Instantiated an idempotent producer.
13:47:25.535 [kafka-producer-network-thread | producer-82] INFO Metadata - [Producer clientId=producer-82] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.535 [kafka-producer-network-thread | producer-82] INFO TransactionManager - [Producer clientId=producer-82] ProducerId set to 14181 with epoch 0
13:47:25.537 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.537 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.537 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.537 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045537
13:47:25.538 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-84
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.539 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-84] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.539 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-84] Instantiated an idempotent producer.
13:47:25.539 [kafka-producer-network-thread | producer-83] INFO Metadata - [Producer clientId=producer-83] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.539 [kafka-producer-network-thread | producer-83] INFO TransactionManager - [Producer clientId=producer-83] ProducerId set to 14182 with epoch 0
13:47:25.541 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.541 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.541 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.541 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045541
13:47:25.542 [kafka-producer-network-thread | producer-84] INFO Metadata - [Producer clientId=producer-84] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.542 [kafka-producer-network-thread | producer-84] INFO TransactionManager - [Producer clientId=producer-84] ProducerId set to 13282 with epoch 0
13:47:25.545 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-85
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.546 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-85] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.546 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-85] Instantiated an idempotent producer.
13:47:25.548 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.548 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.548 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.548 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045548
13:47:25.549 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-86
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.549 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-86] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.549 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-86] Instantiated an idempotent producer.
13:47:25.550 [kafka-producer-network-thread | producer-85] INFO Metadata - [Producer clientId=producer-85] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.550 [kafka-producer-network-thread | producer-85] INFO TransactionManager - [Producer clientId=producer-85] ProducerId set to 12276 with epoch 0
13:47:25.551 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.551 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.551 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.551 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045551
13:47:25.552 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-87
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.552 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-87] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.552 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-87] Instantiated an idempotent producer.
13:47:25.553 [kafka-producer-network-thread | producer-86] INFO Metadata - [Producer clientId=producer-86] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.553 [kafka-producer-network-thread | producer-86] INFO TransactionManager - [Producer clientId=producer-86] ProducerId set to 13284 with epoch 0
13:47:25.554 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.554 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.555 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.555 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045554
13:47:25.555 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-88
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.555 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-88] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.555 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-88] Instantiated an idempotent producer.
13:47:25.556 [kafka-producer-network-thread | producer-87] INFO Metadata - [Producer clientId=producer-87] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.556 [kafka-producer-network-thread | producer-87] INFO TransactionManager - [Producer clientId=producer-87] ProducerId set to 14187 with epoch 0
13:47:25.557 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.557 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.557 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.557 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045557
13:47:25.558 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-89
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.558 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-89] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.558 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-89] Instantiated an idempotent producer.
13:47:25.559 [kafka-producer-network-thread | producer-88] INFO Metadata - [Producer clientId=producer-88] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.559 [kafka-producer-network-thread | producer-88] INFO TransactionManager - [Producer clientId=producer-88] ProducerId set to 13287 with epoch 0
13:47:25.560 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.560 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.560 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.560 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045560
13:47:25.561 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-90
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.561 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-90] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.561 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-90] Instantiated an idempotent producer.
13:47:25.563 [kafka-producer-network-thread | producer-89] INFO Metadata - [Producer clientId=producer-89] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.563 [kafka-producer-network-thread | producer-89] INFO TransactionManager - [Producer clientId=producer-89] ProducerId set to 12282 with epoch 0
13:47:25.563 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.564 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.564 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.564 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045564
13:47:25.564 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-91
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.565 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-91] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.565 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-91] Instantiated an idempotent producer.
13:47:25.565 [kafka-producer-network-thread | producer-90] INFO Metadata - [Producer clientId=producer-90] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.566 [kafka-producer-network-thread | producer-90] INFO TransactionManager - [Producer clientId=producer-90] ProducerId set to 13288 with epoch 0
13:47:25.569 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.569 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.569 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.569 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045569
13:47:25.569 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-92
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.570 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-92] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.570 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-92] Instantiated an idempotent producer.
13:47:25.571 [kafka-producer-network-thread | producer-91] INFO Metadata - [Producer clientId=producer-91] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.571 [kafka-producer-network-thread | producer-91] INFO TransactionManager - [Producer clientId=producer-91] ProducerId set to 13289 with epoch 0
13:47:25.572 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.572 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.572 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.572 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045572
13:47:25.572 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-93
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.573 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-93] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.573 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-93] Instantiated an idempotent producer.
13:47:25.574 [kafka-producer-network-thread | producer-92] INFO Metadata - [Producer clientId=producer-92] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.574 [kafka-producer-network-thread | producer-92] INFO TransactionManager - [Producer clientId=producer-92] ProducerId set to 12286 with epoch 0
13:47:25.575 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.575 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.575 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.575 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045575
13:47:25.575 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-94
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.576 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-94] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.576 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-94] Instantiated an idempotent producer.
13:47:25.577 [kafka-producer-network-thread | producer-93] INFO Metadata - [Producer clientId=producer-93] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.577 [kafka-producer-network-thread | producer-93] INFO TransactionManager - [Producer clientId=producer-93] ProducerId set to 13293 with epoch 0
13:47:25.578 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.578 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.578 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.578 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045578
13:47:25.578 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-95
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.579 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-95] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.579 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-95] Instantiated an idempotent producer.
13:47:25.580 [kafka-producer-network-thread | producer-94] INFO Metadata - [Producer clientId=producer-94] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.580 [kafka-producer-network-thread | producer-94] INFO TransactionManager - [Producer clientId=producer-94] ProducerId set to 13295 with epoch 0
13:47:25.581 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.581 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.581 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.581 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045581
13:47:25.581 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-96
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.582 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-96] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.582 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-96] Instantiated an idempotent producer.
13:47:25.583 [kafka-producer-network-thread | producer-95] INFO Metadata - [Producer clientId=producer-95] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.583 [kafka-producer-network-thread | producer-95] INFO TransactionManager - [Producer clientId=producer-95] ProducerId set to 13296 with epoch 0
13:47:25.584 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.584 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.584 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.584 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045584
13:47:25.584 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-97
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.585 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-97] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.585 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-97] Instantiated an idempotent producer.
13:47:25.586 [kafka-producer-network-thread | producer-96] INFO Metadata - [Producer clientId=producer-96] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.586 [kafka-producer-network-thread | producer-96] INFO TransactionManager - [Producer clientId=producer-96] ProducerId set to 14194 with epoch 0
13:47:25.587 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.587 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.587 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.587 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045587
13:47:25.587 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-98
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.588 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-98] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.588 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-98] Instantiated an idempotent producer.
13:47:25.588 [kafka-producer-network-thread | producer-97] INFO Metadata - [Producer clientId=producer-97] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.588 [kafka-producer-network-thread | producer-97] INFO TransactionManager - [Producer clientId=producer-97] ProducerId set to 12289 with epoch 0
13:47:25.590 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.590 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.590 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.590 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045590
13:47:25.590 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-99
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.591 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-99] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.591 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-99] Instantiated an idempotent producer.
13:47:25.591 [kafka-producer-network-thread | producer-98] INFO Metadata - [Producer clientId=producer-98] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.592 [kafka-producer-network-thread | producer-98] INFO TransactionManager - [Producer clientId=producer-98] ProducerId set to 12291 with epoch 0
13:47:25.592 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.592 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.592 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.592 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045592
13:47:25.593 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-100
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.593 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-100] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.594 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-100] Instantiated an idempotent producer.
13:47:25.594 [kafka-producer-network-thread | producer-99] INFO Metadata - [Producer clientId=producer-99] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.594 [kafka-producer-network-thread | producer-99] INFO TransactionManager - [Producer clientId=producer-99] ProducerId set to 12292 with epoch 0
13:47:25.595 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.596 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.596 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.596 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045596
13:47:25.596 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-101
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.596 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-101] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.597 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-101] Instantiated an idempotent producer.
13:47:25.597 [kafka-producer-network-thread | producer-100] INFO Metadata - [Producer clientId=producer-100] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.597 [kafka-producer-network-thread | producer-100] INFO TransactionManager - [Producer clientId=producer-100] ProducerId set to 13298 with epoch 0
13:47:25.598 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.598 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.598 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.598 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045598
13:47:25.599 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-102
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.599 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-102] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.599 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-102] Instantiated an idempotent producer.
13:47:25.600 [kafka-producer-network-thread | producer-101] INFO Metadata - [Producer clientId=producer-101] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.600 [kafka-producer-network-thread | producer-101] INFO TransactionManager - [Producer clientId=producer-101] ProducerId set to 12294 with epoch 0
13:47:25.601 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.601 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.601 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.601 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045601
13:47:25.602 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-103
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.602 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-103] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.602 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-103] Instantiated an idempotent producer.
13:47:25.603 [kafka-producer-network-thread | producer-102] INFO Metadata - [Producer clientId=producer-102] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.603 [kafka-producer-network-thread | producer-102] INFO TransactionManager - [Producer clientId=producer-102] ProducerId set to 13301 with epoch 0
13:47:25.604 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.604 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.604 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.604 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045604
13:47:25.604 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-104
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.605 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-104] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.605 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-104] Instantiated an idempotent producer.
13:47:25.606 [kafka-producer-network-thread | producer-103] INFO Metadata - [Producer clientId=producer-103] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.606 [kafka-producer-network-thread | producer-103] INFO TransactionManager - [Producer clientId=producer-103] ProducerId set to 12298 with epoch 0
13:47:25.607 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.607 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.607 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.607 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045607
13:47:25.607 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-105
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.608 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-105] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.608 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-105] Instantiated an idempotent producer.
13:47:25.609 [kafka-producer-network-thread | producer-104] INFO Metadata - [Producer clientId=producer-104] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.609 [kafka-producer-network-thread | producer-104] INFO TransactionManager - [Producer clientId=producer-104] ProducerId set to 13303 with epoch 0
13:47:25.610 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.610 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.610 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.610 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045610
13:47:25.610 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-106
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.611 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-106] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.611 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-106] Instantiated an idempotent producer.
13:47:25.612 [kafka-producer-network-thread | producer-105] INFO Metadata - [Producer clientId=producer-105] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.612 [kafka-producer-network-thread | producer-105] INFO TransactionManager - [Producer clientId=producer-105] ProducerId set to 12300 with epoch 0
13:47:25.614 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.614 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.614 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.614 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045614
13:47:25.614 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-107
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.614 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-107] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.615 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-107] Instantiated an idempotent producer.
13:47:25.616 [kafka-producer-network-thread | producer-106] INFO Metadata - [Producer clientId=producer-106] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.616 [kafka-producer-network-thread | producer-106] INFO TransactionManager - [Producer clientId=producer-106] ProducerId set to 13305 with epoch 0
13:47:25.617 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.617 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.617 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.617 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045617
13:47:25.617 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-108
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.617 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-108] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.618 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-108] Instantiated an idempotent producer.
13:47:25.619 [kafka-producer-network-thread | producer-107] INFO Metadata - [Producer clientId=producer-107] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.619 [kafka-producer-network-thread | producer-107] INFO TransactionManager - [Producer clientId=producer-107] ProducerId set to 14202 with epoch 0
13:47:25.625 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.625 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.625 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.625 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045625
13:47:25.625 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-109
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.626 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-109] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.626 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-109] Instantiated an idempotent producer.
13:47:25.627 [kafka-producer-network-thread | producer-108] INFO Metadata - [Producer clientId=producer-108] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.627 [kafka-producer-network-thread | producer-108] INFO TransactionManager - [Producer clientId=producer-108] ProducerId set to 13310 with epoch 0
13:47:25.628 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.628 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.628 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.628 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045628
13:47:25.628 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-110
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.629 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-110] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.629 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-110] Instantiated an idempotent producer.
13:47:25.630 [kafka-producer-network-thread | producer-109] INFO Metadata - [Producer clientId=producer-109] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.630 [kafka-producer-network-thread | producer-109] INFO TransactionManager - [Producer clientId=producer-109] ProducerId set to 13312 with epoch 0
13:47:25.631 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.631 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.631 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.631 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045631
13:47:25.631 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-111
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.632 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-111] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.632 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-111] Instantiated an idempotent producer.
13:47:25.632 [kafka-producer-network-thread | producer-110] INFO Metadata - [Producer clientId=producer-110] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.632 [kafka-producer-network-thread | producer-110] INFO TransactionManager - [Producer clientId=producer-110] ProducerId set to 14206 with epoch 0
13:47:25.634 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.634 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.634 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.634 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045634
13:47:25.634 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-112
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.635 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-112] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.635 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-112] Instantiated an idempotent producer.
13:47:25.637 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.637 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.637 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.637 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045637
13:47:25.637 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-113
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.637 [kafka-producer-network-thread | producer-111] INFO Metadata - [Producer clientId=producer-111] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.638 [kafka-producer-network-thread | producer-111] INFO TransactionManager - [Producer clientId=producer-111] ProducerId set to 13314 with epoch 0
13:47:25.638 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-113] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.638 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-113] Instantiated an idempotent producer.
13:47:25.639 [kafka-producer-network-thread | producer-112] INFO Metadata - [Producer clientId=producer-112] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.639 [kafka-producer-network-thread | producer-112] INFO TransactionManager - [Producer clientId=producer-112] ProducerId set to 13315 with epoch 0
13:47:25.640 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.640 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.640 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.640 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045640
13:47:25.640 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-114
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.641 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-114] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.641 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-114] Instantiated an idempotent producer.
13:47:25.641 [kafka-producer-network-thread | producer-113] INFO Metadata - [Producer clientId=producer-113] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.642 [kafka-producer-network-thread | producer-113] INFO TransactionManager - [Producer clientId=producer-113] ProducerId set to 12304 with epoch 0
13:47:25.643 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.643 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.643 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.643 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045643
13:47:25.643 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-115
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.644 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-115] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.644 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-115] Instantiated an idempotent producer.
13:47:25.644 [kafka-producer-network-thread | producer-114] INFO Metadata - [Producer clientId=producer-114] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.644 [kafka-producer-network-thread | producer-114] INFO TransactionManager - [Producer clientId=producer-114] ProducerId set to 14212 with epoch 0
13:47:25.645 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.646 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.646 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.646 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045646
13:47:25.646 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-116
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.646 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-116] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.647 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-116] Instantiated an idempotent producer.
13:47:25.647 [kafka-producer-network-thread | producer-115] INFO Metadata - [Producer clientId=producer-115] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.647 [kafka-producer-network-thread | producer-115] INFO TransactionManager - [Producer clientId=producer-115] ProducerId set to 13321 with epoch 0
13:47:25.648 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.648 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.648 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.648 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045648
13:47:25.649 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-117
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.649 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-117] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.649 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-117] Instantiated an idempotent producer.
13:47:25.650 [kafka-producer-network-thread | producer-116] INFO Metadata - [Producer clientId=producer-116] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.650 [kafka-producer-network-thread | producer-116] INFO TransactionManager - [Producer clientId=producer-116] ProducerId set to 13323 with epoch 0
13:47:25.651 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.651 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.651 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.651 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045651
13:47:25.652 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-118
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.652 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-118] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.652 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-118] Instantiated an idempotent producer.
13:47:25.653 [kafka-producer-network-thread | producer-117] INFO Metadata - [Producer clientId=producer-117] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.653 [kafka-producer-network-thread | producer-117] INFO TransactionManager - [Producer clientId=producer-117] ProducerId set to 12309 with epoch 0
13:47:25.654 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.654 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.654 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.654 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045654
13:47:25.655 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-119
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.655 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-119] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.655 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-119] Instantiated an idempotent producer.
13:47:25.655 [kafka-producer-network-thread | producer-118] INFO Metadata - [Producer clientId=producer-118] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.656 [kafka-producer-network-thread | producer-118] INFO TransactionManager - [Producer clientId=producer-118] ProducerId set to 14214 with epoch 0
13:47:25.657 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.657 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.657 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.657 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045657
13:47:25.657 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-120
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.658 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-120] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.658 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-120] Instantiated an idempotent producer.
13:47:25.658 [kafka-producer-network-thread | producer-119] INFO Metadata - [Producer clientId=producer-119] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.659 [kafka-producer-network-thread | producer-119] INFO TransactionManager - [Producer clientId=producer-119] ProducerId set to 13325 with epoch 0
13:47:25.660 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.660 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.660 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.660 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045660
13:47:25.660 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-121
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.661 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-121] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.661 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-121] Instantiated an idempotent producer.
13:47:25.661 [kafka-producer-network-thread | producer-120] INFO Metadata - [Producer clientId=producer-120] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.662 [kafka-producer-network-thread | producer-120] INFO TransactionManager - [Producer clientId=producer-120] ProducerId set to 12312 with epoch 0
13:47:25.663 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.663 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.663 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.663 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045663
13:47:25.663 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-122
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.663 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-122] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.664 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-122] Instantiated an idempotent producer.
13:47:25.664 [kafka-producer-network-thread | producer-121] INFO Metadata - [Producer clientId=producer-121] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.664 [kafka-producer-network-thread | producer-121] INFO TransactionManager - [Producer clientId=producer-121] ProducerId set to 13327 with epoch 0
13:47:25.665 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.665 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.665 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.665 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045665
13:47:25.666 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-123
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.666 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-123] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.666 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-123] Instantiated an idempotent producer.
13:47:25.667 [kafka-producer-network-thread | producer-122] INFO Metadata - [Producer clientId=producer-122] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.667 [kafka-producer-network-thread | producer-122] INFO TransactionManager - [Producer clientId=producer-122] ProducerId set to 14218 with epoch 0
13:47:25.668 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.668 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.668 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.668 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045668
13:47:25.669 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-124
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.669 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-124] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.669 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-124] Instantiated an idempotent producer.
13:47:25.670 [kafka-producer-network-thread | producer-123] INFO Metadata - [Producer clientId=producer-123] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.670 [kafka-producer-network-thread | producer-123] INFO TransactionManager - [Producer clientId=producer-123] ProducerId set to 13328 with epoch 0
13:47:25.671 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.671 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.671 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.671 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045671
13:47:25.672 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-125
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.672 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-125] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.672 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-125] Instantiated an idempotent producer.
13:47:25.673 [kafka-producer-network-thread | producer-124] INFO Metadata - [Producer clientId=producer-124] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.673 [kafka-producer-network-thread | producer-124] INFO TransactionManager - [Producer clientId=producer-124] ProducerId set to 12317 with epoch 0
13:47:25.674 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.674 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.674 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.674 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045674
13:47:25.674 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-126
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.675 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-126] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.675 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-126] Instantiated an idempotent producer.
13:47:25.676 [kafka-producer-network-thread | producer-125] INFO Metadata - [Producer clientId=producer-125] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.676 [kafka-producer-network-thread | producer-125] INFO TransactionManager - [Producer clientId=producer-125] ProducerId set to 13329 with epoch 0
13:47:25.677 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.677 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.677 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.677 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045677
13:47:25.677 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-127
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.678 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-127] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.678 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-127] Instantiated an idempotent producer.
13:47:25.680 [kafka-producer-network-thread | producer-126] INFO Metadata - [Producer clientId=producer-126] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.680 [kafka-producer-network-thread | producer-126] INFO TransactionManager - [Producer clientId=producer-126] ProducerId set to 12319 with epoch 0
13:47:25.682 [kafka-producer-network-thread | producer-127] INFO Metadata - [Producer clientId=producer-127] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.682 [kafka-producer-network-thread | producer-127] INFO TransactionManager - [Producer clientId=producer-127] ProducerId set to 13332 with epoch 0
13:47:25.682 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.683 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.683 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.683 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045682
13:47:25.683 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-128
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.684 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-128] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.684 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-128] Instantiated an idempotent producer.
13:47:25.686 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.686 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.686 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.686 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045686
13:47:25.687 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-129
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.687 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-129] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.687 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-129] Instantiated an idempotent producer.
13:47:25.687 [kafka-producer-network-thread | producer-128] INFO Metadata - [Producer clientId=producer-128] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.688 [kafka-producer-network-thread | producer-128] INFO TransactionManager - [Producer clientId=producer-128] ProducerId set to 12322 with epoch 0
13:47:25.689 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.689 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.689 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.689 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045689
13:47:25.691 [kafka-producer-network-thread | producer-129] INFO Metadata - [Producer clientId=producer-129] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.691 [kafka-producer-network-thread | producer-129] INFO TransactionManager - [Producer clientId=producer-129] ProducerId set to 14224 with epoch 0
13:47:25.692 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-130
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.692 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-130] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.692 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-130] Instantiated an idempotent producer.
13:47:25.694 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.694 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.694 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.694 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045694
13:47:25.695 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-131
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.695 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-131] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.695 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-131] Instantiated an idempotent producer.
13:47:25.696 [kafka-producer-network-thread | producer-130] INFO Metadata - [Producer clientId=producer-130] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.697 [kafka-producer-network-thread | producer-130] INFO TransactionManager - [Producer clientId=producer-130] ProducerId set to 13340 with epoch 0
13:47:25.697 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.698 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.698 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.698 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045698
13:47:25.698 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-132
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.698 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-132] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.699 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-132] Instantiated an idempotent producer.
13:47:25.700 [kafka-producer-network-thread | producer-131] INFO Metadata - [Producer clientId=producer-131] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.700 [kafka-producer-network-thread | producer-131] INFO TransactionManager - [Producer clientId=producer-131] ProducerId set to 13342 with epoch 0
13:47:25.700 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.700 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.700 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.701 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045700
13:47:25.701 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-133
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.701 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-133] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.701 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-133] Instantiated an idempotent producer.
13:47:25.702 [kafka-producer-network-thread | producer-132] INFO Metadata - [Producer clientId=producer-132] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.702 [kafka-producer-network-thread | producer-132] INFO TransactionManager - [Producer clientId=producer-132] ProducerId set to 13343 with epoch 0
13:47:25.704 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.704 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.704 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.704 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045704
13:47:25.704 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-134
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.705 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-134] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.705 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-134] Instantiated an idempotent producer.
13:47:25.707 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.707 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.707 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.707 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045707
13:47:25.707 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-135
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.708 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-135] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.708 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-135] Instantiated an idempotent producer.
13:47:25.709 [kafka-producer-network-thread | producer-133] INFO Metadata - [Producer clientId=producer-133] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.709 [kafka-producer-network-thread | producer-133] INFO TransactionManager - [Producer clientId=producer-133] ProducerId set to 13346 with epoch 0
13:47:25.709 [kafka-producer-network-thread | producer-134] INFO Metadata - [Producer clientId=producer-134] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.709 [kafka-producer-network-thread | producer-134] INFO TransactionManager - [Producer clientId=producer-134] ProducerId set to 12324 with epoch 0
13:47:25.710 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.710 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.710 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.710 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045710
13:47:25.710 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-136
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.711 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-136] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.711 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-136] Instantiated an idempotent producer.
13:47:25.712 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.713 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.713 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.713 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045713
13:47:25.713 [kafka-producer-network-thread | producer-135] INFO Metadata - [Producer clientId=producer-135] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.713 [kafka-producer-network-thread | producer-135] INFO TransactionManager - [Producer clientId=producer-135] ProducerId set to 13348 with epoch 0
13:47:25.713 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-137
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.714 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-137] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.714 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-137] Instantiated an idempotent producer.
13:47:25.715 [kafka-producer-network-thread | producer-136] INFO Metadata - [Producer clientId=producer-136] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.715 [kafka-producer-network-thread | producer-136] INFO TransactionManager - [Producer clientId=producer-136] ProducerId set to 14233 with epoch 0
13:47:25.715 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.716 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.716 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.716 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045716
13:47:25.716 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-138
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.716 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-138] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.716 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-138] Instantiated an idempotent producer.
13:47:25.717 [kafka-producer-network-thread | producer-137] INFO Metadata - [Producer clientId=producer-137] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.717 [kafka-producer-network-thread | producer-137] INFO TransactionManager - [Producer clientId=producer-137] ProducerId set to 12327 with epoch 0
13:47:25.718 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.718 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.718 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.718 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045718
13:47:25.719 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-139
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.719 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-139] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.719 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-139] Instantiated an idempotent producer.
13:47:25.720 [kafka-producer-network-thread | producer-138] INFO Metadata - [Producer clientId=producer-138] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.720 [kafka-producer-network-thread | producer-138] INFO TransactionManager - [Producer clientId=producer-138] ProducerId set to 13349 with epoch 0
13:47:25.721 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.721 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.721 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.721 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045721
13:47:25.721 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-140
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.722 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-140] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.722 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-140] Instantiated an idempotent producer.
13:47:25.722 [kafka-producer-network-thread | producer-139] INFO Metadata - [Producer clientId=producer-139] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.723 [kafka-producer-network-thread | producer-139] INFO TransactionManager - [Producer clientId=producer-139] ProducerId set to 12332 with epoch 0
13:47:25.724 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.724 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.724 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.724 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045724
13:47:25.724 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-141
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.724 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-141] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.725 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-141] Instantiated an idempotent producer.
13:47:25.725 [kafka-producer-network-thread | producer-140] INFO Metadata - [Producer clientId=producer-140] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.725 [kafka-producer-network-thread | producer-140] INFO TransactionManager - [Producer clientId=producer-140] ProducerId set to 12333 with epoch 0
13:47:25.728 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.728 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.728 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.728 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045728
13:47:25.728 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-142
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.729 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-142] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.729 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-142] Instantiated an idempotent producer.
13:47:25.729 [kafka-producer-network-thread | producer-141] INFO Metadata - [Producer clientId=producer-141] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.729 [kafka-producer-network-thread | producer-141] INFO TransactionManager - [Producer clientId=producer-141] ProducerId set to 13354 with epoch 0
13:47:25.731 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.731 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.731 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.732 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045731
13:47:25.732 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-143
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.732 [kafka-producer-network-thread | producer-142] INFO Metadata - [Producer clientId=producer-142] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.732 [kafka-producer-network-thread | producer-142] INFO TransactionManager - [Producer clientId=producer-142] ProducerId set to 13355 with epoch 0
13:47:25.732 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-143] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.733 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-143] Instantiated an idempotent producer.
13:47:25.734 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.734 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.734 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.734 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045734
13:47:25.735 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-144
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.735 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-144] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.735 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-144] Instantiated an idempotent producer.
13:47:25.736 [kafka-producer-network-thread | producer-143] INFO Metadata - [Producer clientId=producer-143] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.736 [kafka-producer-network-thread | producer-143] INFO TransactionManager - [Producer clientId=producer-143] ProducerId set to 14236 with epoch 0
13:47:25.738 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.738 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.738 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.738 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045738
13:47:25.738 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-145
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.739 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-145] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.739 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-145] Instantiated an idempotent producer.
13:47:25.739 [kafka-producer-network-thread | producer-144] INFO Metadata - [Producer clientId=producer-144] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.740 [kafka-producer-network-thread | producer-144] INFO TransactionManager - [Producer clientId=producer-144] ProducerId set to 12336 with epoch 0
13:47:25.741 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.741 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.741 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.741 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045741
13:47:25.741 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-146
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.742 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-146] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.742 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-146] Instantiated an idempotent producer.
13:47:25.743 [kafka-producer-network-thread | producer-145] INFO Metadata - [Producer clientId=producer-145] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.743 [kafka-producer-network-thread | producer-145] INFO TransactionManager - [Producer clientId=producer-145] ProducerId set to 14240 with epoch 0
13:47:25.744 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.744 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.744 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.744 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045744
13:47:25.744 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-147
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.745 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-147] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.745 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-147] Instantiated an idempotent producer.
13:47:25.745 [kafka-producer-network-thread | producer-146] INFO Metadata - [Producer clientId=producer-146] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.746 [kafka-producer-network-thread | producer-146] INFO TransactionManager - [Producer clientId=producer-146] ProducerId set to 12339 with epoch 0
13:47:25.747 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.747 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.747 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.747 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045747
13:47:25.747 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-148
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.748 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-148] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.748 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-148] Instantiated an idempotent producer.
13:47:25.749 [kafka-producer-network-thread | producer-147] INFO Metadata - [Producer clientId=producer-147] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.749 [kafka-producer-network-thread | producer-147] INFO TransactionManager - [Producer clientId=producer-147] ProducerId set to 13358 with epoch 0
13:47:25.750 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.750 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.750 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.750 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045750
13:47:25.750 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-149
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.750 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-149] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.751 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-149] Instantiated an idempotent producer.
13:47:25.752 [kafka-producer-network-thread | producer-148] INFO Metadata - [Producer clientId=producer-148] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.753 [kafka-producer-network-thread | producer-148] INFO TransactionManager - [Producer clientId=producer-148] ProducerId set to 12340 with epoch 0
13:47:25.753 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.753 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.753 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.753 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045753
13:47:25.753 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-150
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.754 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-150] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.754 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-150] Instantiated an idempotent producer.
13:47:25.754 [kafka-producer-network-thread | producer-149] INFO Metadata - [Producer clientId=producer-149] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.754 [kafka-producer-network-thread | producer-149] INFO TransactionManager - [Producer clientId=producer-149] ProducerId set to 12341 with epoch 0
13:47:25.756 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.756 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.756 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.756 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045756
13:47:25.756 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-151
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.757 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-151] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.757 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-151] Instantiated an idempotent producer.
13:47:25.757 [kafka-producer-network-thread | producer-150] INFO Metadata - [Producer clientId=producer-150] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.757 [kafka-producer-network-thread | producer-150] INFO TransactionManager - [Producer clientId=producer-150] ProducerId set to 14246 with epoch 0
13:47:25.759 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.759 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.759 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.759 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045759
13:47:25.759 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-152
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.760 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-152] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.760 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-152] Instantiated an idempotent producer.
13:47:25.760 [kafka-producer-network-thread | producer-151] INFO Metadata - [Producer clientId=producer-151] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.761 [kafka-producer-network-thread | producer-151] INFO TransactionManager - [Producer clientId=producer-151] ProducerId set to 13361 with epoch 0
13:47:25.762 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.762 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.762 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.762 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045762
13:47:25.762 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-153
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.762 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-153] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.762 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-153] Instantiated an idempotent producer.
13:47:25.763 [kafka-producer-network-thread | producer-152] INFO Metadata - [Producer clientId=producer-152] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.763 [kafka-producer-network-thread | producer-152] INFO TransactionManager - [Producer clientId=producer-152] ProducerId set to 12342 with epoch 0
13:47:25.764 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.764 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.764 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.764 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045764
13:47:25.764 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-154
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.765 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-154] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.765 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-154] Instantiated an idempotent producer.
13:47:25.767 [kafka-producer-network-thread | producer-153] INFO Metadata - [Producer clientId=producer-153] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.767 [kafka-producer-network-thread | producer-153] INFO TransactionManager - [Producer clientId=producer-153] ProducerId set to 13364 with epoch 0
13:47:25.774 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.774 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.774 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.774 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045774
13:47:25.775 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-155
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.775 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-155] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.776 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-155] Instantiated an idempotent producer.
13:47:25.777 [kafka-producer-network-thread | producer-154] INFO Metadata - [Producer clientId=producer-154] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.777 [kafka-producer-network-thread | producer-154] INFO TransactionManager - [Producer clientId=producer-154] ProducerId set to 12344 with epoch 0
13:47:25.778 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.778 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.778 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.778 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045778
13:47:25.778 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-156
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.779 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-156] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.779 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-156] Instantiated an idempotent producer.
13:47:25.779 [kafka-producer-network-thread | producer-155] INFO Metadata - [Producer clientId=producer-155] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.779 [kafka-producer-network-thread | producer-155] INFO TransactionManager - [Producer clientId=producer-155] ProducerId set to 13367 with epoch 0
13:47:25.782 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.782 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.782 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.782 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045782
13:47:25.783 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-157
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.783 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-157] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.783 [kafka-producer-network-thread | producer-156] INFO Metadata - [Producer clientId=producer-156] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.783 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-157] Instantiated an idempotent producer.
13:47:25.783 [kafka-producer-network-thread | producer-156] INFO TransactionManager - [Producer clientId=producer-156] ProducerId set to 13370 with epoch 0
13:47:25.785 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.785 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.785 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.785 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045785
13:47:25.786 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-158
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.786 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-158] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.786 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-158] Instantiated an idempotent producer.
13:47:25.787 [kafka-producer-network-thread | producer-157] INFO Metadata - [Producer clientId=producer-157] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.787 [kafka-producer-network-thread | producer-157] INFO TransactionManager - [Producer clientId=producer-157] ProducerId set to 12350 with epoch 0
13:47:25.788 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.788 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.788 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.788 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045788
13:47:25.789 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-159
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.789 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-159] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.789 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-159] Instantiated an idempotent producer.
13:47:25.790 [kafka-producer-network-thread | producer-158] INFO Metadata - [Producer clientId=producer-158] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.790 [kafka-producer-network-thread | producer-158] INFO TransactionManager - [Producer clientId=producer-158] ProducerId set to 14253 with epoch 0
13:47:25.791 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.791 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.791 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.791 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045791
13:47:25.792 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-160
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.792 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-160] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.792 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-160] Instantiated an idempotent producer.
13:47:25.793 [kafka-producer-network-thread | producer-159] INFO Metadata - [Producer clientId=producer-159] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.793 [kafka-producer-network-thread | producer-159] INFO TransactionManager - [Producer clientId=producer-159] ProducerId set to 14255 with epoch 0
13:47:25.794 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.794 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.794 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.794 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045794
13:47:25.794 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-161
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.795 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-161] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.795 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-161] Instantiated an idempotent producer.
13:47:25.796 [kafka-producer-network-thread | producer-160] INFO Metadata - [Producer clientId=producer-160] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.796 [kafka-producer-network-thread | producer-160] INFO TransactionManager - [Producer clientId=producer-160] ProducerId set to 14256 with epoch 0
13:47:25.797 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.797 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.797 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.797 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045797
13:47:25.798 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-162
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.798 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-162] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.798 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-162] Instantiated an idempotent producer.
13:47:25.798 [kafka-producer-network-thread | producer-161] INFO Metadata - [Producer clientId=producer-161] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.799 [kafka-producer-network-thread | producer-161] INFO TransactionManager - [Producer clientId=producer-161] ProducerId set to 14258 with epoch 0
13:47:25.800 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.800 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.800 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.800 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045800
13:47:25.801 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-163
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.801 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-163] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.801 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-163] Instantiated an idempotent producer.
13:47:25.801 [kafka-producer-network-thread | producer-162] INFO Metadata - [Producer clientId=producer-162] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.801 [kafka-producer-network-thread | producer-162] INFO TransactionManager - [Producer clientId=producer-162] ProducerId set to 13375 with epoch 0
13:47:25.803 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.803 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.803 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.803 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045803
13:47:25.803 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-164
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.804 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-164] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.804 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-164] Instantiated an idempotent producer.
13:47:25.805 [kafka-producer-network-thread | producer-163] INFO Metadata - [Producer clientId=producer-163] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.805 [kafka-producer-network-thread | producer-163] INFO TransactionManager - [Producer clientId=producer-163] ProducerId set to 12354 with epoch 0
13:47:25.806 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.806 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.806 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.806 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045806
13:47:25.806 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-165
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.807 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-165] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.807 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-165] Instantiated an idempotent producer.
13:47:25.807 [kafka-producer-network-thread | producer-164] INFO Metadata - [Producer clientId=producer-164] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.808 [kafka-producer-network-thread | producer-164] INFO TransactionManager - [Producer clientId=producer-164] ProducerId set to 14260 with epoch 0
13:47:25.810 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.810 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.810 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.810 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045810
13:47:25.811 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-166
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.811 [kafka-producer-network-thread | producer-165] INFO Metadata - [Producer clientId=producer-165] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.811 [kafka-producer-network-thread | producer-165] INFO TransactionManager - [Producer clientId=producer-165] ProducerId set to 12356 with epoch 0
13:47:25.811 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-166] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.811 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-166] Instantiated an idempotent producer.
13:47:25.813 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.813 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.813 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.813 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045813
13:47:25.814 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-167
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.814 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-167] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.814 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-167] Instantiated an idempotent producer.
13:47:25.815 [kafka-producer-network-thread | producer-166] INFO Metadata - [Producer clientId=producer-166] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.815 [kafka-producer-network-thread | producer-166] INFO TransactionManager - [Producer clientId=producer-166] ProducerId set to 14261 with epoch 0
13:47:25.816 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.816 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.816 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.816 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045816
13:47:25.817 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-168
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.817 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-168] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.817 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-168] Instantiated an idempotent producer.
13:47:25.818 [kafka-producer-network-thread | producer-167] INFO Metadata - [Producer clientId=producer-167] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.818 [kafka-producer-network-thread | producer-167] INFO TransactionManager - [Producer clientId=producer-167] ProducerId set to 12357 with epoch 0
13:47:25.819 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.819 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.819 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.819 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045819
13:47:25.820 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-169
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.820 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-169] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.820 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-169] Instantiated an idempotent producer.
13:47:25.821 [kafka-producer-network-thread | producer-168] INFO Metadata - [Producer clientId=producer-168] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.821 [kafka-producer-network-thread | producer-168] INFO TransactionManager - [Producer clientId=producer-168] ProducerId set to 14265 with epoch 0
13:47:25.822 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.822 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.822 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.822 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045822
13:47:25.822 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-170
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.823 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-170] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.823 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-170] Instantiated an idempotent producer.
13:47:25.823 [kafka-producer-network-thread | producer-169] INFO Metadata - [Producer clientId=producer-169] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.824 [kafka-producer-network-thread | producer-169] INFO TransactionManager - [Producer clientId=producer-169] ProducerId set to 13382 with epoch 0
13:47:25.825 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.825 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.825 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.825 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045825
13:47:25.825 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-171
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.826 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-171] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.826 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-171] Instantiated an idempotent producer.
13:47:25.827 [kafka-producer-network-thread | producer-170] INFO Metadata - [Producer clientId=producer-170] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.827 [kafka-producer-network-thread | producer-170] INFO TransactionManager - [Producer clientId=producer-170] ProducerId set to 13383 with epoch 0
13:47:25.828 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.828 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.828 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.828 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045828
13:47:25.829 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-172
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.829 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-172] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.829 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-172] Instantiated an idempotent producer.
13:47:25.829 [kafka-producer-network-thread | producer-171] INFO Metadata - [Producer clientId=producer-171] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.829 [kafka-producer-network-thread | producer-171] INFO TransactionManager - [Producer clientId=producer-171] ProducerId set to 13385 with epoch 0
13:47:25.832 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.832 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.832 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.832 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045832
13:47:25.832 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-173
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.833 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-173] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.833 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-173] Instantiated an idempotent producer.
13:47:25.833 [kafka-producer-network-thread | producer-172] INFO Metadata - [Producer clientId=producer-172] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.833 [kafka-producer-network-thread | producer-172] INFO TransactionManager - [Producer clientId=producer-172] ProducerId set to 14269 with epoch 0
13:47:25.835 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.835 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.835 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.835 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045835
13:47:25.835 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-174
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.836 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-174] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.836 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-174] Instantiated an idempotent producer.
13:47:25.836 [kafka-producer-network-thread | producer-173] INFO Metadata - [Producer clientId=producer-173] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.836 [kafka-producer-network-thread | producer-173] INFO TransactionManager - [Producer clientId=producer-173] ProducerId set to 14271 with epoch 0
13:47:25.837 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.837 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.837 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.837 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045837
13:47:25.838 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-175
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.838 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-175] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.838 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-175] Instantiated an idempotent producer.
13:47:25.839 [kafka-producer-network-thread | producer-174] INFO Metadata - [Producer clientId=producer-174] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.839 [kafka-producer-network-thread | producer-174] INFO TransactionManager - [Producer clientId=producer-174] ProducerId set to 13389 with epoch 0
13:47:25.840 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.840 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.840 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.840 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045840
13:47:25.840 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-176
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.841 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-176] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.841 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-176] Instantiated an idempotent producer.
13:47:25.841 [kafka-producer-network-thread | producer-175] INFO Metadata - [Producer clientId=producer-175] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.842 [kafka-producer-network-thread | producer-175] INFO TransactionManager - [Producer clientId=producer-175] ProducerId set to 12364 with epoch 0
13:47:25.842 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.843 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.843 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.843 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045843
13:47:25.843 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-177
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.843 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-177] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.844 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-177] Instantiated an idempotent producer.
13:47:25.844 [kafka-producer-network-thread | producer-176] INFO Metadata - [Producer clientId=producer-176] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.844 [kafka-producer-network-thread | producer-176] INFO TransactionManager - [Producer clientId=producer-176] ProducerId set to 14274 with epoch 0
13:47:25.845 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.845 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.845 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.845 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045845
13:47:25.846 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-178
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.846 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-178] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.846 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-178] Instantiated an idempotent producer.
13:47:25.847 [kafka-producer-network-thread | producer-177] INFO Metadata - [Producer clientId=producer-177] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.847 [kafka-producer-network-thread | producer-177] INFO TransactionManager - [Producer clientId=producer-177] ProducerId set to 13391 with epoch 0
13:47:25.848 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.848 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.848 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.848 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045848
13:47:25.849 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-179
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.849 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-179] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.849 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-179] Instantiated an idempotent producer.
13:47:25.850 [kafka-producer-network-thread | producer-178] INFO Metadata - [Producer clientId=producer-178] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.850 [kafka-producer-network-thread | producer-178] INFO TransactionManager - [Producer clientId=producer-178] ProducerId set to 12366 with epoch 0
13:47:25.851 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.851 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.851 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.851 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045851
13:47:25.851 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-180
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.851 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-180] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.852 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-180] Instantiated an idempotent producer.
13:47:25.852 [kafka-producer-network-thread | producer-179] INFO Metadata - [Producer clientId=producer-179] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.852 [kafka-producer-network-thread | producer-179] INFO TransactionManager - [Producer clientId=producer-179] ProducerId set to 13393 with epoch 0
13:47:25.853 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.853 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.853 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.853 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045853
13:47:25.854 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-181
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.854 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-181] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.854 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-181] Instantiated an idempotent producer.
13:47:25.855 [kafka-producer-network-thread | producer-180] INFO Metadata - [Producer clientId=producer-180] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.855 [kafka-producer-network-thread | producer-180] INFO TransactionManager - [Producer clientId=producer-180] ProducerId set to 13395 with epoch 0
13:47:25.856 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.856 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.856 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.856 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045856
13:47:25.856 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-182
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.857 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-182] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.857 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-182] Instantiated an idempotent producer.
13:47:25.858 [kafka-producer-network-thread | producer-181] INFO Metadata - [Producer clientId=producer-181] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.858 [kafka-producer-network-thread | producer-181] INFO TransactionManager - [Producer clientId=producer-181] ProducerId set to 12368 with epoch 0
13:47:25.860 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.860 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.860 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.860 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045860
13:47:25.861 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-183
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.861 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-183] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.861 [kafka-producer-network-thread | producer-182] INFO Metadata - [Producer clientId=producer-182] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.861 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-183] Instantiated an idempotent producer.
13:47:25.861 [kafka-producer-network-thread | producer-182] INFO TransactionManager - [Producer clientId=producer-182] ProducerId set to 13397 with epoch 0
13:47:25.863 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.863 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.863 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.863 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045863
13:47:25.864 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-184
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.864 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-184] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.864 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-184] Instantiated an idempotent producer.
13:47:25.865 [kafka-producer-network-thread | producer-183] INFO Metadata - [Producer clientId=producer-183] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.865 [kafka-producer-network-thread | producer-183] INFO TransactionManager - [Producer clientId=producer-183] ProducerId set to 14279 with epoch 0
13:47:25.866 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.866 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.866 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.866 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045866
13:47:25.867 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-185
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.867 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-185] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.867 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-185] Instantiated an idempotent producer.
13:47:25.867 [kafka-producer-network-thread | producer-184] INFO Metadata - [Producer clientId=producer-184] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.868 [kafka-producer-network-thread | producer-184] INFO TransactionManager - [Producer clientId=producer-184] ProducerId set to 14282 with epoch 0
13:47:25.869 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.869 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.869 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.869 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045869
13:47:25.869 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-186
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.870 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-186] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.870 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-186] Instantiated an idempotent producer.
13:47:25.870 [kafka-producer-network-thread | producer-185] INFO Metadata - [Producer clientId=producer-185] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.870 [kafka-producer-network-thread | producer-185] INFO TransactionManager - [Producer clientId=producer-185] ProducerId set to 12371 with epoch 0
13:47:25.872 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.872 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.872 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.872 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045872
13:47:25.872 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-187
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.872 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-187] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.873 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-187] Instantiated an idempotent producer.
13:47:25.873 [kafka-producer-network-thread | producer-186] INFO Metadata - [Producer clientId=producer-186] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.873 [kafka-producer-network-thread | producer-186] INFO TransactionManager - [Producer clientId=producer-186] ProducerId set to 14285 with epoch 0
13:47:25.874 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.874 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.874 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.874 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045874
13:47:25.875 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-188
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.875 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-188] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.875 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-188] Instantiated an idempotent producer.
13:47:25.876 [kafka-producer-network-thread | producer-187] INFO Metadata - [Producer clientId=producer-187] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.876 [kafka-producer-network-thread | producer-187] INFO TransactionManager - [Producer clientId=producer-187] ProducerId set to 13400 with epoch 0
13:47:25.877 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.877 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.877 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.877 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045877
13:47:25.877 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-189
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.878 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-189] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.878 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-189] Instantiated an idempotent producer.
13:47:25.878 [kafka-producer-network-thread | producer-188] INFO Metadata - [Producer clientId=producer-188] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.878 [kafka-producer-network-thread | producer-188] INFO TransactionManager - [Producer clientId=producer-188] ProducerId set to 12376 with epoch 0
13:47:25.880 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.880 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.880 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.880 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045880
13:47:25.880 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-190
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.880 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-190] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.881 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-190] Instantiated an idempotent producer.
13:47:25.881 [kafka-producer-network-thread | producer-189] INFO Metadata - [Producer clientId=producer-189] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.881 [kafka-producer-network-thread | producer-189] INFO TransactionManager - [Producer clientId=producer-189] ProducerId set to 13402 with epoch 0
13:47:25.882 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.882 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.882 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.883 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045882
13:47:25.883 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-191
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.883 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-191] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.884 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-191] Instantiated an idempotent producer.
13:47:25.884 [kafka-producer-network-thread | producer-190] INFO Metadata - [Producer clientId=producer-190] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.884 [kafka-producer-network-thread | producer-190] INFO TransactionManager - [Producer clientId=producer-190] ProducerId set to 12377 with epoch 0
13:47:25.885 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.885 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.885 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.885 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045885
13:47:25.886 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-192
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.886 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-192] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.886 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-192] Instantiated an idempotent producer.
13:47:25.887 [kafka-producer-network-thread | producer-191] INFO Metadata - [Producer clientId=producer-191] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.887 [kafka-producer-network-thread | producer-191] INFO TransactionManager - [Producer clientId=producer-191] ProducerId set to 12378 with epoch 0
13:47:25.888 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.888 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.888 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.888 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045888
13:47:25.889 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-193
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.889 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-193] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.889 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-193] Instantiated an idempotent producer.
13:47:25.890 [kafka-producer-network-thread | producer-192] INFO Metadata - [Producer clientId=producer-192] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.890 [kafka-producer-network-thread | producer-192] INFO TransactionManager - [Producer clientId=producer-192] ProducerId set to 12379 with epoch 0
13:47:25.892 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.892 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.892 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.892 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045892
13:47:25.892 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-194
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.893 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-194] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.893 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-194] Instantiated an idempotent producer.
13:47:25.893 [kafka-producer-network-thread | producer-193] INFO Metadata - [Producer clientId=producer-193] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.893 [kafka-producer-network-thread | producer-193] INFO TransactionManager - [Producer clientId=producer-193] ProducerId set to 12380 with epoch 0
13:47:25.895 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.895 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.895 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.895 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045895
13:47:25.895 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-195
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.896 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-195] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.896 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-195] Instantiated an idempotent producer.
13:47:25.897 [kafka-producer-network-thread | producer-194] INFO Metadata - [Producer clientId=producer-194] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.897 [kafka-producer-network-thread | producer-194] INFO TransactionManager - [Producer clientId=producer-194] ProducerId set to 13407 with epoch 0
13:47:25.898 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.898 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.898 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.898 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045898
13:47:25.898 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-196
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.899 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-196] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.899 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-196] Instantiated an idempotent producer.
13:47:25.899 [kafka-producer-network-thread | producer-195] INFO Metadata - [Producer clientId=producer-195] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.900 [kafka-producer-network-thread | producer-195] INFO TransactionManager - [Producer clientId=producer-195] ProducerId set to 14292 with epoch 0
13:47:25.900 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.901 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.901 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.901 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045900
13:47:25.901 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-197
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.901 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-197] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.901 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-197] Instantiated an idempotent producer.
13:47:25.902 [kafka-producer-network-thread | producer-196] INFO Metadata - [Producer clientId=producer-196] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.902 [kafka-producer-network-thread | producer-196] INFO TransactionManager - [Producer clientId=producer-196] ProducerId set to 12383 with epoch 0
13:47:25.903 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.903 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.903 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.903 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045903
13:47:25.903 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-198
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.904 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-198] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.904 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-198] Instantiated an idempotent producer.
13:47:25.904 [kafka-producer-network-thread | producer-197] INFO Metadata - [Producer clientId=producer-197] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.905 [kafka-producer-network-thread | producer-197] INFO TransactionManager - [Producer clientId=producer-197] ProducerId set to 14293 with epoch 0
13:47:25.906 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.906 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.906 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.906 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045906
13:47:25.906 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-199
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.906 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-199] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.906 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-199] Instantiated an idempotent producer.
13:47:25.908 [kafka-producer-network-thread | producer-198] INFO Metadata - [Producer clientId=producer-198] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.908 [kafka-producer-network-thread | producer-198] INFO TransactionManager - [Producer clientId=producer-198] ProducerId set to 14295 with epoch 0
13:47:25.908 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.908 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.908 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.908 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045908
13:47:25.909 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-200
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.909 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-200] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.909 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-200] Instantiated an idempotent producer.
13:47:25.910 [kafka-producer-network-thread | producer-199] INFO Metadata - [Producer clientId=producer-199] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.910 [kafka-producer-network-thread | producer-199] INFO TransactionManager - [Producer clientId=producer-199] ProducerId set to 13412 with epoch 0
13:47:25.911 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.911 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.911 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.911 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045911
13:47:25.911 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-201
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.912 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-201] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.912 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-201] Instantiated an idempotent producer.
13:47:25.912 [kafka-producer-network-thread | producer-200] INFO Metadata - [Producer clientId=producer-200] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.912 [kafka-producer-network-thread | producer-200] INFO TransactionManager - [Producer clientId=producer-200] ProducerId set to 14298 with epoch 0
13:47:25.914 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.914 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.914 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.914 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045914
13:47:25.914 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-202
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.915 [kafka-producer-network-thread | producer-201] INFO Metadata - [Producer clientId=producer-201] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.915 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-202] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.915 [kafka-producer-network-thread | producer-201] INFO TransactionManager - [Producer clientId=producer-201] ProducerId set to 12386 with epoch 0
13:47:25.915 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-202] Instantiated an idempotent producer.
13:47:25.918 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.918 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.918 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.918 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045918
13:47:25.918 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-203
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.919 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-203] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.919 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-203] Instantiated an idempotent producer.
13:47:25.919 [kafka-producer-network-thread | producer-202] INFO Metadata - [Producer clientId=producer-202] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.919 [kafka-producer-network-thread | producer-202] INFO TransactionManager - [Producer clientId=producer-202] ProducerId set to 12387 with epoch 0
13:47:25.920 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.920 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.920 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.920 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045920
13:47:25.921 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-204
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.921 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-204] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.921 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-204] Instantiated an idempotent producer.
13:47:25.922 [kafka-producer-network-thread | producer-203] INFO Metadata - [Producer clientId=producer-203] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.922 [kafka-producer-network-thread | producer-203] INFO TransactionManager - [Producer clientId=producer-203] ProducerId set to 14302 with epoch 0
13:47:25.924 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.924 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.924 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.924 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045924
13:47:25.924 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-205
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.924 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-205] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.925 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-205] Instantiated an idempotent producer.
13:47:25.925 [kafka-producer-network-thread | producer-204] INFO Metadata - [Producer clientId=producer-204] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.925 [kafka-producer-network-thread | producer-204] INFO TransactionManager - [Producer clientId=producer-204] ProducerId set to 12390 with epoch 0
13:47:25.927 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.927 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.927 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.927 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045927
13:47:25.927 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-206
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.927 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-206] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.927 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-206] Instantiated an idempotent producer.
13:47:25.928 [kafka-producer-network-thread | producer-205] INFO Metadata - [Producer clientId=producer-205] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.928 [kafka-producer-network-thread | producer-205] INFO TransactionManager - [Producer clientId=producer-205] ProducerId set to 14303 with epoch 0
13:47:25.929 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.929 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.929 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.929 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045929
13:47:25.930 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-207
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.930 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-207] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.930 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-207] Instantiated an idempotent producer.
13:47:25.931 [kafka-producer-network-thread | producer-206] INFO Metadata - [Producer clientId=producer-206] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.931 [kafka-producer-network-thread | producer-206] INFO TransactionManager - [Producer clientId=producer-206] ProducerId set to 12392 with epoch 0
13:47:25.932 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.932 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.932 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.932 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045932
13:47:25.932 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-208
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.933 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-208] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.933 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-208] Instantiated an idempotent producer.
13:47:25.934 [kafka-producer-network-thread | producer-207] INFO Metadata - [Producer clientId=producer-207] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.934 [kafka-producer-network-thread | producer-207] INFO TransactionManager - [Producer clientId=producer-207] ProducerId set to 14305 with epoch 0
13:47:25.935 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.935 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.935 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.935 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045935
13:47:25.935 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-209
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.936 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-209] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.936 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-209] Instantiated an idempotent producer.
13:47:25.936 [kafka-producer-network-thread | producer-208] INFO Metadata - [Producer clientId=producer-208] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.936 [kafka-producer-network-thread | producer-208] INFO TransactionManager - [Producer clientId=producer-208] ProducerId set to 12395 with epoch 0
13:47:25.938 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.938 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.938 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.938 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045938
13:47:25.938 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-210
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.939 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-210] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.939 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-210] Instantiated an idempotent producer.
13:47:25.939 [kafka-producer-network-thread | producer-209] INFO Metadata - [Producer clientId=producer-209] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.940 [kafka-producer-network-thread | producer-209] INFO TransactionManager - [Producer clientId=producer-209] ProducerId set to 12396 with epoch 0
13:47:25.941 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.941 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.941 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.941 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045941
13:47:25.941 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-211
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.942 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-211] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.942 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-211] Instantiated an idempotent producer.
13:47:25.943 [kafka-producer-network-thread | producer-210] INFO Metadata - [Producer clientId=producer-210] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.943 [kafka-producer-network-thread | producer-210] INFO TransactionManager - [Producer clientId=producer-210] ProducerId set to 14308 with epoch 0
13:47:25.943 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.943 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.943 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.944 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045943
13:47:25.944 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-212
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.944 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-212] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.944 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-212] Instantiated an idempotent producer.
13:47:25.946 [kafka-producer-network-thread | producer-211] INFO Metadata - [Producer clientId=producer-211] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.946 [kafka-producer-network-thread | producer-211] INFO TransactionManager - [Producer clientId=producer-211] ProducerId set to 14310 with epoch 0
13:47:25.946 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.946 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.946 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.946 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045946
13:47:25.947 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-213
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.947 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-213] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.947 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-213] Instantiated an idempotent producer.
13:47:25.947 [kafka-producer-network-thread | producer-212] INFO Metadata - [Producer clientId=producer-212] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.948 [kafka-producer-network-thread | producer-212] INFO TransactionManager - [Producer clientId=producer-212] ProducerId set to 12401 with epoch 0
13:47:25.949 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.949 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.949 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.949 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045949
13:47:25.949 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-214
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.950 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-214] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.950 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-214] Instantiated an idempotent producer.
13:47:25.950 [kafka-producer-network-thread | producer-213] INFO Metadata - [Producer clientId=producer-213] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.951 [kafka-producer-network-thread | producer-213] INFO TransactionManager - [Producer clientId=producer-213] ProducerId set to 14312 with epoch 0
13:47:25.952 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.952 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.952 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.952 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045952
13:47:25.952 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-215
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.953 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-215] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.953 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-215] Instantiated an idempotent producer.
13:47:25.953 [kafka-producer-network-thread | producer-214] INFO Metadata - [Producer clientId=producer-214] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.953 [kafka-producer-network-thread | producer-214] INFO TransactionManager - [Producer clientId=producer-214] ProducerId set to 14313 with epoch 0
13:47:25.954 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.954 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.954 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.954 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045954
13:47:25.955 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-216
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.955 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-216] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.955 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-216] Instantiated an idempotent producer.
13:47:25.956 [kafka-producer-network-thread | producer-215] INFO Metadata - [Producer clientId=producer-215] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.956 [kafka-producer-network-thread | producer-215] INFO TransactionManager - [Producer clientId=producer-215] ProducerId set to 12405 with epoch 0
13:47:25.957 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.957 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.957 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.957 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045957
13:47:25.958 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-217
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.958 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-217] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.958 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-217] Instantiated an idempotent producer.
13:47:25.958 [kafka-producer-network-thread | producer-216] INFO Metadata - [Producer clientId=producer-216] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.959 [kafka-producer-network-thread | producer-216] INFO TransactionManager - [Producer clientId=producer-216] ProducerId set to 13423 with epoch 0
13:47:25.960 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.960 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.960 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.960 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045960
13:47:25.960 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-218
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.961 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-218] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.961 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-218] Instantiated an idempotent producer.
13:47:25.961 [kafka-producer-network-thread | producer-217] INFO Metadata - [Producer clientId=producer-217] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.962 [kafka-producer-network-thread | producer-217] INFO TransactionManager - [Producer clientId=producer-217] ProducerId set to 13425 with epoch 0
13:47:25.964 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.964 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.964 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.964 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045964
13:47:25.964 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-219
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.965 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-219] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.965 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-219] Instantiated an idempotent producer.
13:47:25.966 [kafka-producer-network-thread | producer-218] INFO Metadata - [Producer clientId=producer-218] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.966 [kafka-producer-network-thread | producer-218] INFO TransactionManager - [Producer clientId=producer-218] ProducerId set to 12408 with epoch 0
13:47:25.967 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.967 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.967 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.967 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045967
13:47:25.968 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-220
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.968 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-220] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.968 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-220] Instantiated an idempotent producer.
13:47:25.969 [kafka-producer-network-thread | producer-219] INFO Metadata - [Producer clientId=producer-219] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.969 [kafka-producer-network-thread | producer-219] INFO TransactionManager - [Producer clientId=producer-219] ProducerId set to 12410 with epoch 0
13:47:25.970 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.970 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.971 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.971 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045970
13:47:25.971 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-221
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.971 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-221] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.972 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-221] Instantiated an idempotent producer.
13:47:25.972 [kafka-producer-network-thread | producer-220] INFO Metadata - [Producer clientId=producer-220] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.972 [kafka-producer-network-thread | producer-220] INFO TransactionManager - [Producer clientId=producer-220] ProducerId set to 13429 with epoch 0
13:47:25.974 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.974 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.974 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.974 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045974
13:47:25.974 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-222
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.974 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-222] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.975 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-222] Instantiated an idempotent producer.
13:47:25.975 [kafka-producer-network-thread | producer-221] INFO Metadata - [Producer clientId=producer-221] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.975 [kafka-producer-network-thread | producer-221] INFO TransactionManager - [Producer clientId=producer-221] ProducerId set to 12413 with epoch 0
13:47:25.976 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.976 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.976 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.976 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045976
13:47:25.978 [kafka-producer-network-thread | producer-222] INFO Metadata - [Producer clientId=producer-222] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.978 [kafka-producer-network-thread | producer-222] INFO TransactionManager - [Producer clientId=producer-222] ProducerId set to 13432 with epoch 0
13:47:25.978 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-223
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.979 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-223] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.979 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-223] Instantiated an idempotent producer.
13:47:25.980 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.980 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.980 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.980 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045980
13:47:25.981 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-224
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.981 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-224] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.981 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-224] Instantiated an idempotent producer.
13:47:25.982 [kafka-producer-network-thread | producer-223] INFO Metadata - [Producer clientId=producer-223] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.982 [kafka-producer-network-thread | producer-223] INFO TransactionManager - [Producer clientId=producer-223] ProducerId set to 14322 with epoch 0
13:47:25.983 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.983 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.983 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.983 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045983
13:47:25.984 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-225
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.984 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-225] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.984 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-225] Instantiated an idempotent producer.
13:47:25.985 [kafka-producer-network-thread | producer-224] INFO Metadata - [Producer clientId=producer-224] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.985 [kafka-producer-network-thread | producer-224] INFO TransactionManager - [Producer clientId=producer-224] ProducerId set to 14324 with epoch 0
13:47:25.986 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.986 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.986 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.986 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045986
13:47:25.986 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-226
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.987 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-226] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.987 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-226] Instantiated an idempotent producer.
13:47:25.988 [kafka-producer-network-thread | producer-225] INFO Metadata - [Producer clientId=producer-225] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.988 [kafka-producer-network-thread | producer-225] INFO TransactionManager - [Producer clientId=producer-225] ProducerId set to 13436 with epoch 0
13:47:25.989 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.989 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.989 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.989 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045989
13:47:25.989 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-227
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.990 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-227] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.990 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-227] Instantiated an idempotent producer.
13:47:25.990 [kafka-producer-network-thread | producer-226] INFO Metadata - [Producer clientId=producer-226] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.991 [kafka-producer-network-thread | producer-226] INFO TransactionManager - [Producer clientId=producer-226] ProducerId set to 12415 with epoch 0
13:47:25.992 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.992 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.992 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.992 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045992
13:47:25.992 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-228
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.993 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-228] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.993 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-228] Instantiated an idempotent producer.
13:47:25.993 [kafka-producer-network-thread | producer-227] INFO Metadata - [Producer clientId=producer-227] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.994 [kafka-producer-network-thread | producer-227] INFO TransactionManager - [Producer clientId=producer-227] ProducerId set to 12417 with epoch 0
13:47:25.994 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.995 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.995 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.995 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045994
13:47:25.995 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-229
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.995 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-229] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.995 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-229] Instantiated an idempotent producer.
13:47:25.996 [kafka-producer-network-thread | producer-228] INFO Metadata - [Producer clientId=producer-228] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:25.996 [kafka-producer-network-thread | producer-228] INFO TransactionManager - [Producer clientId=producer-228] ProducerId set to 13438 with epoch 0
13:47:25.997 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:25.997 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:25.997 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:25.997 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336045997
13:47:25.998 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-230
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:25.998 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-230] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:25.998 [kafka-producer-network-thread | producer-229] INFO Metadata - [Producer clientId=producer-229] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.000 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-230] Instantiated an idempotent producer.
13:47:26.000 [kafka-producer-network-thread | producer-229] INFO TransactionManager - [Producer clientId=producer-229] ProducerId set to 12422 with epoch 0
13:47:26.002 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.002 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.002 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.002 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046002
13:47:26.003 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-231
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.003 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-231] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.003 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-231] Instantiated an idempotent producer.
13:47:26.004 [kafka-producer-network-thread | producer-230] INFO Metadata - [Producer clientId=producer-230] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.004 [kafka-producer-network-thread | producer-230] INFO TransactionManager - [Producer clientId=producer-230] ProducerId set to 14330 with epoch 0
13:47:26.005 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.005 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.005 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.005 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046005
13:47:26.006 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-232
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.006 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-232] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.006 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-232] Instantiated an idempotent producer.
13:47:26.007 [kafka-producer-network-thread | producer-231] INFO Metadata - [Producer clientId=producer-231] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.007 [kafka-producer-network-thread | producer-231] INFO TransactionManager - [Producer clientId=producer-231] ProducerId set to 14332 with epoch 0
13:47:26.008 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.008 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.008 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.008 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046008
13:47:26.008 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-233
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.009 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-233] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.009 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-233] Instantiated an idempotent producer.
13:47:26.009 [kafka-producer-network-thread | producer-232] INFO Metadata - [Producer clientId=producer-232] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.010 [kafka-producer-network-thread | producer-232] INFO TransactionManager - [Producer clientId=producer-232] ProducerId set to 14334 with epoch 0
13:47:26.011 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.011 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.011 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.011 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046011
13:47:26.011 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-234
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.012 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-234] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.012 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-234] Instantiated an idempotent producer.
13:47:26.012 [kafka-producer-network-thread | producer-233] INFO Metadata - [Producer clientId=producer-233] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.012 [kafka-producer-network-thread | producer-233] INFO TransactionManager - [Producer clientId=producer-233] ProducerId set to 13441 with epoch 0
13:47:26.013 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.013 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.013 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.013 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046013
13:47:26.014 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-235
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.014 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-235] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.014 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-235] Instantiated an idempotent producer.
13:47:26.015 [kafka-producer-network-thread | producer-234] INFO Metadata - [Producer clientId=producer-234] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.015 [kafka-producer-network-thread | producer-234] INFO TransactionManager - [Producer clientId=producer-234] ProducerId set to 12428 with epoch 0
13:47:26.016 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.016 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.016 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.016 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046016
13:47:26.016 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-236
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.017 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-236] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.017 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-236] Instantiated an idempotent producer.
13:47:26.018 [kafka-producer-network-thread | producer-235] INFO Metadata - [Producer clientId=producer-235] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.018 [kafka-producer-network-thread | producer-235] INFO TransactionManager - [Producer clientId=producer-235] ProducerId set to 13443 with epoch 0
13:47:26.019 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.019 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.019 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.019 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046019
13:47:26.019 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-237
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.020 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-237] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.020 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-237] Instantiated an idempotent producer.
13:47:26.020 [kafka-producer-network-thread | producer-236] INFO Metadata - [Producer clientId=producer-236] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.020 [kafka-producer-network-thread | producer-236] INFO TransactionManager - [Producer clientId=producer-236] ProducerId set to 14339 with epoch 0
13:47:26.022 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.022 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.022 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.022 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046022
13:47:26.022 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-238
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.022 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-238] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.022 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-238] Instantiated an idempotent producer.
13:47:26.023 [kafka-producer-network-thread | producer-237] INFO Metadata - [Producer clientId=producer-237] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.023 [kafka-producer-network-thread | producer-237] INFO TransactionManager - [Producer clientId=producer-237] ProducerId set to 14340 with epoch 0
13:47:26.024 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.024 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.024 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.024 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046024
13:47:26.025 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-239
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.025 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-239] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.025 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-239] Instantiated an idempotent producer.
13:47:26.027 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.027 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.027 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.027 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046027
13:47:26.028 [kafka-producer-network-thread | producer-238] INFO Metadata - [Producer clientId=producer-238] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.028 [kafka-producer-network-thread | producer-238] INFO TransactionManager - [Producer clientId=producer-238] ProducerId set to 14341 with epoch 0
13:47:26.028 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-240
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.029 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-240] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.029 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-240] Instantiated an idempotent producer.
13:47:26.029 [kafka-producer-network-thread | producer-239] INFO Metadata - [Producer clientId=producer-239] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.030 [kafka-producer-network-thread | producer-239] INFO TransactionManager - [Producer clientId=producer-239] ProducerId set to 12432 with epoch 0
13:47:26.031 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.031 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.031 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.031 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046031
13:47:26.031 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-241
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.032 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-241] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.032 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-241] Instantiated an idempotent producer.
13:47:26.032 [kafka-producer-network-thread | producer-240] INFO Metadata - [Producer clientId=producer-240] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.032 [kafka-producer-network-thread | producer-240] INFO TransactionManager - [Producer clientId=producer-240] ProducerId set to 14343 with epoch 0
13:47:26.034 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.034 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.034 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.034 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046034
13:47:26.034 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-242
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.035 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-242] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.035 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-242] Instantiated an idempotent producer.
13:47:26.036 [kafka-producer-network-thread | producer-241] INFO Metadata - [Producer clientId=producer-241] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.036 [kafka-producer-network-thread | producer-241] INFO TransactionManager - [Producer clientId=producer-241] ProducerId set to 13448 with epoch 0
13:47:26.040 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.040 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.040 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.040 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046040
13:47:26.040 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-243
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.041 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-243] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.041 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-243] Instantiated an idempotent producer.
13:47:26.041 [kafka-producer-network-thread | producer-242] INFO Metadata - [Producer clientId=producer-242] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.042 [kafka-producer-network-thread | producer-242] INFO TransactionManager - [Producer clientId=producer-242] ProducerId set to 12434 with epoch 0
13:47:26.043 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.043 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.043 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.043 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046043
13:47:26.043 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-244
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.044 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-244] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.044 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-244] Instantiated an idempotent producer.
13:47:26.044 [kafka-producer-network-thread | producer-243] INFO Metadata - [Producer clientId=producer-243] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.044 [kafka-producer-network-thread | producer-243] INFO TransactionManager - [Producer clientId=producer-243] ProducerId set to 12436 with epoch 0
13:47:26.046 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.046 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.046 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.046 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046046
13:47:26.046 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-245
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.046 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-245] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.047 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-245] Instantiated an idempotent producer.
13:47:26.048 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.048 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.048 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.048 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046048
13:47:26.049 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-246
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.049 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-246] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.049 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-246] Instantiated an idempotent producer.
13:47:26.050 [kafka-producer-network-thread | producer-244] INFO Metadata - [Producer clientId=producer-244] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.050 [kafka-producer-network-thread | producer-244] INFO TransactionManager - [Producer clientId=producer-244] ProducerId set to 14349 with epoch 0
13:47:26.050 [kafka-producer-network-thread | producer-245] INFO Metadata - [Producer clientId=producer-245] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.050 [kafka-producer-network-thread | producer-245] INFO TransactionManager - [Producer clientId=producer-245] ProducerId set to 14351 with epoch 0
13:47:26.051 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.051 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.051 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.051 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046051
13:47:26.052 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-247
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.052 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-247] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.052 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-247] Instantiated an idempotent producer.
13:47:26.052 [kafka-producer-network-thread | producer-246] INFO Metadata - [Producer clientId=producer-246] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.053 [kafka-producer-network-thread | producer-246] INFO TransactionManager - [Producer clientId=producer-246] ProducerId set to 13450 with epoch 0
13:47:26.054 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.054 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.054 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.054 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046054
13:47:26.055 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-248
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.055 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-248] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.055 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-248] Instantiated an idempotent producer.
13:47:26.056 [kafka-producer-network-thread | producer-247] INFO Metadata - [Producer clientId=producer-247] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.056 [kafka-producer-network-thread | producer-247] INFO TransactionManager - [Producer clientId=producer-247] ProducerId set to 14355 with epoch 0
13:47:26.057 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.057 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.057 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.057 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046057
13:47:26.057 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-249
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.058 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-249] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.058 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-249] Instantiated an idempotent producer.
13:47:26.058 [kafka-producer-network-thread | producer-248] INFO Metadata - [Producer clientId=producer-248] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.059 [kafka-producer-network-thread | producer-248] INFO TransactionManager - [Producer clientId=producer-248] ProducerId set to 13452 with epoch 0
13:47:26.059 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.060 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.060 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.060 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046060
13:47:26.060 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-250
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.060 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-250] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.061 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-250] Instantiated an idempotent producer.
13:47:26.061 [kafka-producer-network-thread | producer-249] INFO Metadata - [Producer clientId=producer-249] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.061 [kafka-producer-network-thread | producer-249] INFO TransactionManager - [Producer clientId=producer-249] ProducerId set to 14357 with epoch 0
13:47:26.062 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.062 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.062 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.062 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046062
13:47:26.063 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-251
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.063 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-251] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.063 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-251] Instantiated an idempotent producer.
13:47:26.064 [kafka-producer-network-thread | producer-250] INFO Metadata - [Producer clientId=producer-250] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.064 [kafka-producer-network-thread | producer-250] INFO TransactionManager - [Producer clientId=producer-250] ProducerId set to 13455 with epoch 0
13:47:26.065 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.065 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.065 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.065 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046065
13:47:26.066 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-252
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.066 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-252] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.066 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-252] Instantiated an idempotent producer.
13:47:26.067 [kafka-producer-network-thread | producer-251] INFO Metadata - [Producer clientId=producer-251] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.067 [kafka-producer-network-thread | producer-251] INFO TransactionManager - [Producer clientId=producer-251] ProducerId set to 13456 with epoch 0
13:47:26.068 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.068 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.068 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.068 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046068
13:47:26.069 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-253
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.069 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-253] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.069 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-253] Instantiated an idempotent producer.
13:47:26.070 [kafka-producer-network-thread | producer-252] INFO Metadata - [Producer clientId=producer-252] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.070 [kafka-producer-network-thread | producer-252] INFO TransactionManager - [Producer clientId=producer-252] ProducerId set to 14361 with epoch 0
13:47:26.071 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.071 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.071 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.071 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046071
13:47:26.071 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-254
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.072 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-254] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.072 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-254] Instantiated an idempotent producer.
13:47:26.073 [kafka-producer-network-thread | producer-253] INFO Metadata - [Producer clientId=producer-253] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.073 [kafka-producer-network-thread | producer-253] INFO TransactionManager - [Producer clientId=producer-253] ProducerId set to 12444 with epoch 0
13:47:26.074 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.074 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.074 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.074 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046074
13:47:26.074 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-255
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.075 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-255] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.075 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-255] Instantiated an idempotent producer.
13:47:26.075 [kafka-producer-network-thread | producer-254] INFO Metadata - [Producer clientId=producer-254] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.075 [kafka-producer-network-thread | producer-254] INFO TransactionManager - [Producer clientId=producer-254] ProducerId set to 14364 with epoch 0
13:47:26.077 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.077 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.077 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.077 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046077
13:47:26.077 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-256
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.077 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-256] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.078 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-256] Instantiated an idempotent producer.
13:47:26.078 [kafka-producer-network-thread | producer-255] INFO Metadata - [Producer clientId=producer-255] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.078 [kafka-producer-network-thread | producer-255] INFO TransactionManager - [Producer clientId=producer-255] ProducerId set to 13458 with epoch 0
13:47:26.079 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.080 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.080 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.080 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046080
13:47:26.080 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-257
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.081 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-257] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.081 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-257] Instantiated an idempotent producer.
13:47:26.082 [kafka-producer-network-thread | producer-256] INFO Metadata - [Producer clientId=producer-256] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.082 [kafka-producer-network-thread | producer-256] INFO TransactionManager - [Producer clientId=producer-256] ProducerId set to 12446 with epoch 0
13:47:26.085 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.085 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.085 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.085 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046085
13:47:26.085 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-258
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.086 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-258] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.086 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-258] Instantiated an idempotent producer.
13:47:26.086 [kafka-producer-network-thread | producer-257] INFO Metadata - [Producer clientId=producer-257] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.086 [kafka-producer-network-thread | producer-257] INFO TransactionManager - [Producer clientId=producer-257] ProducerId set to 14367 with epoch 0
13:47:26.089 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-8cAUgF8-1, groupId=sub-001-8cAUgF8] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-001-8cAUgF8-1-869910e3-f648-4f81-b323-8ddf19638628', protocol='range'}
13:47:26.090 [pool-8-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-1XVEdnU-3, groupId=sub-007-1XVEdnU] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-007-1XVEdnU-3-e874697d-d29b-4559-bf2e-ab10ea1deac1', protocol='range'}
13:47:26.091 [pool-7-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-qJr9NHU-2, groupId=sub-004-qJr9NHU] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-004-qJr9NHU-2-10f24e90-59b2-4042-a108-8091b22f4022', protocol='range'}
13:47:26.093 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.093 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.093 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.093 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046093
13:47:26.094 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-259
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.095 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-259] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.095 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-259] Instantiated an idempotent producer.
13:47:26.099 [pool-8-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-1XVEdnU-3, groupId=sub-007-1XVEdnU] Finished assignment for group at generation 1: {consumer-sub-007-1XVEdnU-3-e874697d-d29b-4559-bf2e-ab10ea1deac1=Assignment(partitions=[test-topic-0000007-0MzD9Ik-0])}
13:47:26.100 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-8cAUgF8-1, groupId=sub-001-8cAUgF8] Finished assignment for group at generation 1: {consumer-sub-001-8cAUgF8-1-869910e3-f648-4f81-b323-8ddf19638628=Assignment(partitions=[test-topic-0000001-8XkVdYY-0])}
13:47:26.100 [pool-7-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-qJr9NHU-2, groupId=sub-004-qJr9NHU] Finished assignment for group at generation 1: {consumer-sub-004-qJr9NHU-2-10f24e90-59b2-4042-a108-8091b22f4022=Assignment(partitions=[test-topic-0000004-20ZHReU-0])}
13:47:26.101 [kafka-producer-network-thread | producer-258] INFO Metadata - [Producer clientId=producer-258] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.101 [kafka-producer-network-thread | producer-258] INFO TransactionManager - [Producer clientId=producer-258] ProducerId set to 14369 with epoch 0
13:47:26.102 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.102 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.102 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.102 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046102
13:47:26.103 [pool-9-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-0wq8Jaw-4, groupId=sub-010-0wq8Jaw] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-010-0wq8Jaw-4-1d286d60-eae3-434a-bf10-2a15be89d1ef', protocol='range'}
13:47:26.103 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-260
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.103 [pool-9-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-0wq8Jaw-4, groupId=sub-010-0wq8Jaw] Finished assignment for group at generation 1: {consumer-sub-010-0wq8Jaw-4-1d286d60-eae3-434a-bf10-2a15be89d1ef=Assignment(partitions=[test-topic-0000010-NyjSQ9A-0])}
13:47:26.104 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-260] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.104 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-260] Instantiated an idempotent producer.
13:47:26.104 [kafka-producer-network-thread | producer-259] INFO Metadata - [Producer clientId=producer-259] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.105 [kafka-producer-network-thread | producer-259] INFO TransactionManager - [Producer clientId=producer-259] ProducerId set to 14370 with epoch 0
13:47:26.106 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.106 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.106 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.106 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046106
13:47:26.106 [pool-9-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-0wq8Jaw-4, groupId=sub-010-0wq8Jaw] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-010-0wq8Jaw-4-1d286d60-eae3-434a-bf10-2a15be89d1ef', protocol='range'}
13:47:26.106 [pool-7-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-qJr9NHU-2, groupId=sub-004-qJr9NHU] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-004-qJr9NHU-2-10f24e90-59b2-4042-a108-8091b22f4022', protocol='range'}
13:47:26.106 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-261
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.106 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-8cAUgF8-1, groupId=sub-001-8cAUgF8] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-001-8cAUgF8-1-869910e3-f648-4f81-b323-8ddf19638628', protocol='range'}
13:47:26.106 [pool-7-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-qJr9NHU-2, groupId=sub-004-qJr9NHU] Notifying assignor about the new Assignment(partitions=[test-topic-0000004-20ZHReU-0])
13:47:26.106 [pool-9-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-0wq8Jaw-4, groupId=sub-010-0wq8Jaw] Notifying assignor about the new Assignment(partitions=[test-topic-0000010-NyjSQ9A-0])
13:47:26.106 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-8cAUgF8-1, groupId=sub-001-8cAUgF8] Notifying assignor about the new Assignment(partitions=[test-topic-0000001-8XkVdYY-0])
13:47:26.107 [pool-8-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-1XVEdnU-3, groupId=sub-007-1XVEdnU] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-007-1XVEdnU-3-e874697d-d29b-4559-bf2e-ab10ea1deac1', protocol='range'}
13:47:26.107 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-261] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.107 [pool-8-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-1XVEdnU-3, groupId=sub-007-1XVEdnU] Notifying assignor about the new Assignment(partitions=[test-topic-0000007-0MzD9Ik-0])
13:47:26.107 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-261] Instantiated an idempotent producer.
13:47:26.109 [kafka-producer-network-thread | producer-260] INFO Metadata - [Producer clientId=producer-260] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.109 [kafka-producer-network-thread | producer-260] INFO TransactionManager - [Producer clientId=producer-260] ProducerId set to 13465 with epoch 0
13:47:26.109 [pool-9-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-0wq8Jaw-4, groupId=sub-010-0wq8Jaw] Adding newly assigned partitions: test-topic-0000010-NyjSQ9A-0
13:47:26.109 [pool-7-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-qJr9NHU-2, groupId=sub-004-qJr9NHU] Adding newly assigned partitions: test-topic-0000004-20ZHReU-0
13:47:26.109 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-8cAUgF8-1, groupId=sub-001-8cAUgF8] Adding newly assigned partitions: test-topic-0000001-8XkVdYY-0
13:47:26.109 [pool-8-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-1XVEdnU-3, groupId=sub-007-1XVEdnU] Adding newly assigned partitions: test-topic-0000007-0MzD9Ik-0
13:47:26.109 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.109 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.109 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.110 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046109
13:47:26.111 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-262
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.112 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-262] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.112 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-4EId-xE-5, groupId=sub-013-4EId-xE] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-013-4EId-xE-5-19c12c65-1824-4f5b-b0f7-87751d065275', protocol='range'}
13:47:26.112 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-262] Instantiated an idempotent producer.
13:47:26.112 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-4EId-xE-5, groupId=sub-013-4EId-xE] Finished assignment for group at generation 1: {consumer-sub-013-4EId-xE-5-19c12c65-1824-4f5b-b0f7-87751d065275=Assignment(partitions=[test-topic-0000013-FjDWcwQ-0])}
13:47:26.114 [kafka-producer-network-thread | producer-261] INFO Metadata - [Producer clientId=producer-261] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.114 [kafka-producer-network-thread | producer-261] INFO TransactionManager - [Producer clientId=producer-261] ProducerId set to 12455 with epoch 0
13:47:26.114 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-4EId-xE-5, groupId=sub-013-4EId-xE] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-013-4EId-xE-5-19c12c65-1824-4f5b-b0f7-87751d065275', protocol='range'}
13:47:26.114 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-4EId-xE-5, groupId=sub-013-4EId-xE] Notifying assignor about the new Assignment(partitions=[test-topic-0000013-FjDWcwQ-0])
13:47:26.114 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-4EId-xE-5, groupId=sub-013-4EId-xE] Adding newly assigned partitions: test-topic-0000013-FjDWcwQ-0
13:47:26.117 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.117 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.117 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.117 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046117
13:47:26.117 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-263
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.118 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-263] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.118 [pool-7-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-qJr9NHU-2, groupId=sub-004-qJr9NHU] Found no committed offset for partition test-topic-0000004-20ZHReU-0
13:47:26.118 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-8cAUgF8-1, groupId=sub-001-8cAUgF8] Found no committed offset for partition test-topic-0000001-8XkVdYY-0
13:47:26.118 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-4EId-xE-5, groupId=sub-013-4EId-xE] Found no committed offset for partition test-topic-0000013-FjDWcwQ-0
13:47:26.119 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-263] Instantiated an idempotent producer.
13:47:26.121 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.121 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.121 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.121 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046121
13:47:26.121 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-264
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.118 [pool-9-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-0wq8Jaw-4, groupId=sub-010-0wq8Jaw] Found no committed offset for partition test-topic-0000010-NyjSQ9A-0
13:47:26.118 [kafka-producer-network-thread | producer-262] INFO Metadata - [Producer clientId=producer-262] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.123 [pool-8-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-1XVEdnU-3, groupId=sub-007-1XVEdnU] Found no committed offset for partition test-topic-0000007-0MzD9Ik-0
13:47:26.123 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-IXOF7Yo-6, groupId=sub-016-IXOF7Yo] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-016-IXOF7Yo-6-686f61ba-2433-44d6-8d7a-292102d0c9c6', protocol='range'}
13:47:26.123 [kafka-producer-network-thread | producer-262] INFO TransactionManager - [Producer clientId=producer-262] ProducerId set to 14374 with epoch 0
13:47:26.124 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-IXOF7Yo-6, groupId=sub-016-IXOF7Yo] Finished assignment for group at generation 1: {consumer-sub-016-IXOF7Yo-6-686f61ba-2433-44d6-8d7a-292102d0c9c6=Assignment(partitions=[test-topic-0000016-sZJ3Jho-0])}
13:47:26.124 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-264] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.124 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-264] Instantiated an idempotent producer.
13:47:26.126 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-IXOF7Yo-6, groupId=sub-016-IXOF7Yo] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-016-IXOF7Yo-6-686f61ba-2433-44d6-8d7a-292102d0c9c6', protocol='range'}
13:47:26.126 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-IXOF7Yo-6, groupId=sub-016-IXOF7Yo] Notifying assignor about the new Assignment(partitions=[test-topic-0000016-sZJ3Jho-0])
13:47:26.126 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-IXOF7Yo-6, groupId=sub-016-IXOF7Yo] Adding newly assigned partitions: test-topic-0000016-sZJ3Jho-0
13:47:26.126 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-IXOF7Yo-6, groupId=sub-016-IXOF7Yo] Found no committed offset for partition test-topic-0000016-sZJ3Jho-0
13:47:26.128 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.128 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.128 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.128 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046128
13:47:26.129 [kafka-producer-network-thread | producer-263] INFO Metadata - [Producer clientId=producer-263] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.129 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-265
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.130 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-265] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.130 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-265] Instantiated an idempotent producer.
13:47:26.130 [pool-9-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-010-0wq8Jaw-4, groupId=sub-010-0wq8Jaw] Resetting offset for partition test-topic-0000010-NyjSQ9A-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.130 [pool-11-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-016-IXOF7Yo-6, groupId=sub-016-IXOF7Yo] Resetting offset for partition test-topic-0000016-sZJ3Jho-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.131 [pool-6-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-001-8cAUgF8-1, groupId=sub-001-8cAUgF8] Resetting offset for partition test-topic-0000001-8XkVdYY-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.131 [pool-10-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-013-4EId-xE-5, groupId=sub-013-4EId-xE] Resetting offset for partition test-topic-0000013-FjDWcwQ-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.132 [pool-7-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-004-qJr9NHU-2, groupId=sub-004-qJr9NHU] Resetting offset for partition test-topic-0000004-20ZHReU-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.132 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.132 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.132 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.132 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046132
13:47:26.133 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-266
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.134 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-266] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.134 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-266] Instantiated an idempotent producer.
13:47:26.134 [kafka-producer-network-thread | producer-265] INFO Metadata - [Producer clientId=producer-265] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.135 [kafka-producer-network-thread | producer-265] INFO TransactionManager - [Producer clientId=producer-265] ProducerId set to 12462 with epoch 0
13:47:26.136 [pool-8-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-007-1XVEdnU-3, groupId=sub-007-1XVEdnU] Resetting offset for partition test-topic-0000007-0MzD9Ik-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.137 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.137 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.137 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.137 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046137
13:47:26.137 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-267
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.137 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-ka4JHhg-7, groupId=sub-019-ka4JHhg] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-019-ka4JHhg-7-08485441-8eb0-490f-b320-476da927fe70', protocol='range'}
13:47:26.138 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-ka4JHhg-7, groupId=sub-019-ka4JHhg] Finished assignment for group at generation 1: {consumer-sub-019-ka4JHhg-7-08485441-8eb0-490f-b320-476da927fe70=Assignment(partitions=[test-topic-0000019-KoBZyHg-0])}
13:47:26.138 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-267] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.138 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-267] Instantiated an idempotent producer.
13:47:26.138 [kafka-producer-network-thread | producer-263] INFO TransactionManager - [Producer clientId=producer-263] ProducerId set to 12459 with epoch 0
13:47:26.138 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-c_9WhqM-8, groupId=sub-022-c_9WhqM] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-022-c_9WhqM-8-0cfedd2c-5e4b-439c-89f5-c530f1fd75b0', protocol='range'}
13:47:26.139 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-c_9WhqM-8, groupId=sub-022-c_9WhqM] Finished assignment for group at generation 1: {consumer-sub-022-c_9WhqM-8-0cfedd2c-5e4b-439c-89f5-c530f1fd75b0=Assignment(partitions=[test-topic-0000022-SNsKA5U-0])}
13:47:26.139 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-ka4JHhg-7, groupId=sub-019-ka4JHhg] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-019-ka4JHhg-7-08485441-8eb0-490f-b320-476da927fe70', protocol='range'}
13:47:26.140 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-ka4JHhg-7, groupId=sub-019-ka4JHhg] Notifying assignor about the new Assignment(partitions=[test-topic-0000019-KoBZyHg-0])
13:47:26.140 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-ka4JHhg-7, groupId=sub-019-ka4JHhg] Adding newly assigned partitions: test-topic-0000019-KoBZyHg-0
13:47:26.140 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-ka4JHhg-7, groupId=sub-019-ka4JHhg] Found no committed offset for partition test-topic-0000019-KoBZyHg-0
13:47:26.141 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-c_9WhqM-8, groupId=sub-022-c_9WhqM] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-022-c_9WhqM-8-0cfedd2c-5e4b-439c-89f5-c530f1fd75b0', protocol='range'}
13:47:26.141 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-c_9WhqM-8, groupId=sub-022-c_9WhqM] Notifying assignor about the new Assignment(partitions=[test-topic-0000022-SNsKA5U-0])
13:47:26.141 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-c_9WhqM-8, groupId=sub-022-c_9WhqM] Adding newly assigned partitions: test-topic-0000022-SNsKA5U-0
13:47:26.141 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.141 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.141 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.141 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046141
13:47:26.141 [kafka-producer-network-thread | producer-266] INFO Metadata - [Producer clientId=producer-266] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.141 [kafka-producer-network-thread | producer-264] INFO Metadata - [Producer clientId=producer-264] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.142 [kafka-producer-network-thread | producer-266] INFO TransactionManager - [Producer clientId=producer-266] ProducerId set to 14380 with epoch 0
13:47:26.142 [kafka-producer-network-thread | producer-264] INFO TransactionManager - [Producer clientId=producer-264] ProducerId set to 12465 with epoch 0
13:47:26.142 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-c_9WhqM-8, groupId=sub-022-c_9WhqM] Found no committed offset for partition test-topic-0000022-SNsKA5U-0
13:47:26.142 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-268
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.142 [pool-12-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-019-ka4JHhg-7, groupId=sub-019-ka4JHhg] Resetting offset for partition test-topic-0000019-KoBZyHg-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.142 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-268] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.143 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-268] Instantiated an idempotent producer.
13:47:26.143 [kafka-producer-network-thread | producer-267] INFO Metadata - [Producer clientId=producer-267] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.143 [kafka-producer-network-thread | producer-267] INFO TransactionManager - [Producer clientId=producer-267] ProducerId set to 13472 with epoch 0
13:47:26.144 [pool-13-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-022-c_9WhqM-8, groupId=sub-022-c_9WhqM] Resetting offset for partition test-topic-0000022-SNsKA5U-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.144 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.144 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.145 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.145 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046144
13:47:26.145 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-269
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.145 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-269] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.145 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-269] Instantiated an idempotent producer.
13:47:26.146 [kafka-producer-network-thread | producer-268] INFO Metadata - [Producer clientId=producer-268] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.146 [kafka-producer-network-thread | producer-268] INFO TransactionManager - [Producer clientId=producer-268] ProducerId set to 12467 with epoch 0
13:47:26.147 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.147 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.147 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.147 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046147
13:47:26.148 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-270
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.148 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-270] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.148 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-270] Instantiated an idempotent producer.
13:47:26.149 [kafka-producer-network-thread | producer-269] INFO Metadata - [Producer clientId=producer-269] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.149 [kafka-producer-network-thread | producer-269] INFO TransactionManager - [Producer clientId=producer-269] ProducerId set to 14382 with epoch 0
13:47:26.150 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.150 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.150 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.150 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046150
13:47:26.150 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-KJTUWl8-9, groupId=sub-025-KJTUWl8] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-025-KJTUWl8-9-f2738c01-4377-4b85-95e4-2f04cae6ec84', protocol='range'}
13:47:26.151 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-KJTUWl8-9, groupId=sub-025-KJTUWl8] Finished assignment for group at generation 1: {consumer-sub-025-KJTUWl8-9-f2738c01-4377-4b85-95e4-2f04cae6ec84=Assignment(partitions=[test-topic-0000025-6uZz340-0])}
13:47:26.151 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-271
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.151 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-271] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.151 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-271] Instantiated an idempotent producer.
13:47:26.151 [kafka-producer-network-thread | producer-270] INFO Metadata - [Producer clientId=producer-270] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.152 [kafka-producer-network-thread | producer-270] INFO TransactionManager - [Producer clientId=producer-270] ProducerId set to 14383 with epoch 0
13:47:26.152 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-KJTUWl8-9, groupId=sub-025-KJTUWl8] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-025-KJTUWl8-9-f2738c01-4377-4b85-95e4-2f04cae6ec84', protocol='range'}
13:47:26.152 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-b0xyLAc-10, groupId=sub-028-b0xyLAc] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-028-b0xyLAc-10-007bb323-4fcf-47ef-b092-03d27bf3667f', protocol='range'}
13:47:26.152 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-KJTUWl8-9, groupId=sub-025-KJTUWl8] Notifying assignor about the new Assignment(partitions=[test-topic-0000025-6uZz340-0])
13:47:26.153 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-KJTUWl8-9, groupId=sub-025-KJTUWl8] Adding newly assigned partitions: test-topic-0000025-6uZz340-0
13:47:26.153 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-b0xyLAc-10, groupId=sub-028-b0xyLAc] Finished assignment for group at generation 1: {consumer-sub-028-b0xyLAc-10-007bb323-4fcf-47ef-b092-03d27bf3667f=Assignment(partitions=[test-topic-0000028-3nLVSQs-0])}
13:47:26.153 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.153 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.153 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.153 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046153
13:47:26.153 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-KJTUWl8-9, groupId=sub-025-KJTUWl8] Found no committed offset for partition test-topic-0000025-6uZz340-0
13:47:26.153 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-272
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.154 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-272] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.154 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-b0xyLAc-10, groupId=sub-028-b0xyLAc] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-028-b0xyLAc-10-007bb323-4fcf-47ef-b092-03d27bf3667f', protocol='range'}
13:47:26.154 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-272] Instantiated an idempotent producer.
13:47:26.155 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-b0xyLAc-10, groupId=sub-028-b0xyLAc] Notifying assignor about the new Assignment(partitions=[test-topic-0000028-3nLVSQs-0])
13:47:26.155 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-b0xyLAc-10, groupId=sub-028-b0xyLAc] Adding newly assigned partitions: test-topic-0000028-3nLVSQs-0
13:47:26.155 [kafka-producer-network-thread | producer-271] INFO Metadata - [Producer clientId=producer-271] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.155 [kafka-producer-network-thread | producer-271] INFO TransactionManager - [Producer clientId=producer-271] ProducerId set to 12470 with epoch 0
13:47:26.155 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-b0xyLAc-10, groupId=sub-028-b0xyLAc] Found no committed offset for partition test-topic-0000028-3nLVSQs-0
13:47:26.155 [pool-14-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-025-KJTUWl8-9, groupId=sub-025-KJTUWl8] Resetting offset for partition test-topic-0000025-6uZz340-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.156 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.156 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.156 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.156 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046156
13:47:26.157 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-273
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.157 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-273] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.157 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-273] Instantiated an idempotent producer.
13:47:26.159 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-f2tqkFU-11, groupId=sub-031-f2tqkFU] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-031-f2tqkFU-11-dac8594c-a57b-4152-9ce6-7c7e5411443d', protocol='range'}
13:47:26.159 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-f2tqkFU-11, groupId=sub-031-f2tqkFU] Finished assignment for group at generation 1: {consumer-sub-031-f2tqkFU-11-dac8594c-a57b-4152-9ce6-7c7e5411443d=Assignment(partitions=[test-topic-0000031-DjUOkAw-0])}
13:47:26.160 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-f2tqkFU-11, groupId=sub-031-f2tqkFU] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-031-f2tqkFU-11-dac8594c-a57b-4152-9ce6-7c7e5411443d', protocol='range'}
13:47:26.160 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-f2tqkFU-11, groupId=sub-031-f2tqkFU] Notifying assignor about the new Assignment(partitions=[test-topic-0000031-DjUOkAw-0])
13:47:26.160 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-f2tqkFU-11, groupId=sub-031-f2tqkFU] Adding newly assigned partitions: test-topic-0000031-DjUOkAw-0
13:47:26.161 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-f2tqkFU-11, groupId=sub-031-f2tqkFU] Found no committed offset for partition test-topic-0000031-DjUOkAw-0
13:47:26.164 [pool-15-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-028-b0xyLAc-10, groupId=sub-028-b0xyLAc] Resetting offset for partition test-topic-0000028-3nLVSQs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.164 [pool-16-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-031-f2tqkFU-11, groupId=sub-031-f2tqkFU] Resetting offset for partition test-topic-0000031-DjUOkAw-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.164 [kafka-producer-network-thread | producer-272] INFO Metadata - [Producer clientId=producer-272] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.165 [kafka-producer-network-thread | producer-272] INFO TransactionManager - [Producer clientId=producer-272] ProducerId set to 13477 with epoch 0
13:47:26.165 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.165 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.165 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.165 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046165
13:47:26.165 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-274
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.166 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-274] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.166 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-274] Instantiated an idempotent producer.
13:47:26.167 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-e9D4N40-12, groupId=sub-034-e9D4N40] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-034-e9D4N40-12-42255493-6dae-40d8-800a-0abeea6f68c3', protocol='range'}
13:47:26.167 [kafka-producer-network-thread | producer-273] INFO Metadata - [Producer clientId=producer-273] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.167 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-e9D4N40-12, groupId=sub-034-e9D4N40] Finished assignment for group at generation 1: {consumer-sub-034-e9D4N40-12-42255493-6dae-40d8-800a-0abeea6f68c3=Assignment(partitions=[test-topic-0000034-P1Rw2WM-0])}
13:47:26.167 [kafka-producer-network-thread | producer-273] INFO TransactionManager - [Producer clientId=producer-273] ProducerId set to 13478 with epoch 0
13:47:26.168 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.168 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.168 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.168 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046168
13:47:26.168 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-275
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.169 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-e9D4N40-12, groupId=sub-034-e9D4N40] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-034-e9D4N40-12-42255493-6dae-40d8-800a-0abeea6f68c3', protocol='range'}
13:47:26.169 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-275] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.169 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-e9D4N40-12, groupId=sub-034-e9D4N40] Notifying assignor about the new Assignment(partitions=[test-topic-0000034-P1Rw2WM-0])
13:47:26.169 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-275] Instantiated an idempotent producer.
13:47:26.169 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-e9D4N40-12, groupId=sub-034-e9D4N40] Adding newly assigned partitions: test-topic-0000034-P1Rw2WM-0
13:47:26.169 [kafka-producer-network-thread | producer-274] INFO Metadata - [Producer clientId=producer-274] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.169 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-e9D4N40-12, groupId=sub-034-e9D4N40] Found no committed offset for partition test-topic-0000034-P1Rw2WM-0
13:47:26.169 [kafka-producer-network-thread | producer-274] INFO TransactionManager - [Producer clientId=producer-274] ProducerId set to 14391 with epoch 0
13:47:26.171 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.171 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.171 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.171 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046171
13:47:26.171 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-276
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.171 [pool-17-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-034-e9D4N40-12, groupId=sub-034-e9D4N40] Resetting offset for partition test-topic-0000034-P1Rw2WM-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.172 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-276] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.172 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-276] Instantiated an idempotent producer.
13:47:26.172 [kafka-producer-network-thread | producer-275] INFO Metadata - [Producer clientId=producer-275] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.172 [kafka-producer-network-thread | producer-275] INFO TransactionManager - [Producer clientId=producer-275] ProducerId set to 12472 with epoch 0
13:47:26.173 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.173 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.173 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.173 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046173
13:47:26.174 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-277
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.174 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-KMpaHu8-13, groupId=sub-037-KMpaHu8] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-037-KMpaHu8-13-8467a1ea-0a48-4993-8497-f15521f1a240', protocol='range'}
13:47:26.174 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-KMpaHu8-13, groupId=sub-037-KMpaHu8] Finished assignment for group at generation 1: {consumer-sub-037-KMpaHu8-13-8467a1ea-0a48-4993-8497-f15521f1a240=Assignment(partitions=[test-topic-0000037-2r-FtRk-0])}
13:47:26.174 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-277] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.174 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-277] Instantiated an idempotent producer.
13:47:26.175 [kafka-producer-network-thread | producer-276] INFO Metadata - [Producer clientId=producer-276] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.175 [kafka-producer-network-thread | producer-276] INFO TransactionManager - [Producer clientId=producer-276] ProducerId set to 12474 with epoch 0
13:47:26.176 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-KMpaHu8-13, groupId=sub-037-KMpaHu8] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-037-KMpaHu8-13-8467a1ea-0a48-4993-8497-f15521f1a240', protocol='range'}
13:47:26.176 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-KMpaHu8-13, groupId=sub-037-KMpaHu8] Notifying assignor about the new Assignment(partitions=[test-topic-0000037-2r-FtRk-0])
13:47:26.176 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.176 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-KMpaHu8-13, groupId=sub-037-KMpaHu8] Adding newly assigned partitions: test-topic-0000037-2r-FtRk-0
13:47:26.176 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.176 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.176 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046176
13:47:26.176 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-278
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.177 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-KMpaHu8-13, groupId=sub-037-KMpaHu8] Found no committed offset for partition test-topic-0000037-2r-FtRk-0
13:47:26.177 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-278] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.177 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-278] Instantiated an idempotent producer.
13:47:26.178 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040--rMXIGQ-14, groupId=sub-040--rMXIGQ] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-040--rMXIGQ-14-85be1785-b452-4f52-93e8-61d0ecb6f84b', protocol='range'}
13:47:26.178 [kafka-producer-network-thread | producer-277] INFO Metadata - [Producer clientId=producer-277] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.178 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040--rMXIGQ-14, groupId=sub-040--rMXIGQ] Finished assignment for group at generation 1: {consumer-sub-040--rMXIGQ-14-85be1785-b452-4f52-93e8-61d0ecb6f84b=Assignment(partitions=[test-topic-0000040-ctRrJ_E-0])}
13:47:26.178 [kafka-producer-network-thread | producer-277] INFO TransactionManager - [Producer clientId=producer-277] ProducerId set to 12475 with epoch 0
13:47:26.178 [pool-18-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-037-KMpaHu8-13, groupId=sub-037-KMpaHu8] Resetting offset for partition test-topic-0000037-2r-FtRk-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.180 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040--rMXIGQ-14, groupId=sub-040--rMXIGQ] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-040--rMXIGQ-14-85be1785-b452-4f52-93e8-61d0ecb6f84b', protocol='range'}
13:47:26.180 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040--rMXIGQ-14, groupId=sub-040--rMXIGQ] Notifying assignor about the new Assignment(partitions=[test-topic-0000040-ctRrJ_E-0])
13:47:26.180 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040--rMXIGQ-14, groupId=sub-040--rMXIGQ] Adding newly assigned partitions: test-topic-0000040-ctRrJ_E-0
13:47:26.180 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040--rMXIGQ-14, groupId=sub-040--rMXIGQ] Found no committed offset for partition test-topic-0000040-ctRrJ_E-0
13:47:26.181 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.181 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.181 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.181 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046181
13:47:26.181 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-279
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.182 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-279] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.182 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-279] Instantiated an idempotent producer.
13:47:26.183 [pool-19-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-040--rMXIGQ-14, groupId=sub-040--rMXIGQ] Resetting offset for partition test-topic-0000040-ctRrJ_E-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.183 [kafka-producer-network-thread | producer-278] INFO Metadata - [Producer clientId=producer-278] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.183 [kafka-producer-network-thread | producer-278] INFO TransactionManager - [Producer clientId=producer-278] ProducerId set to 13481 with epoch 0
13:47:26.184 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-ydmoIdM-15, groupId=sub-043-ydmoIdM] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-043-ydmoIdM-15-d1883fd7-194e-4cd3-a303-c4c2f26c1e3d', protocol='range'}
13:47:26.184 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.184 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.184 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.184 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046184
13:47:26.184 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-ydmoIdM-15, groupId=sub-043-ydmoIdM] Finished assignment for group at generation 1: {consumer-sub-043-ydmoIdM-15-d1883fd7-194e-4cd3-a303-c4c2f26c1e3d=Assignment(partitions=[test-topic-0000043-CI9z7bU-0])}
13:47:26.185 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-280
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.185 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-280] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.185 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-280] Instantiated an idempotent producer.
13:47:26.186 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-ydmoIdM-15, groupId=sub-043-ydmoIdM] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-043-ydmoIdM-15-d1883fd7-194e-4cd3-a303-c4c2f26c1e3d', protocol='range'}
13:47:26.186 [kafka-producer-network-thread | producer-279] INFO Metadata - [Producer clientId=producer-279] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.186 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-ydmoIdM-15, groupId=sub-043-ydmoIdM] Notifying assignor about the new Assignment(partitions=[test-topic-0000043-CI9z7bU-0])
13:47:26.186 [kafka-producer-network-thread | producer-279] INFO TransactionManager - [Producer clientId=producer-279] ProducerId set to 12476 with epoch 0
13:47:26.186 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-ydmoIdM-15, groupId=sub-043-ydmoIdM] Adding newly assigned partitions: test-topic-0000043-CI9z7bU-0
13:47:26.186 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-ydmoIdM-15, groupId=sub-043-ydmoIdM] Found no committed offset for partition test-topic-0000043-CI9z7bU-0
13:47:26.187 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.187 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.187 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.187 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046187
13:47:26.188 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-281
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.188 [pool-20-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-043-ydmoIdM-15, groupId=sub-043-ydmoIdM] Resetting offset for partition test-topic-0000043-CI9z7bU-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.188 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-281] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.188 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-281] Instantiated an idempotent producer.
13:47:26.189 [kafka-producer-network-thread | producer-280] INFO Metadata - [Producer clientId=producer-280] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.189 [kafka-producer-network-thread | producer-280] INFO TransactionManager - [Producer clientId=producer-280] ProducerId set to 12477 with epoch 0
13:47:26.190 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.190 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.190 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.190 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046190
13:47:26.191 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-282
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.191 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-282] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.191 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-282] Instantiated an idempotent producer.
13:47:26.191 [kafka-producer-network-thread | producer-281] INFO Metadata - [Producer clientId=producer-281] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.191 [kafka-producer-network-thread | producer-281] INFO TransactionManager - [Producer clientId=producer-281] ProducerId set to 13483 with epoch 0
13:47:26.193 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-y_Q8I9c-16, groupId=sub-046-y_Q8I9c] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-046-y_Q8I9c-16-e00d26a5-5f3d-4533-8d8a-b7b781258498', protocol='range'}
13:47:26.193 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.193 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.193 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.193 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046193
13:47:26.193 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-y_Q8I9c-16, groupId=sub-046-y_Q8I9c] Finished assignment for group at generation 1: {consumer-sub-046-y_Q8I9c-16-e00d26a5-5f3d-4533-8d8a-b7b781258498=Assignment(partitions=[test-topic-0000046-P0FMkh4-0])}
13:47:26.193 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-283
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.194 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-283] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.194 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-283] Instantiated an idempotent producer.
13:47:26.195 [kafka-producer-network-thread | producer-282] INFO Metadata - [Producer clientId=producer-282] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.195 [kafka-producer-network-thread | producer-282] INFO TransactionManager - [Producer clientId=producer-282] ProducerId set to 14401 with epoch 0
13:47:26.195 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-y_Q8I9c-16, groupId=sub-046-y_Q8I9c] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-046-y_Q8I9c-16-e00d26a5-5f3d-4533-8d8a-b7b781258498', protocol='range'}
13:47:26.195 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-y_Q8I9c-16, groupId=sub-046-y_Q8I9c] Notifying assignor about the new Assignment(partitions=[test-topic-0000046-P0FMkh4-0])
13:47:26.195 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-y_Q8I9c-16, groupId=sub-046-y_Q8I9c] Adding newly assigned partitions: test-topic-0000046-P0FMkh4-0
13:47:26.196 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-y_Q8I9c-16, groupId=sub-046-y_Q8I9c] Found no committed offset for partition test-topic-0000046-P0FMkh4-0
13:47:26.196 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-tL_8fMg-17, groupId=sub-049-tL_8fMg] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-049-tL_8fMg-17-e365320a-1af4-406c-8052-0b043620df87', protocol='range'}
13:47:26.196 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-tL_8fMg-17, groupId=sub-049-tL_8fMg] Finished assignment for group at generation 1: {consumer-sub-049-tL_8fMg-17-e365320a-1af4-406c-8052-0b043620df87=Assignment(partitions=[test-topic-0000049-9ea7a3Q-0])}
13:47:26.197 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.197 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.197 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.197 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046197
13:47:26.197 [pool-21-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-046-y_Q8I9c-16, groupId=sub-046-y_Q8I9c] Resetting offset for partition test-topic-0000046-P0FMkh4-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.198 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-284
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.198 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-tL_8fMg-17, groupId=sub-049-tL_8fMg] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-049-tL_8fMg-17-e365320a-1af4-406c-8052-0b043620df87', protocol='range'}
13:47:26.198 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-tL_8fMg-17, groupId=sub-049-tL_8fMg] Notifying assignor about the new Assignment(partitions=[test-topic-0000049-9ea7a3Q-0])
13:47:26.198 [kafka-producer-network-thread | producer-283] INFO Metadata - [Producer clientId=producer-283] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.198 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-tL_8fMg-17, groupId=sub-049-tL_8fMg] Adding newly assigned partitions: test-topic-0000049-9ea7a3Q-0
13:47:26.198 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-284] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.198 [kafka-producer-network-thread | producer-283] INFO TransactionManager - [Producer clientId=producer-283] ProducerId set to 12483 with epoch 0
13:47:26.198 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-284] Instantiated an idempotent producer.
13:47:26.199 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-tL_8fMg-17, groupId=sub-049-tL_8fMg] Found no committed offset for partition test-topic-0000049-9ea7a3Q-0
13:47:26.200 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.200 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.200 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.200 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046200
13:47:26.201 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-285
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.201 [pool-22-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-049-tL_8fMg-17, groupId=sub-049-tL_8fMg] Resetting offset for partition test-topic-0000049-9ea7a3Q-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.201 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-285] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.201 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-285] Instantiated an idempotent producer.
13:47:26.202 [kafka-producer-network-thread | producer-284] INFO Metadata - [Producer clientId=producer-284] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.202 [kafka-producer-network-thread | producer-284] INFO TransactionManager - [Producer clientId=producer-284] ProducerId set to 12484 with epoch 0
13:47:26.203 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-__A3iWA-18, groupId=sub-052-__A3iWA] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-052-__A3iWA-18-023a3ae4-84f7-4464-b66f-54b25253c60d', protocol='range'}
13:47:26.203 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-__A3iWA-18, groupId=sub-052-__A3iWA] Finished assignment for group at generation 1: {consumer-sub-052-__A3iWA-18-023a3ae4-84f7-4464-b66f-54b25253c60d=Assignment(partitions=[test-topic-0000052-AfpVojk-0])}
13:47:26.205 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-__A3iWA-18, groupId=sub-052-__A3iWA] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-052-__A3iWA-18-023a3ae4-84f7-4464-b66f-54b25253c60d', protocol='range'}
13:47:26.205 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-__A3iWA-18, groupId=sub-052-__A3iWA] Notifying assignor about the new Assignment(partitions=[test-topic-0000052-AfpVojk-0])
13:47:26.205 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-__A3iWA-18, groupId=sub-052-__A3iWA] Adding newly assigned partitions: test-topic-0000052-AfpVojk-0
13:47:26.206 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-__A3iWA-18, groupId=sub-052-__A3iWA] Found no committed offset for partition test-topic-0000052-AfpVojk-0
13:47:26.207 [pool-23-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-052-__A3iWA-18, groupId=sub-052-__A3iWA] Resetting offset for partition test-topic-0000052-AfpVojk-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.210 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.210 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.210 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.210 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046210
13:47:26.210 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-286
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.211 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-286] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.211 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-286] Instantiated an idempotent producer.
13:47:26.211 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-055-yjFnWF4-19, groupId=sub-055-yjFnWF4] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-055-yjFnWF4-19-824ba906-339a-4328-8e5e-b3f9c2ddcf0b', protocol='range'}
13:47:26.211 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-055-yjFnWF4-19, groupId=sub-055-yjFnWF4] Finished assignment for group at generation 1: {consumer-sub-055-yjFnWF4-19-824ba906-339a-4328-8e5e-b3f9c2ddcf0b=Assignment(partitions=[test-topic-0000055-C3trC3o-0])}
13:47:26.211 [kafka-producer-network-thread | producer-285] INFO Metadata - [Producer clientId=producer-285] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.212 [kafka-producer-network-thread | producer-285] INFO TransactionManager - [Producer clientId=producer-285] ProducerId set to 14408 with epoch 0
13:47:26.212 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.212 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.212 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.212 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046212
13:47:26.213 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-055-yjFnWF4-19, groupId=sub-055-yjFnWF4] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-055-yjFnWF4-19-824ba906-339a-4328-8e5e-b3f9c2ddcf0b', protocol='range'}
13:47:26.213 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-287
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.213 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-055-yjFnWF4-19, groupId=sub-055-yjFnWF4] Notifying assignor about the new Assignment(partitions=[test-topic-0000055-C3trC3o-0])
13:47:26.213 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-055-yjFnWF4-19, groupId=sub-055-yjFnWF4] Adding newly assigned partitions: test-topic-0000055-C3trC3o-0
13:47:26.213 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-287] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.213 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-287] Instantiated an idempotent producer.
13:47:26.214 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-055-yjFnWF4-19, groupId=sub-055-yjFnWF4] Found no committed offset for partition test-topic-0000055-C3trC3o-0
13:47:26.214 [kafka-producer-network-thread | producer-286] INFO Metadata - [Producer clientId=producer-286] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.214 [kafka-producer-network-thread | producer-286] INFO TransactionManager - [Producer clientId=producer-286] ProducerId set to 13489 with epoch 0
13:47:26.215 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.215 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.215 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.215 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046215
13:47:26.215 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-288
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.215 [pool-24-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-055-yjFnWF4-19, groupId=sub-055-yjFnWF4] Resetting offset for partition test-topic-0000055-C3trC3o-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.216 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-288] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.216 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-288] Instantiated an idempotent producer.
13:47:26.217 [kafka-producer-network-thread | producer-287] INFO Metadata - [Producer clientId=producer-287] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.217 [kafka-producer-network-thread | producer-287] INFO TransactionManager - [Producer clientId=producer-287] ProducerId set to 13490 with epoch 0
13:47:26.218 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.218 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.218 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.218 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046218
13:47:26.218 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-061-krTjLto-21, groupId=sub-061-krTjLto] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-061-krTjLto-21-0d680477-53cd-4d29-93d5-fd962ebd8fee', protocol='range'}
13:47:26.218 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-289
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.218 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-061-krTjLto-21, groupId=sub-061-krTjLto] Finished assignment for group at generation 1: {consumer-sub-061-krTjLto-21-0d680477-53cd-4d29-93d5-fd962ebd8fee=Assignment(partitions=[test-topic-0000061-btSl6bw-0])}
13:47:26.218 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-289] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.219 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-289] Instantiated an idempotent producer.
13:47:26.219 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-058-2rRMNfE-20, groupId=sub-058-2rRMNfE] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-058-2rRMNfE-20-bb065645-f6e7-450e-945e-2f362ae23030', protocol='range'}
13:47:26.219 [kafka-producer-network-thread | producer-288] INFO Metadata - [Producer clientId=producer-288] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.219 [kafka-producer-network-thread | producer-288] INFO TransactionManager - [Producer clientId=producer-288] ProducerId set to 13493 with epoch 0
13:47:26.219 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-058-2rRMNfE-20, groupId=sub-058-2rRMNfE] Finished assignment for group at generation 1: {consumer-sub-058-2rRMNfE-20-bb065645-f6e7-450e-945e-2f362ae23030=Assignment(partitions=[test-topic-0000058-ru5jUZw-0])}
13:47:26.220 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-061-krTjLto-21, groupId=sub-061-krTjLto] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-061-krTjLto-21-0d680477-53cd-4d29-93d5-fd962ebd8fee', protocol='range'}
13:47:26.220 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-061-krTjLto-21, groupId=sub-061-krTjLto] Notifying assignor about the new Assignment(partitions=[test-topic-0000061-btSl6bw-0])
13:47:26.220 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-061-krTjLto-21, groupId=sub-061-krTjLto] Adding newly assigned partitions: test-topic-0000061-btSl6bw-0
13:47:26.220 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.220 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.221 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.221 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046220
13:47:26.221 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-058-2rRMNfE-20, groupId=sub-058-2rRMNfE] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-058-2rRMNfE-20-bb065645-f6e7-450e-945e-2f362ae23030', protocol='range'}
13:47:26.221 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-058-2rRMNfE-20, groupId=sub-058-2rRMNfE] Notifying assignor about the new Assignment(partitions=[test-topic-0000058-ru5jUZw-0])
13:47:26.221 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-061-krTjLto-21, groupId=sub-061-krTjLto] Found no committed offset for partition test-topic-0000061-btSl6bw-0
13:47:26.221 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-058-2rRMNfE-20, groupId=sub-058-2rRMNfE] Adding newly assigned partitions: test-topic-0000058-ru5jUZw-0
13:47:26.221 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-290
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.221 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-058-2rRMNfE-20, groupId=sub-058-2rRMNfE] Found no committed offset for partition test-topic-0000058-ru5jUZw-0
13:47:26.222 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-290] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.222 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-290] Instantiated an idempotent producer.
13:47:26.222 [kafka-producer-network-thread | producer-289] INFO Metadata - [Producer clientId=producer-289] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.222 [kafka-producer-network-thread | producer-289] INFO TransactionManager - [Producer clientId=producer-289] ProducerId set to 12491 with epoch 0
13:47:26.222 [pool-26-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-061-krTjLto-21, groupId=sub-061-krTjLto] Resetting offset for partition test-topic-0000061-btSl6bw-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.223 [pool-25-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-058-2rRMNfE-20, groupId=sub-058-2rRMNfE] Resetting offset for partition test-topic-0000058-ru5jUZw-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.224 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.224 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.224 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.224 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046224
13:47:26.224 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-291
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.225 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-291] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.225 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-291] Instantiated an idempotent producer.
13:47:26.225 [kafka-producer-network-thread | producer-290] INFO Metadata - [Producer clientId=producer-290] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.225 [kafka-producer-network-thread | producer-290] INFO TransactionManager - [Producer clientId=producer-290] ProducerId set to 13495 with epoch 0
13:47:26.227 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.227 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.227 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.227 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046227
13:47:26.227 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-292
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.228 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-292] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.228 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-292] Instantiated an idempotent producer.
13:47:26.228 [kafka-producer-network-thread | producer-291] INFO Metadata - [Producer clientId=producer-291] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.228 [kafka-producer-network-thread | producer-291] INFO TransactionManager - [Producer clientId=producer-291] ProducerId set to 12494 with epoch 0
13:47:26.230 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.230 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.230 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.230 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046230
13:47:26.230 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-293
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.231 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-293] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.231 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-293] Instantiated an idempotent producer.
13:47:26.231 [kafka-producer-network-thread | producer-292] INFO Metadata - [Producer clientId=producer-292] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.231 [kafka-producer-network-thread | producer-292] INFO TransactionManager - [Producer clientId=producer-292] ProducerId set to 13498 with epoch 0
13:47:26.231 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-064-DKhien4-22, groupId=sub-064-DKhien4] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-064-DKhien4-22-d9f9d40c-393f-49f6-b1a1-93b15d1ace31', protocol='range'}
13:47:26.231 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-064-DKhien4-22, groupId=sub-064-DKhien4] Finished assignment for group at generation 1: {consumer-sub-064-DKhien4-22-d9f9d40c-393f-49f6-b1a1-93b15d1ace31=Assignment(partitions=[test-topic-0000064-JLHPfAM-0])}
13:47:26.232 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.233 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.233 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.233 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046233
13:47:26.233 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-064-DKhien4-22, groupId=sub-064-DKhien4] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-064-DKhien4-22-d9f9d40c-393f-49f6-b1a1-93b15d1ace31', protocol='range'}
13:47:26.233 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-294
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.233 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-064-DKhien4-22, groupId=sub-064-DKhien4] Notifying assignor about the new Assignment(partitions=[test-topic-0000064-JLHPfAM-0])
13:47:26.233 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-064-DKhien4-22, groupId=sub-064-DKhien4] Adding newly assigned partitions: test-topic-0000064-JLHPfAM-0
13:47:26.233 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-294] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.234 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-294] Instantiated an idempotent producer.
13:47:26.234 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-064-DKhien4-22, groupId=sub-064-DKhien4] Found no committed offset for partition test-topic-0000064-JLHPfAM-0
13:47:26.234 [kafka-producer-network-thread | producer-293] INFO Metadata - [Producer clientId=producer-293] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.234 [kafka-producer-network-thread | producer-293] INFO TransactionManager - [Producer clientId=producer-293] ProducerId set to 12497 with epoch 0
13:47:26.235 [pool-27-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-064-DKhien4-22, groupId=sub-064-DKhien4] Resetting offset for partition test-topic-0000064-JLHPfAM-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.235 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.236 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.236 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.236 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046236
13:47:26.236 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-295
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.237 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-295] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.237 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-295] Instantiated an idempotent producer.
13:47:26.237 [kafka-producer-network-thread | producer-294] INFO Metadata - [Producer clientId=producer-294] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.237 [kafka-producer-network-thread | producer-294] INFO TransactionManager - [Producer clientId=producer-294] ProducerId set to 12500 with epoch 0
13:47:26.239 [pool-28-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-067-lT7d7GQ-23, groupId=sub-067-lT7d7GQ] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-067-lT7d7GQ-23-93ecb203-40f4-4810-9080-5863470e37c6', protocol='range'}
13:47:26.240 [pool-28-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-067-lT7d7GQ-23, groupId=sub-067-lT7d7GQ] Finished assignment for group at generation 1: {consumer-sub-067-lT7d7GQ-23-93ecb203-40f4-4810-9080-5863470e37c6=Assignment(partitions=[test-topic-0000067-6rhS1nY-0])}
13:47:26.241 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.241 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.241 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.241 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046241
13:47:26.241 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-296
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.241 [pool-28-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-067-lT7d7GQ-23, groupId=sub-067-lT7d7GQ] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-067-lT7d7GQ-23-93ecb203-40f4-4810-9080-5863470e37c6', protocol='range'}
13:47:26.241 [pool-28-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-067-lT7d7GQ-23, groupId=sub-067-lT7d7GQ] Notifying assignor about the new Assignment(partitions=[test-topic-0000067-6rhS1nY-0])
13:47:26.242 [pool-28-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-067-lT7d7GQ-23, groupId=sub-067-lT7d7GQ] Adding newly assigned partitions: test-topic-0000067-6rhS1nY-0
13:47:26.242 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-296] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.242 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-296] Instantiated an idempotent producer.
13:47:26.242 [pool-28-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-067-lT7d7GQ-23, groupId=sub-067-lT7d7GQ] Found no committed offset for partition test-topic-0000067-6rhS1nY-0
13:47:26.242 [kafka-producer-network-thread | producer-295] INFO Metadata - [Producer clientId=producer-295] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.243 [kafka-producer-network-thread | producer-295] INFO TransactionManager - [Producer clientId=producer-295] ProducerId set to 12502 with epoch 0
13:47:26.244 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.244 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.244 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.244 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046244
13:47:26.244 [pool-28-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-067-lT7d7GQ-23, groupId=sub-067-lT7d7GQ] Resetting offset for partition test-topic-0000067-6rhS1nY-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.244 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-297
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.245 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-297] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.245 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-297] Instantiated an idempotent producer.
13:47:26.245 [kafka-producer-network-thread | producer-296] INFO Metadata - [Producer clientId=producer-296] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.245 [kafka-producer-network-thread | producer-296] INFO TransactionManager - [Producer clientId=producer-296] ProducerId set to 14416 with epoch 0
13:47:26.246 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.246 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.246 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.246 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046246
13:47:26.246 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-298
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.247 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-298] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.247 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-298] Instantiated an idempotent producer.
13:47:26.248 [kafka-producer-network-thread | producer-297] INFO Metadata - [Producer clientId=producer-297] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.248 [kafka-producer-network-thread | producer-297] INFO TransactionManager - [Producer clientId=producer-297] ProducerId set to 12503 with epoch 0
13:47:26.249 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.249 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.249 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.249 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046249
13:47:26.249 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-299
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.250 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-299] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.250 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-299] Instantiated an idempotent producer.
13:47:26.250 [pool-29-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-070-MXbUqc8-24, groupId=sub-070-MXbUqc8] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-070-MXbUqc8-24-8c2e0162-ed88-487b-b7b5-3a775056d576', protocol='range'}
13:47:26.250 [pool-29-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-070-MXbUqc8-24, groupId=sub-070-MXbUqc8] Finished assignment for group at generation 1: {consumer-sub-070-MXbUqc8-24-8c2e0162-ed88-487b-b7b5-3a775056d576=Assignment(partitions=[test-topic-0000070-toSKqSk-0])}
13:47:26.250 [kafka-producer-network-thread | producer-298] INFO Metadata - [Producer clientId=producer-298] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.251 [kafka-producer-network-thread | producer-298] INFO TransactionManager - [Producer clientId=producer-298] ProducerId set to 13501 with epoch 0
13:47:26.252 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.252 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.252 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.252 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046252
13:47:26.252 [pool-29-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-070-MXbUqc8-24, groupId=sub-070-MXbUqc8] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-070-MXbUqc8-24-8c2e0162-ed88-487b-b7b5-3a775056d576', protocol='range'}
13:47:26.252 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-300
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.252 [pool-29-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-070-MXbUqc8-24, groupId=sub-070-MXbUqc8] Notifying assignor about the new Assignment(partitions=[test-topic-0000070-toSKqSk-0])
13:47:26.252 [pool-29-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-070-MXbUqc8-24, groupId=sub-070-MXbUqc8] Adding newly assigned partitions: test-topic-0000070-toSKqSk-0
13:47:26.253 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-300] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.253 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-300] Instantiated an idempotent producer.
13:47:26.253 [kafka-producer-network-thread | producer-299] INFO Metadata - [Producer clientId=producer-299] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.253 [pool-29-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-070-MXbUqc8-24, groupId=sub-070-MXbUqc8] Found no committed offset for partition test-topic-0000070-toSKqSk-0
13:47:26.253 [kafka-producer-network-thread | producer-299] INFO TransactionManager - [Producer clientId=producer-299] ProducerId set to 13502 with epoch 0
13:47:26.255 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.255 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.255 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.255 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046255
13:47:26.255 [pool-29-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-070-MXbUqc8-24, groupId=sub-070-MXbUqc8] Resetting offset for partition test-topic-0000070-toSKqSk-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.255 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-301
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.256 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-301] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.256 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-301] Instantiated an idempotent producer.
13:47:26.257 [kafka-producer-network-thread | producer-300] INFO Metadata - [Producer clientId=producer-300] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.257 [kafka-producer-network-thread | producer-300] INFO TransactionManager - [Producer clientId=producer-300] ProducerId set to 12504 with epoch 0
13:47:26.258 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.258 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.258 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.258 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046258
13:47:26.258 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-302
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.259 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-302] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.259 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-302] Instantiated an idempotent producer.
13:47:26.259 [pool-30-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-073-_TxsaHQ-25, groupId=sub-073-_TxsaHQ] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-073-_TxsaHQ-25-0a2b51ad-32bb-47dd-895a-a6fe22a579ae', protocol='range'}
13:47:26.259 [pool-30-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-073-_TxsaHQ-25, groupId=sub-073-_TxsaHQ] Finished assignment for group at generation 1: {consumer-sub-073-_TxsaHQ-25-0a2b51ad-32bb-47dd-895a-a6fe22a579ae=Assignment(partitions=[test-topic-0000073-HZ6O7Og-0])}
13:47:26.260 [kafka-producer-network-thread | producer-301] INFO Metadata - [Producer clientId=producer-301] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.260 [kafka-producer-network-thread | producer-301] INFO TransactionManager - [Producer clientId=producer-301] ProducerId set to 12505 with epoch 0
13:47:26.261 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.261 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.261 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.261 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046261
13:47:26.261 [pool-30-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-073-_TxsaHQ-25, groupId=sub-073-_TxsaHQ] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-073-_TxsaHQ-25-0a2b51ad-32bb-47dd-895a-a6fe22a579ae', protocol='range'}
13:47:26.261 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-303
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.261 [pool-30-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-073-_TxsaHQ-25, groupId=sub-073-_TxsaHQ] Notifying assignor about the new Assignment(partitions=[test-topic-0000073-HZ6O7Og-0])
13:47:26.262 [pool-30-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-073-_TxsaHQ-25, groupId=sub-073-_TxsaHQ] Adding newly assigned partitions: test-topic-0000073-HZ6O7Og-0
13:47:26.262 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-303] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.262 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-303] Instantiated an idempotent producer.
13:47:26.262 [pool-30-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-073-_TxsaHQ-25, groupId=sub-073-_TxsaHQ] Found no committed offset for partition test-topic-0000073-HZ6O7Og-0
13:47:26.262 [pool-31-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-076-h05AUYU-26, groupId=sub-076-h05AUYU] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-076-h05AUYU-26-cceb5020-b8ee-4fdc-b543-5137506aa175', protocol='range'}
13:47:26.262 [kafka-producer-network-thread | producer-302] INFO Metadata - [Producer clientId=producer-302] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.262 [pool-31-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-076-h05AUYU-26, groupId=sub-076-h05AUYU] Finished assignment for group at generation 1: {consumer-sub-076-h05AUYU-26-cceb5020-b8ee-4fdc-b543-5137506aa175=Assignment(partitions=[test-topic-0000076-9xcr8KU-0])}
13:47:26.263 [kafka-producer-network-thread | producer-302] INFO TransactionManager - [Producer clientId=producer-302] ProducerId set to 12506 with epoch 0
13:47:26.264 [pool-30-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-073-_TxsaHQ-25, groupId=sub-073-_TxsaHQ] Resetting offset for partition test-topic-0000073-HZ6O7Og-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.264 [pool-31-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-076-h05AUYU-26, groupId=sub-076-h05AUYU] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-076-h05AUYU-26-cceb5020-b8ee-4fdc-b543-5137506aa175', protocol='range'}
13:47:26.264 [pool-31-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-076-h05AUYU-26, groupId=sub-076-h05AUYU] Notifying assignor about the new Assignment(partitions=[test-topic-0000076-9xcr8KU-0])
13:47:26.264 [pool-31-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-076-h05AUYU-26, groupId=sub-076-h05AUYU] Adding newly assigned partitions: test-topic-0000076-9xcr8KU-0
13:47:26.265 [pool-31-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-076-h05AUYU-26, groupId=sub-076-h05AUYU] Found no committed offset for partition test-topic-0000076-9xcr8KU-0
13:47:26.266 [pool-31-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-076-h05AUYU-26, groupId=sub-076-h05AUYU] Resetting offset for partition test-topic-0000076-9xcr8KU-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.277 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.277 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.277 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.277 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046277
13:47:26.277 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-304
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.278 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-304] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.278 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-304] Instantiated an idempotent producer.
13:47:26.278 [kafka-producer-network-thread | producer-303] INFO Metadata - [Producer clientId=producer-303] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.278 [kafka-producer-network-thread | producer-303] INFO TransactionManager - [Producer clientId=producer-303] ProducerId set to 13503 with epoch 0
13:47:26.278 [pool-32-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-079-bjUU63o-27, groupId=sub-079-bjUU63o] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-079-bjUU63o-27-f227535e-77b2-423e-a588-01305190c02c', protocol='range'}
13:47:26.279 [pool-32-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-079-bjUU63o-27, groupId=sub-079-bjUU63o] Finished assignment for group at generation 1: {consumer-sub-079-bjUU63o-27-f227535e-77b2-423e-a588-01305190c02c=Assignment(partitions=[test-topic-0000079-4XRk71U-0])}
13:47:26.280 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.280 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.280 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.280 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046280
13:47:26.280 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-305
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.280 [pool-32-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-079-bjUU63o-27, groupId=sub-079-bjUU63o] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-079-bjUU63o-27-f227535e-77b2-423e-a588-01305190c02c', protocol='range'}
13:47:26.281 [pool-32-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-079-bjUU63o-27, groupId=sub-079-bjUU63o] Notifying assignor about the new Assignment(partitions=[test-topic-0000079-4XRk71U-0])
13:47:26.281 [pool-32-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-079-bjUU63o-27, groupId=sub-079-bjUU63o] Adding newly assigned partitions: test-topic-0000079-4XRk71U-0
13:47:26.281 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-305] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.281 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-305] Instantiated an idempotent producer.
13:47:26.281 [pool-32-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-079-bjUU63o-27, groupId=sub-079-bjUU63o] Found no committed offset for partition test-topic-0000079-4XRk71U-0
13:47:26.281 [pool-33-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-082-7tZMVGo-28, groupId=sub-082-7tZMVGo] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-082-7tZMVGo-28-5272bd4b-f9b5-4e98-9790-db7d4994ba62', protocol='range'}
13:47:26.282 [pool-33-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-082-7tZMVGo-28, groupId=sub-082-7tZMVGo] Finished assignment for group at generation 1: {consumer-sub-082-7tZMVGo-28-5272bd4b-f9b5-4e98-9790-db7d4994ba62=Assignment(partitions=[test-topic-0000082-x9QjlwQ-0])}
13:47:26.283 [pool-32-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-079-bjUU63o-27, groupId=sub-079-bjUU63o] Resetting offset for partition test-topic-0000079-4XRk71U-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.283 [pool-33-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-082-7tZMVGo-28, groupId=sub-082-7tZMVGo] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-082-7tZMVGo-28-5272bd4b-f9b5-4e98-9790-db7d4994ba62', protocol='range'}
13:47:26.284 [pool-33-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-082-7tZMVGo-28, groupId=sub-082-7tZMVGo] Notifying assignor about the new Assignment(partitions=[test-topic-0000082-x9QjlwQ-0])
13:47:26.284 [pool-33-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-082-7tZMVGo-28, groupId=sub-082-7tZMVGo] Adding newly assigned partitions: test-topic-0000082-x9QjlwQ-0
13:47:26.284 [pool-33-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-082-7tZMVGo-28, groupId=sub-082-7tZMVGo] Found no committed offset for partition test-topic-0000082-x9QjlwQ-0
13:47:26.286 [kafka-producer-network-thread | producer-304] INFO Metadata - [Producer clientId=producer-304] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.286 [kafka-producer-network-thread | producer-304] INFO TransactionManager - [Producer clientId=producer-304] ProducerId set to 14417 with epoch 0
13:47:26.286 [pool-33-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-082-7tZMVGo-28, groupId=sub-082-7tZMVGo] Resetting offset for partition test-topic-0000082-x9QjlwQ-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.287 [pool-34-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-085-JIGyKs0-29, groupId=sub-085-JIGyKs0] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-085-JIGyKs0-29-7ba0f02c-c67c-4106-bb04-8bb77f11904a', protocol='range'}
13:47:26.288 [pool-34-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-085-JIGyKs0-29, groupId=sub-085-JIGyKs0] Finished assignment for group at generation 1: {consumer-sub-085-JIGyKs0-29-7ba0f02c-c67c-4106-bb04-8bb77f11904a=Assignment(partitions=[test-topic-0000085-_VhFHj0-0])}
13:47:26.289 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.289 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.289 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.289 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046289
13:47:26.289 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-306
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.289 [pool-34-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-085-JIGyKs0-29, groupId=sub-085-JIGyKs0] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-085-JIGyKs0-29-7ba0f02c-c67c-4106-bb04-8bb77f11904a', protocol='range'}
13:47:26.289 [pool-34-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-085-JIGyKs0-29, groupId=sub-085-JIGyKs0] Notifying assignor about the new Assignment(partitions=[test-topic-0000085-_VhFHj0-0])
13:47:26.290 [pool-34-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-085-JIGyKs0-29, groupId=sub-085-JIGyKs0] Adding newly assigned partitions: test-topic-0000085-_VhFHj0-0
13:47:26.290 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-306] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.290 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-306] Instantiated an idempotent producer.
13:47:26.290 [pool-34-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-085-JIGyKs0-29, groupId=sub-085-JIGyKs0] Found no committed offset for partition test-topic-0000085-_VhFHj0-0
13:47:26.291 [kafka-producer-network-thread | producer-305] INFO Metadata - [Producer clientId=producer-305] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.291 [kafka-producer-network-thread | producer-305] INFO TransactionManager - [Producer clientId=producer-305] ProducerId set to 14418 with epoch 0
13:47:26.292 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.292 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.292 [pool-34-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-085-JIGyKs0-29, groupId=sub-085-JIGyKs0] Resetting offset for partition test-topic-0000085-_VhFHj0-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.293 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.293 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046292
13:47:26.293 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-307
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.294 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-307] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.294 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-307] Instantiated an idempotent producer.
13:47:26.294 [kafka-producer-network-thread | producer-306] INFO Metadata - [Producer clientId=producer-306] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.295 [kafka-producer-network-thread | producer-306] INFO TransactionManager - [Producer clientId=producer-306] ProducerId set to 12507 with epoch 0
13:47:26.297 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.297 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.297 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.297 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046297
13:47:26.297 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-308
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.298 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-308] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.298 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-308] Instantiated an idempotent producer.
13:47:26.299 [kafka-producer-network-thread | producer-307] INFO Metadata - [Producer clientId=producer-307] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.299 [kafka-producer-network-thread | producer-307] INFO TransactionManager - [Producer clientId=producer-307] ProducerId set to 12508 with epoch 0
13:47:26.299 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.300 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.300 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.300 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046300
13:47:26.300 [qtp435803541-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.216:9092, 10.0.0.52:9092, 10.0.0.229:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-309
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:47:26.300 [pool-35-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-088-Jm7oJ_0-30, groupId=sub-088-Jm7oJ_0] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-088-Jm7oJ_0-30-b6bdc197-f24c-4e21-9622-cccee8a49c30', protocol='range'}
13:47:26.300 [qtp435803541-29] WARN KafkaProducer - [Producer clientId=producer-309] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
13:47:26.301 [qtp435803541-29] INFO KafkaProducer - [Producer clientId=producer-309] Instantiated an idempotent producer.
13:47:26.301 [pool-35-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-088-Jm7oJ_0-30, groupId=sub-088-Jm7oJ_0] Finished assignment for group at generation 1: {consumer-sub-088-Jm7oJ_0-30-b6bdc197-f24c-4e21-9622-cccee8a49c30=Assignment(partitions=[test-topic-0000088-m_LR2ic-0])}
13:47:26.302 [kafka-producer-network-thread | producer-308] INFO Metadata - [Producer clientId=producer-308] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.302 [kafka-producer-network-thread | producer-308] INFO TransactionManager - [Producer clientId=producer-308] ProducerId set to 13504 with epoch 0
13:47:26.302 [pool-35-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-088-Jm7oJ_0-30, groupId=sub-088-Jm7oJ_0] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-088-Jm7oJ_0-30-b6bdc197-f24c-4e21-9622-cccee8a49c30', protocol='range'}
13:47:26.302 [qtp435803541-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
13:47:26.303 [qtp435803541-29] INFO AppInfoParser - Kafka version: 3.6.1
13:47:26.303 [qtp435803541-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
13:47:26.303 [qtp435803541-29] INFO AppInfoParser - Kafka startTimeMs: 1717336046303
13:47:26.303 [pool-35-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-088-Jm7oJ_0-30, groupId=sub-088-Jm7oJ_0] Notifying assignor about the new Assignment(partitions=[test-topic-0000088-m_LR2ic-0])
13:47:26.303 [pool-36-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-091-Tt1W9dc-31, groupId=sub-091-Tt1W9dc] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-091-Tt1W9dc-31-6623427b-7a19-462e-8027-78db3bf84408', protocol='range'}
13:47:26.303 [pool-35-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-088-Jm7oJ_0-30, groupId=sub-088-Jm7oJ_0] Adding newly assigned partitions: test-topic-0000088-m_LR2ic-0
13:47:26.303 [pool-36-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-091-Tt1W9dc-31, groupId=sub-091-Tt1W9dc] Finished assignment for group at generation 1: {consumer-sub-091-Tt1W9dc-31-6623427b-7a19-462e-8027-78db3bf84408=Assignment(partitions=[test-topic-0000091-DPFMLNo-0])}
13:47:26.303 [pool-35-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-088-Jm7oJ_0-30, groupId=sub-088-Jm7oJ_0] Found no committed offset for partition test-topic-0000088-m_LR2ic-0
13:47:26.304 [kafka-producer-network-thread | producer-309] INFO Metadata - [Producer clientId=producer-309] Cluster ID: npFGiqmcThOkfBOsM5z8_g
13:47:26.304 [kafka-producer-network-thread | producer-309] INFO TransactionManager - [Producer clientId=producer-309] ProducerId set to 14419 with epoch 0
13:47:26.305 [pool-36-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-091-Tt1W9dc-31, groupId=sub-091-Tt1W9dc] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-091-Tt1W9dc-31-6623427b-7a19-462e-8027-78db3bf84408', protocol='range'}
13:47:26.305 [pool-36-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-091-Tt1W9dc-31, groupId=sub-091-Tt1W9dc] Notifying assignor about the new Assignment(partitions=[test-topic-0000091-DPFMLNo-0])
13:47:26.305 [pool-36-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-091-Tt1W9dc-31, groupId=sub-091-Tt1W9dc] Adding newly assigned partitions: test-topic-0000091-DPFMLNo-0
13:47:26.305 [pool-35-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-088-Jm7oJ_0-30, groupId=sub-088-Jm7oJ_0] Resetting offset for partition test-topic-0000088-m_LR2ic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.305 [pool-36-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-091-Tt1W9dc-31, groupId=sub-091-Tt1W9dc] Found no committed offset for partition test-topic-0000091-DPFMLNo-0
13:47:26.307 [pool-36-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-091-Tt1W9dc-31, groupId=sub-091-Tt1W9dc] Resetting offset for partition test-topic-0000091-DPFMLNo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.313 [pool-37-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-094-RJlVLD4-32, groupId=sub-094-RJlVLD4] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-094-RJlVLD4-32-9a0c0626-4a1e-43e3-8170-e322d7b6262e', protocol='range'}
13:47:26.313 [pool-37-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-094-RJlVLD4-32, groupId=sub-094-RJlVLD4] Finished assignment for group at generation 1: {consumer-sub-094-RJlVLD4-32-9a0c0626-4a1e-43e3-8170-e322d7b6262e=Assignment(partitions=[test-topic-0000094-EbfdF94-0])}
13:47:26.314 [pool-38-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-097-qcwhv34-33, groupId=sub-097-qcwhv34] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-097-qcwhv34-33-df5dd98a-898e-4271-8e2e-7d49bab8c194', protocol='range'}
13:47:26.315 [pool-38-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-097-qcwhv34-33, groupId=sub-097-qcwhv34] Finished assignment for group at generation 1: {consumer-sub-097-qcwhv34-33-df5dd98a-898e-4271-8e2e-7d49bab8c194=Assignment(partitions=[test-topic-0000097-rYrnvdk-0])}
13:47:26.315 [pool-37-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-094-RJlVLD4-32, groupId=sub-094-RJlVLD4] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-094-RJlVLD4-32-9a0c0626-4a1e-43e3-8170-e322d7b6262e', protocol='range'}
13:47:26.315 [pool-37-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-094-RJlVLD4-32, groupId=sub-094-RJlVLD4] Notifying assignor about the new Assignment(partitions=[test-topic-0000094-EbfdF94-0])
13:47:26.315 [pool-37-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-094-RJlVLD4-32, groupId=sub-094-RJlVLD4] Adding newly assigned partitions: test-topic-0000094-EbfdF94-0
13:47:26.314 [qtp435803541-29] INFO LocalWorker - Created 309 producers in 1069.476481 ms from {
  "topics" : [ "test-topic-0000000-FGdYwL4", "test-topic-0000001-8XkVdYY", "test-topic-0000002-AnoRVV8", "test-topic-0000003-KFfayTM", "test-topic-0000004-20ZHReU", "test-topic-0000005-z0C2FwA", "test-topic-0000006-c-q-i_g", "test-topic-0000007-0MzD9Ik", "test-topic-0000008-0nz4Z_4", "test-topic-0000009-AqQ2Zlw", "test-topic-0000010-NyjSQ9A", "test-topic-0000011-zxqm4Mw", "test-topic-0000012-YCyXSbE", "test-topic-0000013-FjDWcwQ", "test-topic-0000014-CFLlGHs", "test-topic-0000015-URbXFF4", "test-topic-0000016-sZJ3Jho", "test-topic-0000017--qoxjPU", "test-topic-0000018-o_iMPj4", "test-topic-0000019-KoBZyHg", "test-topic-0000020-M_uOt0w", "test-topic-0000021-g1wDjUE", "test-topic-0000022-SNsKA5U", "test-topic-0000023-UFWJKKU", "test-topic-0000024-G8RFGsk", "test-topic-0000025-6uZz340", "test-topic-0000026-PbTraa0", "test-topic-0000027-Xa_RnUU", "test-topic-0000028-3nLVSQs", "test-topic-0000029-Sq99nJQ", "test-topic-0000030-SxXiKuo", "test-topic-0000031-DjUOkAw", "test-topic-0000032-uNikqf8", "test-topic-0000033-I9YklqI", "test-topic-0000034-P1Rw2WM", "test-topic-0000035-xhp6_oA", "test-topic-0000036-KEdU3IU", "test-topic-0000037-2r-FtRk", "test-topic-0000038-aoczOmE", "test-topic-0000039-DLNOGX4", "test-topic-0000040-ctRrJ_E", "test-topic-0000041-Ni38W-8", "test-topic-0000042-IpLh0uQ", "test-topic-0000043-CI9z7bU", "test-topic-0000044-HLDd740", "test-topic-0000045-5ePkOy4", "test-topic-0000046-P0FMkh4", "test-topic-0000047-Ci_iHCk", "test-topic-0000048-cYfObxU", "test-topic-0000049-9ea7a3Q", "test-topic-0000050-n3f7BfU", "test-topic-0000051-A0OfkLI", "test-topic-0000052-AfpVojk", "test-topic-0000053-Wz-ZTHE", "test-topic-0000054-omow84U", "test-topic-0000055-C3trC3o", "test-topic-0000056-Hx4phh4", "test-topic-0000057-b1ROO2s", "test-topic-0000058-ru5jUZw", "test-topic-0000059-2eeNhoM", "test-topic-0000060-YhVhRN4", "test-topic-0000061-btSl6bw", "test-topic-0000062-rNORlD0", "test-topic-0000063-TutZTB8", "test-topic-0000064-JLHPfAM", "test-topic-0000065-9yyIq1M", "test-topic-0000066-iVHj7R4", "test-topic-0000067-6rhS1nY", "test-topic-0000068-BieU9RA", "test-topic-0000069-j5MG-i8", "test-topic-0000070-toSKqSk", "test-topic-0000071-VWYOjwM", "test-topic-0000072-atXhL_g", "test-topic-0000073-HZ6O7Og", "test-topic-0000074-qfrgGP4", "test-topic-0000075-2Is6uhc", "test-topic-0000076-9xcr8KU", "test-topic-0000077-bmvIbiY", "test-topic-0000078-Wk4biHA", "test-topic-0000079-4XRk71U", "test-topic-0000080-GSwinHI", "test-topic-0000081-slvhhyc", "test-topic-0000082-x9QjlwQ", "test-topic-0000083-g-BmxjQ", "test-topic-0000084-fyVCbbE", "test-topic-0000085-_VhFHj0", "test-topic-0000086-cFlu7XU", "test-topic-0000087-81GfjoE", "test-topic-0000088-m_LR2ic", "test-topic-0000089-x68IQU4", "test-topic-0000090-2-rWqDI", "test-topic-0000091-DPFMLNo", "test-topic-0000092-JXGTSos", "test-topic-0000093-bXeEJlM", "test-topic-0000094-EbfdF94", "test-topic-0000095-zPq6gTM", "test-topic-0000096-mZKQCwo", "test-topic-0000097-rYrnvdk", "test-topic-0000098-H_jRR9A", "test-topic-0000099-1cEYHFU", "test-topic-0000100-tJC42sY", "test-topic-0000101-LhC-54M", "test-topic-0000102-l_C9r5E", "test-topic-0000103-7T4hNlo", "test-topic-0000104--zE_tVI", "test-topic-0000105-9pKxJ4U", "test-topic-0000106-DPglyWA", "test-topic-0000107-fjH0hDM", "test-topic-0000108-NB7wiDg", "test-topic-0000109-8qoD6Cs", "test-topic-0000110-mZfst7E", "test-topic-0000111-F9DCrSI", "test-topic-0000112-6kLoZnM", "test-topic-0000113-cTWFNVA", "test-topic-0000114-W7btOqk", "test-topic-0000115-6QgytUA", "test-topic-0000116-7UdnOfU", "test-topic-0000117-qR0MMec", "test-topic-0000118-8tUi_6Q", "test-topic-0000119-SlKUM_g", "test-topic-0000120-0BOroSI", "test-topic-0000121-MbWpUXY", "test-topic-0000122-BF9iF7A", "test-topic-0000123-NQtdm4o", "test-topic-0000124-amAHj1M", "test-topic-0000125-hZwj-ZY", "test-topic-0000126-uv0Lyw8", "test-topic-0000127-yWtvaYk", "test-topic-0000128-VV-rVFU", "test-topic-0000129-hjtxynQ", "test-topic-0000130-Qg40Jrk", "test-topic-0000131-xgUrqV0", "test-topic-0000132-cm7Hf-c", "test-topic-0000133-98k_qFM", "test-topic-0000134-Sxkgttc", "test-topic-0000135-E9VX0MI", "test-topic-0000136-Y9wEdEA", "test-topic-0000137-Aei-Pwg", "test-topic-0000138-jGAmpUk", "test-topic-0000139-OuBTmUg", "test-topic-0000140-EL2B4IY", "test-topic-0000141-sMY3in0", "test-topic-0000142-PXtC-SI", "test-topic-0000143-6mgkLNE", "test-topic-0000144-I87Cc1c", "test-topic-0000145-c-Awvi4", "test-topic-0000146-mwLxECY", "test-topic-0000147-daxMmRU", "test-topic-0000148-jGtN0UQ", "test-topic-0000149-oEEgv68", "test-topic-0000150-aU4kmlM", "test-topic-0000151-TzSMYyE", "test-topic-0000152-_ZPbr1E", "test-topic-0000153-7WDuOCQ", "test-topic-0000154-akFkJ1U", "test-topic-0000155-lbbml9M", "test-topic-0000156-bv1bM3c", "test-topic-0000157--IfLOsc", "test-topic-0000158-6CVrNxA", "test-topic-0000159-g4aXvWA", "test-topic-0000160-mjkwCiM", "test-topic-0000161-wcsZU2U", "test-topic-0000162-MJrR-xo", "test-topic-0000163-_ZTSuc8", "test-topic-0000164-gepakbQ", "test-topic-0000165-K5Vt5OE", "test-topic-0000166-FI2baz0", "test-topic-0000167-BwF1nXw", "test-topic-0000168-5AaQRwA", "test-topic-0000169-frnmqbA", "test-topic-0000170-Z0q4Nvo", "test-topic-0000171-TTgHNZY", "test-topic-0000172-zC5ZI88", "test-topic-0000173-6CJkJ1w", "test-topic-0000174-hJfH_wE", "test-topic-0000175-nKeyHaA", "test-topic-0000176-BBDA9F8", "test-topic-0000177-9whUEAI", "test-topic-0000178-yN-12bs", "test-topic-0000179-Z_sMFPQ", "test-topic-0000180-mtow198", "test-topic-0000181-ILkihtM", "test-topic-0000182-FHEBZ8A", "test-topic-0000183-Q_DaKzs", "test-topic-0000184-YsWG3PQ", "test-topic-0000185-PO-JbZY", "test-topic-0000186-TMi6IGE", "test-topic-0000187-a7MsovY", "test-topic-0000188-c6J0hjY", "test-topic-0000189-Br5CACQ", "test-topic-0000190-2u2ROx8", "test-topic-0000191-PNN1Bfk", "test-topic-0000192-qIO7V7Q", "test-topic-0000193-ngfcLus", "test-topic-0000194-K-KCWTE", "test-topic-0000195-ur-Cgeo", "test-topic-0000196-IG_hwmw", "test-topic-0000197-CIa3XGw", "test-topic-0000198-MRrztgM", "test-topic-0000199-CtttN8Y", "test-topic-0000200-viNaANQ", "test-topic-0000201-rmNvfr4", "test-topic-0000202-3wtSyrE", "test-topic-0000203-9hn2q10", "test-topic-0000204-qe1T4lQ", "test-topic-0000205-2mMmqQg", "test-topic-0000206-sMqOXZA", "test-topic-0000207-bDTZgNE", "test-topic-0000208-zYqakMk", "test-topic-0000209-PZuErwo", "test-topic-0000210-IbT0AKY", "test-topic-0000211-_2UT4Bs", "test-topic-0000212-WR9CMwQ", "test-topic-0000213-a7hif5U", "test-topic-0000214-MyxBVy0", "test-topic-0000215-Zb4mOoY", "test-topic-0000216-PzoM90k", "test-topic-0000217-XNxlkQ4", "test-topic-0000218-6mkEIxQ", "test-topic-0000219-9wHMOjI", "test-topic-0000220-uOm4gtE", "test-topic-0000221-7QIO4vA", "test-topic-0000222-0FOfLX4", "test-topic-0000223-NBqvIZI", "test-topic-0000224-vd-16Eg", "test-topic-0000225-Xfv4mUc", "test-topic-0000226-cmxwGEA", "test-topic-0000227-PhYZ_-4", "test-topic-0000228-j6bdXE8", "test-topic-0000229-0fBbwTU", "test-topic-0000230-4s4B6Wc", "test-topic-0000231-W7upruI", "test-topic-0000232-L-U4evs", "test-topic-0000233-0IL1GHA", "test-topic-0000234-MztKqbA", "test-topic-0000235-daWCzN0", "test-topic-0000236-5rPzsFQ", "test-topic-0000237-x236eMI", "test-topic-0000238-ATNTxQ8", "test-topic-0000239-m_eFS6M", "test-topic-0000240-vwD5iNc", "test-topic-0000241-hsO-5Ig", "test-topic-0000242-5iXW5yk", "test-topic-0000243-fpyKxcw", "test-topic-0000244-CATmwfU", "test-topic-0000245-wLMd2DQ", "test-topic-0000246-Bq6i99k", "test-topic-0000247-9BaoGEM", "test-topic-0000248-1cRH_ZI", "test-topic-0000249-j-ThMcs", "test-topic-0000250-hNzff68", "test-topic-0000251-eeL7Zsc", "test-topic-0000252-fa1UjIY", "test-topic-0000253-71qwhGg", "test-topic-0000254-CpZ6PWw", "test-topic-0000255-qWOKGG0", "test-topic-0000256-GzCULh8", "test-topic-0000257-P_M3hrc", "test-topic-0000258-Rv9SxIc", "test-topic-0000259-x2gdERw", "test-topic-0000260-DnsmC54", "test-topic-0000261-w2mnyns", "test-topic-0000262-Z_n6Lqs", "test-topic-0000263-OjpC5g8", "test-topic-0000264-eT2e8rU", "test-topic-0000265-JPOr-Ag", "test-topic-0000266-pK4XlDY", "test-topic-0000267-CjvJg1c", "test-topic-0000268-xHueAw8", "test-topic-0000269-7mR6KMk", "test-topic-0000270-tS64jLw", "test-topic-0000271-ZxlpV1c", "test-topic-0000272-GXSuIMg", "test-topic-0000273-EO6kLrs", "test-topic-0000274-WBszb4M", "test-topic-0000275-I49nzkY", "test-topic-0000276-6JL1QfE", "test-topic-0000277-YF-_KfA", "test-topic-0000278-zWRdAgQ", "test-topic-0000279-M7JpqrA", "test-topic-0000280-Y3fRkQ4", "test-topic-0000281-FQQUyA4", "test-topic-0000282-WxYsVXA", "test-topic-0000283-MdXNdPY", "test-topic-0000284-Agh4b5c", "test-topic-0000285-SwrrYM0", "test-topic-0000286-yTITFyk", "test-topic-0000287-3fcwIsk", "test-topic-0000288-AZcX53o", "test-topic-0000289-gnsmK30", "test-topic-0000290-5aCzRUQ", "test-topic-0000291-mMcByrM", "test-topic-0000292-3RYn65c", "test-topic-0000293-H8U0kX0", "test-topic-0000294-ASiqRA4", "test-topic-0000295-5y9DMI4", "test-topic-0000296-217YnkM", "test-topic-0000297-6tCb4MU", "test-topic-0000298-cXyRfIQ", "test-topic-0000299-9nrgwQo", "test-topic-0000300-gX_eTcY", "test-topic-0000301-j62eWV0", "test-topic-0000302-DlyJbtk", "test-topic-0000303-EgI1jiw" ],
  "producerIndex" : 0,
  "isTpcH" : true
}
13:47:26.316 [pool-37-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-094-RJlVLD4-32, groupId=sub-094-RJlVLD4] Found no committed offset for partition test-topic-0000094-EbfdF94-0
13:47:26.317 [main] INFO WorkloadGenerator - Created 304 producers in 1083.862581 ms
13:47:26.318 [main] INFO WorkloadGenerator - Waiting for consumers to be ready...
13:47:26.318 [pool-38-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-097-qcwhv34-33, groupId=sub-097-qcwhv34] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-097-qcwhv34-33-df5dd98a-898e-4271-8e2e-7d49bab8c194', protocol='range'}
13:47:26.319 [pool-38-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-097-qcwhv34-33, groupId=sub-097-qcwhv34] Notifying assignor about the new Assignment(partitions=[test-topic-0000097-rYrnvdk-0])
13:47:26.320 [pool-38-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-097-qcwhv34-33, groupId=sub-097-qcwhv34] Adding newly assigned partitions: test-topic-0000097-rYrnvdk-0
13:47:26.320 [pool-38-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-097-qcwhv34-33, groupId=sub-097-qcwhv34] Found no committed offset for partition test-topic-0000097-rYrnvdk-0
13:47:26.323 [pool-37-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-094-RJlVLD4-32, groupId=sub-094-RJlVLD4] Resetting offset for partition test-topic-0000094-EbfdF94-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.323 [pool-38-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-097-qcwhv34-33, groupId=sub-097-qcwhv34] Resetting offset for partition test-topic-0000097-rYrnvdk-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.323 [pool-39-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-100-dbigFbc-34, groupId=sub-100-dbigFbc] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-100-dbigFbc-34-e3316b80-68d7-4134-a409-69c0ca7e3e95', protocol='range'}
13:47:26.324 [pool-39-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-100-dbigFbc-34, groupId=sub-100-dbigFbc] Finished assignment for group at generation 1: {consumer-sub-100-dbigFbc-34-e3316b80-68d7-4134-a409-69c0ca7e3e95=Assignment(partitions=[test-topic-0000100-tJC42sY-0])}
13:47:26.325 [pool-40-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-103-YQZE89U-35, groupId=sub-103-YQZE89U] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-103-YQZE89U-35-fd1eb4e0-ce28-4ad3-ad4d-53a52ab4e128', protocol='range'}
13:47:26.325 [pool-40-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-103-YQZE89U-35, groupId=sub-103-YQZE89U] Finished assignment for group at generation 1: {consumer-sub-103-YQZE89U-35-fd1eb4e0-ce28-4ad3-ad4d-53a52ab4e128=Assignment(partitions=[test-topic-0000103-7T4hNlo-0])}
13:47:26.326 [pool-39-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-100-dbigFbc-34, groupId=sub-100-dbigFbc] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-100-dbigFbc-34-e3316b80-68d7-4134-a409-69c0ca7e3e95', protocol='range'}
13:47:26.326 [pool-39-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-100-dbigFbc-34, groupId=sub-100-dbigFbc] Notifying assignor about the new Assignment(partitions=[test-topic-0000100-tJC42sY-0])
13:47:26.326 [pool-39-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-100-dbigFbc-34, groupId=sub-100-dbigFbc] Adding newly assigned partitions: test-topic-0000100-tJC42sY-0
13:47:26.326 [pool-39-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-100-dbigFbc-34, groupId=sub-100-dbigFbc] Found no committed offset for partition test-topic-0000100-tJC42sY-0
13:47:26.326 [pool-40-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-103-YQZE89U-35, groupId=sub-103-YQZE89U] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-103-YQZE89U-35-fd1eb4e0-ce28-4ad3-ad4d-53a52ab4e128', protocol='range'}
13:47:26.327 [pool-40-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-103-YQZE89U-35, groupId=sub-103-YQZE89U] Notifying assignor about the new Assignment(partitions=[test-topic-0000103-7T4hNlo-0])
13:47:26.327 [pool-40-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-103-YQZE89U-35, groupId=sub-103-YQZE89U] Adding newly assigned partitions: test-topic-0000103-7T4hNlo-0
13:47:26.327 [pool-40-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-103-YQZE89U-35, groupId=sub-103-YQZE89U] Found no committed offset for partition test-topic-0000103-7T4hNlo-0
13:47:26.328 [pool-39-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-100-dbigFbc-34, groupId=sub-100-dbigFbc] Resetting offset for partition test-topic-0000100-tJC42sY-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.329 [pool-40-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-103-YQZE89U-35, groupId=sub-103-YQZE89U] Resetting offset for partition test-topic-0000103-7T4hNlo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.332 [pool-41-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-106-914gNsw-36, groupId=sub-106-914gNsw] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-106-914gNsw-36-d7c02a5c-53c6-487d-8336-b9171d8d2194', protocol='range'}
13:47:26.332 [pool-41-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-106-914gNsw-36, groupId=sub-106-914gNsw] Finished assignment for group at generation 1: {consumer-sub-106-914gNsw-36-d7c02a5c-53c6-487d-8336-b9171d8d2194=Assignment(partitions=[test-topic-0000106-DPglyWA-0])}
13:47:26.334 [pool-41-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-106-914gNsw-36, groupId=sub-106-914gNsw] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-106-914gNsw-36-d7c02a5c-53c6-487d-8336-b9171d8d2194', protocol='range'}
13:47:26.334 [pool-41-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-106-914gNsw-36, groupId=sub-106-914gNsw] Notifying assignor about the new Assignment(partitions=[test-topic-0000106-DPglyWA-0])
13:47:26.334 [pool-41-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-106-914gNsw-36, groupId=sub-106-914gNsw] Adding newly assigned partitions: test-topic-0000106-DPglyWA-0
13:47:26.335 [pool-41-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-106-914gNsw-36, groupId=sub-106-914gNsw] Found no committed offset for partition test-topic-0000106-DPglyWA-0
13:47:26.336 [pool-42-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-109-JW4VXwQ-37, groupId=sub-109-JW4VXwQ] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-109-JW4VXwQ-37-3c284ab9-79a4-42f5-8f75-cf3d3b23c29a', protocol='range'}
13:47:26.336 [pool-42-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-109-JW4VXwQ-37, groupId=sub-109-JW4VXwQ] Finished assignment for group at generation 1: {consumer-sub-109-JW4VXwQ-37-3c284ab9-79a4-42f5-8f75-cf3d3b23c29a=Assignment(partitions=[test-topic-0000109-8qoD6Cs-0])}
13:47:26.337 [pool-41-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-106-914gNsw-36, groupId=sub-106-914gNsw] Resetting offset for partition test-topic-0000106-DPglyWA-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.338 [pool-42-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-109-JW4VXwQ-37, groupId=sub-109-JW4VXwQ] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-109-JW4VXwQ-37-3c284ab9-79a4-42f5-8f75-cf3d3b23c29a', protocol='range'}
13:47:26.338 [pool-42-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-109-JW4VXwQ-37, groupId=sub-109-JW4VXwQ] Notifying assignor about the new Assignment(partitions=[test-topic-0000109-8qoD6Cs-0])
13:47:26.338 [pool-42-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-109-JW4VXwQ-37, groupId=sub-109-JW4VXwQ] Adding newly assigned partitions: test-topic-0000109-8qoD6Cs-0
13:47:26.339 [pool-42-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-109-JW4VXwQ-37, groupId=sub-109-JW4VXwQ] Found no committed offset for partition test-topic-0000109-8qoD6Cs-0
13:47:26.341 [pool-42-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-109-JW4VXwQ-37, groupId=sub-109-JW4VXwQ] Resetting offset for partition test-topic-0000109-8qoD6Cs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.341 [pool-43-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-112-VeqiZ0Q-38, groupId=sub-112-VeqiZ0Q] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-112-VeqiZ0Q-38-e5932f3b-da7b-41a2-ae31-318d5f31ad5c', protocol='range'}
13:47:26.342 [pool-43-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-112-VeqiZ0Q-38, groupId=sub-112-VeqiZ0Q] Finished assignment for group at generation 1: {consumer-sub-112-VeqiZ0Q-38-e5932f3b-da7b-41a2-ae31-318d5f31ad5c=Assignment(partitions=[test-topic-0000112-6kLoZnM-0])}
13:47:26.343 [pool-43-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-112-VeqiZ0Q-38, groupId=sub-112-VeqiZ0Q] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-112-VeqiZ0Q-38-e5932f3b-da7b-41a2-ae31-318d5f31ad5c', protocol='range'}
13:47:26.343 [pool-43-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-112-VeqiZ0Q-38, groupId=sub-112-VeqiZ0Q] Notifying assignor about the new Assignment(partitions=[test-topic-0000112-6kLoZnM-0])
13:47:26.344 [pool-43-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-112-VeqiZ0Q-38, groupId=sub-112-VeqiZ0Q] Adding newly assigned partitions: test-topic-0000112-6kLoZnM-0
13:47:26.344 [pool-43-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-112-VeqiZ0Q-38, groupId=sub-112-VeqiZ0Q] Found no committed offset for partition test-topic-0000112-6kLoZnM-0
13:47:26.346 [pool-43-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-112-VeqiZ0Q-38, groupId=sub-112-VeqiZ0Q] Resetting offset for partition test-topic-0000112-6kLoZnM-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.346 [pool-44-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-115-iVExwXU-39, groupId=sub-115-iVExwXU] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-115-iVExwXU-39-da19d54e-b3ef-47f7-86e6-508ef96c112f', protocol='range'}
13:47:26.346 [pool-44-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-115-iVExwXU-39, groupId=sub-115-iVExwXU] Finished assignment for group at generation 1: {consumer-sub-115-iVExwXU-39-da19d54e-b3ef-47f7-86e6-508ef96c112f=Assignment(partitions=[test-topic-0000115-6QgytUA-0])}
13:47:26.348 [pool-44-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-115-iVExwXU-39, groupId=sub-115-iVExwXU] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-115-iVExwXU-39-da19d54e-b3ef-47f7-86e6-508ef96c112f', protocol='range'}
13:47:26.348 [pool-44-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-115-iVExwXU-39, groupId=sub-115-iVExwXU] Notifying assignor about the new Assignment(partitions=[test-topic-0000115-6QgytUA-0])
13:47:26.348 [pool-44-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-115-iVExwXU-39, groupId=sub-115-iVExwXU] Adding newly assigned partitions: test-topic-0000115-6QgytUA-0
13:47:26.349 [pool-44-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-115-iVExwXU-39, groupId=sub-115-iVExwXU] Found no committed offset for partition test-topic-0000115-6QgytUA-0
13:47:26.350 [pool-45-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-118-4zQp7Rc-40, groupId=sub-118-4zQp7Rc] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-118-4zQp7Rc-40-4d4b2b18-538d-4869-b948-c702223e740c', protocol='range'}
13:47:26.350 [pool-45-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-118-4zQp7Rc-40, groupId=sub-118-4zQp7Rc] Finished assignment for group at generation 1: {consumer-sub-118-4zQp7Rc-40-4d4b2b18-538d-4869-b948-c702223e740c=Assignment(partitions=[test-topic-0000118-8tUi_6Q-0])}
13:47:26.351 [pool-44-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-115-iVExwXU-39, groupId=sub-115-iVExwXU] Resetting offset for partition test-topic-0000115-6QgytUA-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.351 [pool-45-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-118-4zQp7Rc-40, groupId=sub-118-4zQp7Rc] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-118-4zQp7Rc-40-4d4b2b18-538d-4869-b948-c702223e740c', protocol='range'}
13:47:26.352 [pool-45-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-118-4zQp7Rc-40, groupId=sub-118-4zQp7Rc] Notifying assignor about the new Assignment(partitions=[test-topic-0000118-8tUi_6Q-0])
13:47:26.352 [pool-45-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-118-4zQp7Rc-40, groupId=sub-118-4zQp7Rc] Adding newly assigned partitions: test-topic-0000118-8tUi_6Q-0
13:47:26.352 [pool-45-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-118-4zQp7Rc-40, groupId=sub-118-4zQp7Rc] Found no committed offset for partition test-topic-0000118-8tUi_6Q-0
13:47:26.354 [pool-45-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-118-4zQp7Rc-40, groupId=sub-118-4zQp7Rc] Resetting offset for partition test-topic-0000118-8tUi_6Q-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.357 [pool-46-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-121-oObYq0E-41, groupId=sub-121-oObYq0E] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-121-oObYq0E-41-005298f7-b253-4951-b524-43462d09a2b0', protocol='range'}
13:47:26.357 [pool-46-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-121-oObYq0E-41, groupId=sub-121-oObYq0E] Finished assignment for group at generation 1: {consumer-sub-121-oObYq0E-41-005298f7-b253-4951-b524-43462d09a2b0=Assignment(partitions=[test-topic-0000121-MbWpUXY-0])}
13:47:26.359 [pool-46-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-121-oObYq0E-41, groupId=sub-121-oObYq0E] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-121-oObYq0E-41-005298f7-b253-4951-b524-43462d09a2b0', protocol='range'}
13:47:26.359 [pool-46-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-121-oObYq0E-41, groupId=sub-121-oObYq0E] Notifying assignor about the new Assignment(partitions=[test-topic-0000121-MbWpUXY-0])
13:47:26.359 [pool-46-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-121-oObYq0E-41, groupId=sub-121-oObYq0E] Adding newly assigned partitions: test-topic-0000121-MbWpUXY-0
13:47:26.359 [pool-46-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-121-oObYq0E-41, groupId=sub-121-oObYq0E] Found no committed offset for partition test-topic-0000121-MbWpUXY-0
13:47:26.360 [pool-47-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-124-en_Hg3o-42, groupId=sub-124-en_Hg3o] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-124-en_Hg3o-42-5fc124b0-7f6e-4f66-a323-074ef035875b', protocol='range'}
13:47:26.360 [pool-47-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-124-en_Hg3o-42, groupId=sub-124-en_Hg3o] Finished assignment for group at generation 1: {consumer-sub-124-en_Hg3o-42-5fc124b0-7f6e-4f66-a323-074ef035875b=Assignment(partitions=[test-topic-0000124-amAHj1M-0])}
13:47:26.361 [pool-46-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-121-oObYq0E-41, groupId=sub-121-oObYq0E] Resetting offset for partition test-topic-0000121-MbWpUXY-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.361 [pool-47-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-124-en_Hg3o-42, groupId=sub-124-en_Hg3o] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-124-en_Hg3o-42-5fc124b0-7f6e-4f66-a323-074ef035875b', protocol='range'}
13:47:26.362 [pool-47-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-124-en_Hg3o-42, groupId=sub-124-en_Hg3o] Notifying assignor about the new Assignment(partitions=[test-topic-0000124-amAHj1M-0])
13:47:26.362 [pool-47-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-124-en_Hg3o-42, groupId=sub-124-en_Hg3o] Adding newly assigned partitions: test-topic-0000124-amAHj1M-0
13:47:26.362 [pool-47-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-124-en_Hg3o-42, groupId=sub-124-en_Hg3o] Found no committed offset for partition test-topic-0000124-amAHj1M-0
13:47:26.364 [pool-47-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-124-en_Hg3o-42, groupId=sub-124-en_Hg3o] Resetting offset for partition test-topic-0000124-amAHj1M-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.365 [pool-48-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-127-2kfMsto-43, groupId=sub-127-2kfMsto] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-127-2kfMsto-43-1c3781bf-41cf-4419-a262-b5234f50fb73', protocol='range'}
13:47:26.365 [pool-48-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-127-2kfMsto-43, groupId=sub-127-2kfMsto] Finished assignment for group at generation 1: {consumer-sub-127-2kfMsto-43-1c3781bf-41cf-4419-a262-b5234f50fb73=Assignment(partitions=[test-topic-0000127-yWtvaYk-0])}
13:47:26.369 [pool-49-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-130-dO66nCk-44, groupId=sub-130-dO66nCk] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-130-dO66nCk-44-2959ab19-f592-49ef-a403-a547a7284a9b', protocol='range'}
13:47:26.369 [pool-49-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-130-dO66nCk-44, groupId=sub-130-dO66nCk] Finished assignment for group at generation 1: {consumer-sub-130-dO66nCk-44-2959ab19-f592-49ef-a403-a547a7284a9b=Assignment(partitions=[test-topic-0000130-Qg40Jrk-0])}
13:47:26.372 [pool-48-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-127-2kfMsto-43, groupId=sub-127-2kfMsto] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-127-2kfMsto-43-1c3781bf-41cf-4419-a262-b5234f50fb73', protocol='range'}
13:47:26.372 [pool-48-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-127-2kfMsto-43, groupId=sub-127-2kfMsto] Notifying assignor about the new Assignment(partitions=[test-topic-0000127-yWtvaYk-0])
13:47:26.372 [pool-48-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-127-2kfMsto-43, groupId=sub-127-2kfMsto] Adding newly assigned partitions: test-topic-0000127-yWtvaYk-0
13:47:26.372 [pool-49-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-130-dO66nCk-44, groupId=sub-130-dO66nCk] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-130-dO66nCk-44-2959ab19-f592-49ef-a403-a547a7284a9b', protocol='range'}
13:47:26.373 [pool-49-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-130-dO66nCk-44, groupId=sub-130-dO66nCk] Notifying assignor about the new Assignment(partitions=[test-topic-0000130-Qg40Jrk-0])
13:47:26.373 [pool-49-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-130-dO66nCk-44, groupId=sub-130-dO66nCk] Adding newly assigned partitions: test-topic-0000130-Qg40Jrk-0
13:47:26.373 [pool-50-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-133-G2mVtvo-45, groupId=sub-133-G2mVtvo] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-133-G2mVtvo-45-fcf70cf4-ce4b-4040-82a9-b740e1fc8175', protocol='range'}
13:47:26.373 [pool-49-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-130-dO66nCk-44, groupId=sub-130-dO66nCk] Found no committed offset for partition test-topic-0000130-Qg40Jrk-0
13:47:26.373 [pool-50-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-133-G2mVtvo-45, groupId=sub-133-G2mVtvo] Finished assignment for group at generation 1: {consumer-sub-133-G2mVtvo-45-fcf70cf4-ce4b-4040-82a9-b740e1fc8175=Assignment(partitions=[test-topic-0000133-98k_qFM-0])}
13:47:26.375 [pool-50-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-133-G2mVtvo-45, groupId=sub-133-G2mVtvo] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-133-G2mVtvo-45-fcf70cf4-ce4b-4040-82a9-b740e1fc8175', protocol='range'}
13:47:26.375 [pool-50-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-133-G2mVtvo-45, groupId=sub-133-G2mVtvo] Notifying assignor about the new Assignment(partitions=[test-topic-0000133-98k_qFM-0])
13:47:26.376 [pool-50-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-133-G2mVtvo-45, groupId=sub-133-G2mVtvo] Adding newly assigned partitions: test-topic-0000133-98k_qFM-0
13:47:26.376 [pool-50-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-133-G2mVtvo-45, groupId=sub-133-G2mVtvo] Found no committed offset for partition test-topic-0000133-98k_qFM-0
13:47:26.377 [pool-48-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-127-2kfMsto-43, groupId=sub-127-2kfMsto] Found no committed offset for partition test-topic-0000127-yWtvaYk-0
13:47:26.378 [pool-51-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-136-6zUzaPQ-46, groupId=sub-136-6zUzaPQ] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-136-6zUzaPQ-46-d6e8672a-a7b3-4aef-b2d9-821f5f78d093', protocol='range'}
13:47:26.378 [pool-49-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-130-dO66nCk-44, groupId=sub-130-dO66nCk] Resetting offset for partition test-topic-0000130-Qg40Jrk-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.378 [pool-51-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-136-6zUzaPQ-46, groupId=sub-136-6zUzaPQ] Finished assignment for group at generation 1: {consumer-sub-136-6zUzaPQ-46-d6e8672a-a7b3-4aef-b2d9-821f5f78d093=Assignment(partitions=[test-topic-0000136-Y9wEdEA-0])}
13:47:26.380 [pool-51-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-136-6zUzaPQ-46, groupId=sub-136-6zUzaPQ] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-136-6zUzaPQ-46-d6e8672a-a7b3-4aef-b2d9-821f5f78d093', protocol='range'}
13:47:26.380 [pool-51-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-136-6zUzaPQ-46, groupId=sub-136-6zUzaPQ] Notifying assignor about the new Assignment(partitions=[test-topic-0000136-Y9wEdEA-0])
13:47:26.381 [pool-51-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-136-6zUzaPQ-46, groupId=sub-136-6zUzaPQ] Adding newly assigned partitions: test-topic-0000136-Y9wEdEA-0
13:47:26.381 [pool-51-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-136-6zUzaPQ-46, groupId=sub-136-6zUzaPQ] Found no committed offset for partition test-topic-0000136-Y9wEdEA-0
13:47:26.382 [pool-50-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-133-G2mVtvo-45, groupId=sub-133-G2mVtvo] Resetting offset for partition test-topic-0000133-98k_qFM-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.382 [pool-48-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-127-2kfMsto-43, groupId=sub-127-2kfMsto] Resetting offset for partition test-topic-0000127-yWtvaYk-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.384 [pool-51-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-136-6zUzaPQ-46, groupId=sub-136-6zUzaPQ] Resetting offset for partition test-topic-0000136-Y9wEdEA-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.384 [pool-52-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-139-HsIn88s-47, groupId=sub-139-HsIn88s] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-139-HsIn88s-47-1e76a18f-4fd4-4f59-8f05-071f85da7d1a', protocol='range'}
13:47:26.385 [pool-52-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-139-HsIn88s-47, groupId=sub-139-HsIn88s] Finished assignment for group at generation 1: {consumer-sub-139-HsIn88s-47-1e76a18f-4fd4-4f59-8f05-071f85da7d1a=Assignment(partitions=[test-topic-0000139-OuBTmUg-0])}
13:47:26.386 [pool-52-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-139-HsIn88s-47, groupId=sub-139-HsIn88s] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-139-HsIn88s-47-1e76a18f-4fd4-4f59-8f05-071f85da7d1a', protocol='range'}
13:47:26.386 [pool-52-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-139-HsIn88s-47, groupId=sub-139-HsIn88s] Notifying assignor about the new Assignment(partitions=[test-topic-0000139-OuBTmUg-0])
13:47:26.387 [pool-52-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-139-HsIn88s-47, groupId=sub-139-HsIn88s] Adding newly assigned partitions: test-topic-0000139-OuBTmUg-0
13:47:26.387 [pool-52-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-139-HsIn88s-47, groupId=sub-139-HsIn88s] Found no committed offset for partition test-topic-0000139-OuBTmUg-0
13:47:26.389 [pool-52-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-139-HsIn88s-47, groupId=sub-139-HsIn88s] Resetting offset for partition test-topic-0000139-OuBTmUg-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.390 [pool-53-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-142-LVH9jo4-48, groupId=sub-142-LVH9jo4] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-142-LVH9jo4-48-287abba6-1191-4823-b235-730c1eed8d89', protocol='range'}
13:47:26.391 [pool-53-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-142-LVH9jo4-48, groupId=sub-142-LVH9jo4] Finished assignment for group at generation 1: {consumer-sub-142-LVH9jo4-48-287abba6-1191-4823-b235-730c1eed8d89=Assignment(partitions=[test-topic-0000142-PXtC-SI-0])}
13:47:26.392 [pool-53-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-142-LVH9jo4-48, groupId=sub-142-LVH9jo4] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-142-LVH9jo4-48-287abba6-1191-4823-b235-730c1eed8d89', protocol='range'}
13:47:26.393 [pool-53-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-142-LVH9jo4-48, groupId=sub-142-LVH9jo4] Notifying assignor about the new Assignment(partitions=[test-topic-0000142-PXtC-SI-0])
13:47:26.393 [pool-53-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-142-LVH9jo4-48, groupId=sub-142-LVH9jo4] Adding newly assigned partitions: test-topic-0000142-PXtC-SI-0
13:47:26.394 [pool-53-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-142-LVH9jo4-48, groupId=sub-142-LVH9jo4] Found no committed offset for partition test-topic-0000142-PXtC-SI-0
13:47:26.396 [pool-53-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-142-LVH9jo4-48, groupId=sub-142-LVH9jo4] Resetting offset for partition test-topic-0000142-PXtC-SI-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.400 [pool-54-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-145-Car66lg-49, groupId=sub-145-Car66lg] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-145-Car66lg-49-94d64ef4-42d0-4a9f-9016-13d66945953f', protocol='range'}
13:47:26.400 [pool-54-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-145-Car66lg-49, groupId=sub-145-Car66lg] Finished assignment for group at generation 1: {consumer-sub-145-Car66lg-49-94d64ef4-42d0-4a9f-9016-13d66945953f=Assignment(partitions=[test-topic-0000145-c-Awvi4-0])}
13:47:26.402 [pool-54-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-145-Car66lg-49, groupId=sub-145-Car66lg] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-145-Car66lg-49-94d64ef4-42d0-4a9f-9016-13d66945953f', protocol='range'}
13:47:26.402 [pool-54-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-145-Car66lg-49, groupId=sub-145-Car66lg] Notifying assignor about the new Assignment(partitions=[test-topic-0000145-c-Awvi4-0])
13:47:26.402 [pool-54-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-145-Car66lg-49, groupId=sub-145-Car66lg] Adding newly assigned partitions: test-topic-0000145-c-Awvi4-0
13:47:26.403 [pool-54-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-145-Car66lg-49, groupId=sub-145-Car66lg] Found no committed offset for partition test-topic-0000145-c-Awvi4-0
13:47:26.405 [pool-54-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-145-Car66lg-49, groupId=sub-145-Car66lg] Resetting offset for partition test-topic-0000145-c-Awvi4-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.405 [pool-55-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-148-g2FltJE-50, groupId=sub-148-g2FltJE] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-148-g2FltJE-50-6f8572e0-1cb0-484a-8c22-d2bbbcfb8f6d', protocol='range'}
13:47:26.405 [pool-55-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-148-g2FltJE-50, groupId=sub-148-g2FltJE] Finished assignment for group at generation 1: {consumer-sub-148-g2FltJE-50-6f8572e0-1cb0-484a-8c22-d2bbbcfb8f6d=Assignment(partitions=[test-topic-0000148-jGtN0UQ-0])}
13:47:26.407 [pool-55-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-148-g2FltJE-50, groupId=sub-148-g2FltJE] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-148-g2FltJE-50-6f8572e0-1cb0-484a-8c22-d2bbbcfb8f6d', protocol='range'}
13:47:26.407 [pool-55-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-148-g2FltJE-50, groupId=sub-148-g2FltJE] Notifying assignor about the new Assignment(partitions=[test-topic-0000148-jGtN0UQ-0])
13:47:26.407 [pool-55-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-148-g2FltJE-50, groupId=sub-148-g2FltJE] Adding newly assigned partitions: test-topic-0000148-jGtN0UQ-0
13:47:26.408 [pool-55-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-148-g2FltJE-50, groupId=sub-148-g2FltJE] Found no committed offset for partition test-topic-0000148-jGtN0UQ-0
13:47:26.409 [pool-56-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-151-UaCj-zQ-51, groupId=sub-151-UaCj-zQ] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-151-UaCj-zQ-51-f6cd4f25-df95-4a2c-bfb6-91f0daf41224', protocol='range'}
13:47:26.410 [pool-56-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-151-UaCj-zQ-51, groupId=sub-151-UaCj-zQ] Finished assignment for group at generation 1: {consumer-sub-151-UaCj-zQ-51-f6cd4f25-df95-4a2c-bfb6-91f0daf41224=Assignment(partitions=[test-topic-0000151-TzSMYyE-0])}
13:47:26.410 [pool-55-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-148-g2FltJE-50, groupId=sub-148-g2FltJE] Resetting offset for partition test-topic-0000148-jGtN0UQ-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.412 [pool-56-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-151-UaCj-zQ-51, groupId=sub-151-UaCj-zQ] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-151-UaCj-zQ-51-f6cd4f25-df95-4a2c-bfb6-91f0daf41224', protocol='range'}
13:47:26.412 [pool-56-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-151-UaCj-zQ-51, groupId=sub-151-UaCj-zQ] Notifying assignor about the new Assignment(partitions=[test-topic-0000151-TzSMYyE-0])
13:47:26.412 [pool-56-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-151-UaCj-zQ-51, groupId=sub-151-UaCj-zQ] Adding newly assigned partitions: test-topic-0000151-TzSMYyE-0
13:47:26.412 [pool-56-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-151-UaCj-zQ-51, groupId=sub-151-UaCj-zQ] Found no committed offset for partition test-topic-0000151-TzSMYyE-0
13:47:26.415 [pool-56-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-151-UaCj-zQ-51, groupId=sub-151-UaCj-zQ] Resetting offset for partition test-topic-0000151-TzSMYyE-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.415 [pool-57-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-154-JBCLsNU-52, groupId=sub-154-JBCLsNU] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-154-JBCLsNU-52-1615aa8c-82d7-4315-ad42-2c3439812c89', protocol='range'}
13:47:26.416 [pool-57-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-154-JBCLsNU-52, groupId=sub-154-JBCLsNU] Finished assignment for group at generation 1: {consumer-sub-154-JBCLsNU-52-1615aa8c-82d7-4315-ad42-2c3439812c89=Assignment(partitions=[test-topic-0000154-akFkJ1U-0])}
13:47:26.418 [pool-58-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-157-z_6wOmk-53, groupId=sub-157-z_6wOmk] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-157-z_6wOmk-53-1bc5d14d-6ab7-4ea7-ba0d-f5b412529a91', protocol='range'}
13:47:26.419 [pool-58-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-157-z_6wOmk-53, groupId=sub-157-z_6wOmk] Finished assignment for group at generation 1: {consumer-sub-157-z_6wOmk-53-1bc5d14d-6ab7-4ea7-ba0d-f5b412529a91=Assignment(partitions=[test-topic-0000157--IfLOsc-0])}
13:47:26.419 [pool-57-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-154-JBCLsNU-52, groupId=sub-154-JBCLsNU] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-154-JBCLsNU-52-1615aa8c-82d7-4315-ad42-2c3439812c89', protocol='range'}
13:47:26.419 [pool-57-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-154-JBCLsNU-52, groupId=sub-154-JBCLsNU] Notifying assignor about the new Assignment(partitions=[test-topic-0000154-akFkJ1U-0])
13:47:26.419 [pool-57-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-154-JBCLsNU-52, groupId=sub-154-JBCLsNU] Adding newly assigned partitions: test-topic-0000154-akFkJ1U-0
13:47:26.420 [pool-57-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-154-JBCLsNU-52, groupId=sub-154-JBCLsNU] Found no committed offset for partition test-topic-0000154-akFkJ1U-0
13:47:26.420 [pool-58-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-157-z_6wOmk-53, groupId=sub-157-z_6wOmk] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-157-z_6wOmk-53-1bc5d14d-6ab7-4ea7-ba0d-f5b412529a91', protocol='range'}
13:47:26.421 [pool-58-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-157-z_6wOmk-53, groupId=sub-157-z_6wOmk] Notifying assignor about the new Assignment(partitions=[test-topic-0000157--IfLOsc-0])
13:47:26.421 [pool-58-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-157-z_6wOmk-53, groupId=sub-157-z_6wOmk] Adding newly assigned partitions: test-topic-0000157--IfLOsc-0
13:47:26.421 [pool-58-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-157-z_6wOmk-53, groupId=sub-157-z_6wOmk] Found no committed offset for partition test-topic-0000157--IfLOsc-0
13:47:26.422 [pool-57-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-154-JBCLsNU-52, groupId=sub-154-JBCLsNU] Resetting offset for partition test-topic-0000154-akFkJ1U-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.423 [pool-59-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-160-dA9F2fM-54, groupId=sub-160-dA9F2fM] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-160-dA9F2fM-54-e31b8255-2633-4fa9-a6dc-a4de36f2b4a0', protocol='range'}
13:47:26.423 [pool-59-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-160-dA9F2fM-54, groupId=sub-160-dA9F2fM] Finished assignment for group at generation 1: {consumer-sub-160-dA9F2fM-54-e31b8255-2633-4fa9-a6dc-a4de36f2b4a0=Assignment(partitions=[test-topic-0000160-mjkwCiM-0])}
13:47:26.425 [pool-58-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-157-z_6wOmk-53, groupId=sub-157-z_6wOmk] Resetting offset for partition test-topic-0000157--IfLOsc-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.425 [pool-59-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-160-dA9F2fM-54, groupId=sub-160-dA9F2fM] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-160-dA9F2fM-54-e31b8255-2633-4fa9-a6dc-a4de36f2b4a0', protocol='range'}
13:47:26.425 [pool-59-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-160-dA9F2fM-54, groupId=sub-160-dA9F2fM] Notifying assignor about the new Assignment(partitions=[test-topic-0000160-mjkwCiM-0])
13:47:26.425 [pool-59-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-160-dA9F2fM-54, groupId=sub-160-dA9F2fM] Adding newly assigned partitions: test-topic-0000160-mjkwCiM-0
13:47:26.426 [pool-59-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-160-dA9F2fM-54, groupId=sub-160-dA9F2fM] Found no committed offset for partition test-topic-0000160-mjkwCiM-0
13:47:26.428 [pool-59-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-160-dA9F2fM-54, groupId=sub-160-dA9F2fM] Resetting offset for partition test-topic-0000160-mjkwCiM-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.429 [pool-60-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-163-JNFcgqM-55, groupId=sub-163-JNFcgqM] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-163-JNFcgqM-55-900fb8cb-b26f-4321-98b1-6f2a8b40dfac', protocol='range'}
13:47:26.429 [pool-60-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-163-JNFcgqM-55, groupId=sub-163-JNFcgqM] Finished assignment for group at generation 1: {consumer-sub-163-JNFcgqM-55-900fb8cb-b26f-4321-98b1-6f2a8b40dfac=Assignment(partitions=[test-topic-0000163-_ZTSuc8-0])}
13:47:26.431 [pool-60-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-163-JNFcgqM-55, groupId=sub-163-JNFcgqM] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-163-JNFcgqM-55-900fb8cb-b26f-4321-98b1-6f2a8b40dfac', protocol='range'}
13:47:26.431 [pool-60-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-163-JNFcgqM-55, groupId=sub-163-JNFcgqM] Notifying assignor about the new Assignment(partitions=[test-topic-0000163-_ZTSuc8-0])
13:47:26.431 [pool-60-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-163-JNFcgqM-55, groupId=sub-163-JNFcgqM] Adding newly assigned partitions: test-topic-0000163-_ZTSuc8-0
13:47:26.431 [pool-60-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-163-JNFcgqM-55, groupId=sub-163-JNFcgqM] Found no committed offset for partition test-topic-0000163-_ZTSuc8-0
13:47:26.433 [pool-61-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-166-gpfNMxQ-56, groupId=sub-166-gpfNMxQ] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-166-gpfNMxQ-56-880d3cbe-b2fc-4c8e-911e-c1bc2a866786', protocol='range'}
13:47:26.434 [pool-61-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-166-gpfNMxQ-56, groupId=sub-166-gpfNMxQ] Finished assignment for group at generation 1: {consumer-sub-166-gpfNMxQ-56-880d3cbe-b2fc-4c8e-911e-c1bc2a866786=Assignment(partitions=[test-topic-0000166-FI2baz0-0])}
13:47:26.434 [pool-60-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-163-JNFcgqM-55, groupId=sub-163-JNFcgqM] Resetting offset for partition test-topic-0000163-_ZTSuc8-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.435 [pool-61-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-166-gpfNMxQ-56, groupId=sub-166-gpfNMxQ] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-166-gpfNMxQ-56-880d3cbe-b2fc-4c8e-911e-c1bc2a866786', protocol='range'}
13:47:26.436 [pool-61-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-166-gpfNMxQ-56, groupId=sub-166-gpfNMxQ] Notifying assignor about the new Assignment(partitions=[test-topic-0000166-FI2baz0-0])
13:47:26.436 [pool-61-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-166-gpfNMxQ-56, groupId=sub-166-gpfNMxQ] Adding newly assigned partitions: test-topic-0000166-FI2baz0-0
13:47:26.436 [pool-61-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-166-gpfNMxQ-56, groupId=sub-166-gpfNMxQ] Found no committed offset for partition test-topic-0000166-FI2baz0-0
13:47:26.438 [pool-61-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-166-gpfNMxQ-56, groupId=sub-166-gpfNMxQ] Resetting offset for partition test-topic-0000166-FI2baz0-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.443 [pool-63-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-172-wfkKd_8-58, groupId=sub-172-wfkKd_8] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-172-wfkKd_8-58-d0dad028-7e14-4b24-bcd9-e8845ff04726', protocol='range'}
13:47:26.444 [pool-63-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-172-wfkKd_8-58, groupId=sub-172-wfkKd_8] Finished assignment for group at generation 1: {consumer-sub-172-wfkKd_8-58-d0dad028-7e14-4b24-bcd9-e8845ff04726=Assignment(partitions=[test-topic-0000172-zC5ZI88-0])}
13:47:26.444 [pool-62-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-169-Ss3iap4-57, groupId=sub-169-Ss3iap4] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-169-Ss3iap4-57-a4c1ff45-9ea5-405e-942b-d60a96cace53', protocol='range'}
13:47:26.445 [pool-62-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-169-Ss3iap4-57, groupId=sub-169-Ss3iap4] Finished assignment for group at generation 1: {consumer-sub-169-Ss3iap4-57-a4c1ff45-9ea5-405e-942b-d60a96cace53=Assignment(partitions=[test-topic-0000169-frnmqbA-0])}
13:47:26.445 [pool-63-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-172-wfkKd_8-58, groupId=sub-172-wfkKd_8] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-172-wfkKd_8-58-d0dad028-7e14-4b24-bcd9-e8845ff04726', protocol='range'}
13:47:26.445 [pool-63-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-172-wfkKd_8-58, groupId=sub-172-wfkKd_8] Notifying assignor about the new Assignment(partitions=[test-topic-0000172-zC5ZI88-0])
13:47:26.446 [pool-63-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-172-wfkKd_8-58, groupId=sub-172-wfkKd_8] Adding newly assigned partitions: test-topic-0000172-zC5ZI88-0
13:47:26.446 [pool-63-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-172-wfkKd_8-58, groupId=sub-172-wfkKd_8] Found no committed offset for partition test-topic-0000172-zC5ZI88-0
13:47:26.446 [pool-62-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-169-Ss3iap4-57, groupId=sub-169-Ss3iap4] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-169-Ss3iap4-57-a4c1ff45-9ea5-405e-942b-d60a96cace53', protocol='range'}
13:47:26.446 [pool-62-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-169-Ss3iap4-57, groupId=sub-169-Ss3iap4] Notifying assignor about the new Assignment(partitions=[test-topic-0000169-frnmqbA-0])
13:47:26.446 [pool-62-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-169-Ss3iap4-57, groupId=sub-169-Ss3iap4] Adding newly assigned partitions: test-topic-0000169-frnmqbA-0
13:47:26.447 [pool-62-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-169-Ss3iap4-57, groupId=sub-169-Ss3iap4] Found no committed offset for partition test-topic-0000169-frnmqbA-0
13:47:26.448 [pool-63-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-172-wfkKd_8-58, groupId=sub-172-wfkKd_8] Resetting offset for partition test-topic-0000172-zC5ZI88-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.449 [pool-64-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-175-e6kTYM0-59, groupId=sub-175-e6kTYM0] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-175-e6kTYM0-59-c7c2b8d0-6dc5-47c4-bdbf-9f2e04ead312', protocol='range'}
13:47:26.449 [pool-62-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-169-Ss3iap4-57, groupId=sub-169-Ss3iap4] Resetting offset for partition test-topic-0000169-frnmqbA-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.449 [pool-64-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-175-e6kTYM0-59, groupId=sub-175-e6kTYM0] Finished assignment for group at generation 1: {consumer-sub-175-e6kTYM0-59-c7c2b8d0-6dc5-47c4-bdbf-9f2e04ead312=Assignment(partitions=[test-topic-0000175-nKeyHaA-0])}
13:47:26.451 [pool-64-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-175-e6kTYM0-59, groupId=sub-175-e6kTYM0] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-175-e6kTYM0-59-c7c2b8d0-6dc5-47c4-bdbf-9f2e04ead312', protocol='range'}
13:47:26.451 [pool-64-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-175-e6kTYM0-59, groupId=sub-175-e6kTYM0] Notifying assignor about the new Assignment(partitions=[test-topic-0000175-nKeyHaA-0])
13:47:26.451 [pool-64-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-175-e6kTYM0-59, groupId=sub-175-e6kTYM0] Adding newly assigned partitions: test-topic-0000175-nKeyHaA-0
13:47:26.452 [pool-64-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-175-e6kTYM0-59, groupId=sub-175-e6kTYM0] Found no committed offset for partition test-topic-0000175-nKeyHaA-0
13:47:26.454 [pool-64-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-175-e6kTYM0-59, groupId=sub-175-e6kTYM0] Resetting offset for partition test-topic-0000175-nKeyHaA-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.454 [pool-65-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-178-Baic5GM-60, groupId=sub-178-Baic5GM] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-178-Baic5GM-60-64b95a95-0fda-4b00-ae5d-01f9e33cd8d6', protocol='range'}
13:47:26.454 [pool-65-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-178-Baic5GM-60, groupId=sub-178-Baic5GM] Finished assignment for group at generation 1: {consumer-sub-178-Baic5GM-60-64b95a95-0fda-4b00-ae5d-01f9e33cd8d6=Assignment(partitions=[test-topic-0000178-yN-12bs-0])}
13:47:26.456 [pool-65-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-178-Baic5GM-60, groupId=sub-178-Baic5GM] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-178-Baic5GM-60-64b95a95-0fda-4b00-ae5d-01f9e33cd8d6', protocol='range'}
13:47:26.456 [pool-65-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-178-Baic5GM-60, groupId=sub-178-Baic5GM] Notifying assignor about the new Assignment(partitions=[test-topic-0000178-yN-12bs-0])
13:47:26.456 [pool-65-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-178-Baic5GM-60, groupId=sub-178-Baic5GM] Adding newly assigned partitions: test-topic-0000178-yN-12bs-0
13:47:26.457 [pool-65-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-178-Baic5GM-60, groupId=sub-178-Baic5GM] Found no committed offset for partition test-topic-0000178-yN-12bs-0
13:47:26.459 [pool-66-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-181-SEtrYEw-61, groupId=sub-181-SEtrYEw] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-181-SEtrYEw-61-3343e373-0410-4584-8905-9c35f2a61c41', protocol='range'}
13:47:26.459 [pool-66-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-181-SEtrYEw-61, groupId=sub-181-SEtrYEw] Finished assignment for group at generation 1: {consumer-sub-181-SEtrYEw-61-3343e373-0410-4584-8905-9c35f2a61c41=Assignment(partitions=[test-topic-0000181-ILkihtM-0])}
13:47:26.459 [pool-65-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-178-Baic5GM-60, groupId=sub-178-Baic5GM] Resetting offset for partition test-topic-0000178-yN-12bs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.460 [pool-66-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-181-SEtrYEw-61, groupId=sub-181-SEtrYEw] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-181-SEtrYEw-61-3343e373-0410-4584-8905-9c35f2a61c41', protocol='range'}
13:47:26.461 [pool-66-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-181-SEtrYEw-61, groupId=sub-181-SEtrYEw] Notifying assignor about the new Assignment(partitions=[test-topic-0000181-ILkihtM-0])
13:47:26.461 [pool-66-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-181-SEtrYEw-61, groupId=sub-181-SEtrYEw] Adding newly assigned partitions: test-topic-0000181-ILkihtM-0
13:47:26.461 [pool-66-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-181-SEtrYEw-61, groupId=sub-181-SEtrYEw] Found no committed offset for partition test-topic-0000181-ILkihtM-0
13:47:26.463 [pool-66-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-181-SEtrYEw-61, groupId=sub-181-SEtrYEw] Resetting offset for partition test-topic-0000181-ILkihtM-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.469 [pool-67-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-184-wUomEWE-62, groupId=sub-184-wUomEWE] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-184-wUomEWE-62-6d157b5e-ada9-4965-884c-5fbb9e9fbf96', protocol='range'}
13:47:26.469 [pool-67-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-184-wUomEWE-62, groupId=sub-184-wUomEWE] Finished assignment for group at generation 1: {consumer-sub-184-wUomEWE-62-6d157b5e-ada9-4965-884c-5fbb9e9fbf96=Assignment(partitions=[test-topic-0000184-YsWG3PQ-0])}
13:47:26.470 [pool-68-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-187-8Yu2BAk-63, groupId=sub-187-8Yu2BAk] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-187-8Yu2BAk-63-e8fe87fa-8277-4f3b-8d93-61ffd14f3fdd', protocol='range'}
13:47:26.470 [pool-68-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-187-8Yu2BAk-63, groupId=sub-187-8Yu2BAk] Finished assignment for group at generation 1: {consumer-sub-187-8Yu2BAk-63-e8fe87fa-8277-4f3b-8d93-61ffd14f3fdd=Assignment(partitions=[test-topic-0000187-a7MsovY-0])}
13:47:26.471 [pool-67-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-184-wUomEWE-62, groupId=sub-184-wUomEWE] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-184-wUomEWE-62-6d157b5e-ada9-4965-884c-5fbb9e9fbf96', protocol='range'}
13:47:26.471 [pool-67-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-184-wUomEWE-62, groupId=sub-184-wUomEWE] Notifying assignor about the new Assignment(partitions=[test-topic-0000184-YsWG3PQ-0])
13:47:26.471 [pool-67-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-184-wUomEWE-62, groupId=sub-184-wUomEWE] Adding newly assigned partitions: test-topic-0000184-YsWG3PQ-0
13:47:26.471 [pool-67-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-184-wUomEWE-62, groupId=sub-184-wUomEWE] Found no committed offset for partition test-topic-0000184-YsWG3PQ-0
13:47:26.471 [pool-68-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-187-8Yu2BAk-63, groupId=sub-187-8Yu2BAk] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-187-8Yu2BAk-63-e8fe87fa-8277-4f3b-8d93-61ffd14f3fdd', protocol='range'}
13:47:26.472 [pool-68-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-187-8Yu2BAk-63, groupId=sub-187-8Yu2BAk] Notifying assignor about the new Assignment(partitions=[test-topic-0000187-a7MsovY-0])
13:47:26.473 [pool-68-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-187-8Yu2BAk-63, groupId=sub-187-8Yu2BAk] Adding newly assigned partitions: test-topic-0000187-a7MsovY-0
13:47:26.473 [pool-67-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-184-wUomEWE-62, groupId=sub-184-wUomEWE] Resetting offset for partition test-topic-0000184-YsWG3PQ-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.473 [pool-68-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-187-8Yu2BAk-63, groupId=sub-187-8Yu2BAk] Found no committed offset for partition test-topic-0000187-a7MsovY-0
13:47:26.475 [pool-68-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-187-8Yu2BAk-63, groupId=sub-187-8Yu2BAk] Resetting offset for partition test-topic-0000187-a7MsovY-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.476 [pool-69-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-190-gMLfOV4-64, groupId=sub-190-gMLfOV4] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-190-gMLfOV4-64-404fe1dd-d188-4f1b-9d0d-37aeccac64ff', protocol='range'}
13:47:26.476 [pool-69-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-190-gMLfOV4-64, groupId=sub-190-gMLfOV4] Finished assignment for group at generation 1: {consumer-sub-190-gMLfOV4-64-404fe1dd-d188-4f1b-9d0d-37aeccac64ff=Assignment(partitions=[test-topic-0000190-2u2ROx8-0])}
13:47:26.477 [pool-69-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-190-gMLfOV4-64, groupId=sub-190-gMLfOV4] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-190-gMLfOV4-64-404fe1dd-d188-4f1b-9d0d-37aeccac64ff', protocol='range'}
13:47:26.478 [pool-69-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-190-gMLfOV4-64, groupId=sub-190-gMLfOV4] Notifying assignor about the new Assignment(partitions=[test-topic-0000190-2u2ROx8-0])
13:47:26.478 [pool-69-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-190-gMLfOV4-64, groupId=sub-190-gMLfOV4] Adding newly assigned partitions: test-topic-0000190-2u2ROx8-0
13:47:26.478 [pool-69-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-190-gMLfOV4-64, groupId=sub-190-gMLfOV4] Found no committed offset for partition test-topic-0000190-2u2ROx8-0
13:47:26.480 [pool-69-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-190-gMLfOV4-64, groupId=sub-190-gMLfOV4] Resetting offset for partition test-topic-0000190-2u2ROx8-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.484 [pool-70-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-193-C454ui4-65, groupId=sub-193-C454ui4] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-193-C454ui4-65-3d9dabca-8916-4bb8-b626-3ebf50fdc09d', protocol='range'}
13:47:26.485 [pool-70-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-193-C454ui4-65, groupId=sub-193-C454ui4] Finished assignment for group at generation 1: {consumer-sub-193-C454ui4-65-3d9dabca-8916-4bb8-b626-3ebf50fdc09d=Assignment(partitions=[test-topic-0000193-ngfcLus-0])}
13:47:26.486 [pool-70-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-193-C454ui4-65, groupId=sub-193-C454ui4] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-193-C454ui4-65-3d9dabca-8916-4bb8-b626-3ebf50fdc09d', protocol='range'}
13:47:26.487 [pool-70-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-193-C454ui4-65, groupId=sub-193-C454ui4] Notifying assignor about the new Assignment(partitions=[test-topic-0000193-ngfcLus-0])
13:47:26.487 [pool-70-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-193-C454ui4-65, groupId=sub-193-C454ui4] Adding newly assigned partitions: test-topic-0000193-ngfcLus-0
13:47:26.487 [pool-70-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-193-C454ui4-65, groupId=sub-193-C454ui4] Found no committed offset for partition test-topic-0000193-ngfcLus-0
13:47:26.489 [pool-70-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-193-C454ui4-65, groupId=sub-193-C454ui4] Resetting offset for partition test-topic-0000193-ngfcLus-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.501 [pool-71-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-196-oTB6GQc-66, groupId=sub-196-oTB6GQc] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-196-oTB6GQc-66-20f8562d-c1c2-45c0-a767-dcb8663c143b', protocol='range'}
13:47:26.501 [pool-71-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-196-oTB6GQc-66, groupId=sub-196-oTB6GQc] Finished assignment for group at generation 1: {consumer-sub-196-oTB6GQc-66-20f8562d-c1c2-45c0-a767-dcb8663c143b=Assignment(partitions=[test-topic-0000196-IG_hwmw-0])}
13:47:26.503 [pool-71-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-196-oTB6GQc-66, groupId=sub-196-oTB6GQc] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-196-oTB6GQc-66-20f8562d-c1c2-45c0-a767-dcb8663c143b', protocol='range'}
13:47:26.503 [pool-71-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-196-oTB6GQc-66, groupId=sub-196-oTB6GQc] Notifying assignor about the new Assignment(partitions=[test-topic-0000196-IG_hwmw-0])
13:47:26.504 [pool-71-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-196-oTB6GQc-66, groupId=sub-196-oTB6GQc] Adding newly assigned partitions: test-topic-0000196-IG_hwmw-0
13:47:26.504 [pool-71-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-196-oTB6GQc-66, groupId=sub-196-oTB6GQc] Found no committed offset for partition test-topic-0000196-IG_hwmw-0
13:47:26.506 [pool-71-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-196-oTB6GQc-66, groupId=sub-196-oTB6GQc] Resetting offset for partition test-topic-0000196-IG_hwmw-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.510 [pool-72-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-199-UuD-INE-67, groupId=sub-199-UuD-INE] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-199-UuD-INE-67-0a0d9dfd-3cfc-42b2-982f-6fd1cb1ceb32', protocol='range'}
13:47:26.510 [pool-72-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-199-UuD-INE-67, groupId=sub-199-UuD-INE] Finished assignment for group at generation 1: {consumer-sub-199-UuD-INE-67-0a0d9dfd-3cfc-42b2-982f-6fd1cb1ceb32=Assignment(partitions=[test-topic-0000199-CtttN8Y-0])}
13:47:26.513 [pool-72-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-199-UuD-INE-67, groupId=sub-199-UuD-INE] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-199-UuD-INE-67-0a0d9dfd-3cfc-42b2-982f-6fd1cb1ceb32', protocol='range'}
13:47:26.513 [pool-72-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-199-UuD-INE-67, groupId=sub-199-UuD-INE] Notifying assignor about the new Assignment(partitions=[test-topic-0000199-CtttN8Y-0])
13:47:26.513 [pool-72-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-199-UuD-INE-67, groupId=sub-199-UuD-INE] Adding newly assigned partitions: test-topic-0000199-CtttN8Y-0
13:47:26.513 [pool-72-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-199-UuD-INE-67, groupId=sub-199-UuD-INE] Found no committed offset for partition test-topic-0000199-CtttN8Y-0
13:47:26.515 [pool-72-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-199-UuD-INE-67, groupId=sub-199-UuD-INE] Resetting offset for partition test-topic-0000199-CtttN8Y-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.518 [pool-74-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-205-y-mu9t8-69, groupId=sub-205-y-mu9t8] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-205-y-mu9t8-69-4be45384-e0e7-4656-a99f-67e2cc376520', protocol='range'}
13:47:26.518 [pool-74-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-205-y-mu9t8-69, groupId=sub-205-y-mu9t8] Finished assignment for group at generation 1: {consumer-sub-205-y-mu9t8-69-4be45384-e0e7-4656-a99f-67e2cc376520=Assignment(partitions=[test-topic-0000205-2mMmqQg-0])}
13:47:26.520 [pool-74-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-205-y-mu9t8-69, groupId=sub-205-y-mu9t8] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-205-y-mu9t8-69-4be45384-e0e7-4656-a99f-67e2cc376520', protocol='range'}
13:47:26.520 [pool-74-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-205-y-mu9t8-69, groupId=sub-205-y-mu9t8] Notifying assignor about the new Assignment(partitions=[test-topic-0000205-2mMmqQg-0])
13:47:26.520 [pool-74-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-205-y-mu9t8-69, groupId=sub-205-y-mu9t8] Adding newly assigned partitions: test-topic-0000205-2mMmqQg-0
13:47:26.521 [pool-74-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-205-y-mu9t8-69, groupId=sub-205-y-mu9t8] Found no committed offset for partition test-topic-0000205-2mMmqQg-0
13:47:26.521 [pool-73-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-202-V2MvyKU-68, groupId=sub-202-V2MvyKU] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-202-V2MvyKU-68-4020617f-d32f-431f-aa42-9a8fe38dcb52', protocol='range'}
13:47:26.523 [pool-73-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-202-V2MvyKU-68, groupId=sub-202-V2MvyKU] Finished assignment for group at generation 1: {consumer-sub-202-V2MvyKU-68-4020617f-d32f-431f-aa42-9a8fe38dcb52=Assignment(partitions=[test-topic-0000202-3wtSyrE-0])}
13:47:26.523 [pool-74-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-205-y-mu9t8-69, groupId=sub-205-y-mu9t8] Resetting offset for partition test-topic-0000205-2mMmqQg-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.524 [pool-73-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-202-V2MvyKU-68, groupId=sub-202-V2MvyKU] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-202-V2MvyKU-68-4020617f-d32f-431f-aa42-9a8fe38dcb52', protocol='range'}
13:47:26.524 [pool-73-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-202-V2MvyKU-68, groupId=sub-202-V2MvyKU] Notifying assignor about the new Assignment(partitions=[test-topic-0000202-3wtSyrE-0])
13:47:26.524 [pool-73-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-202-V2MvyKU-68, groupId=sub-202-V2MvyKU] Adding newly assigned partitions: test-topic-0000202-3wtSyrE-0
13:47:26.525 [pool-73-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-202-V2MvyKU-68, groupId=sub-202-V2MvyKU] Found no committed offset for partition test-topic-0000202-3wtSyrE-0
13:47:26.527 [pool-73-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-202-V2MvyKU-68, groupId=sub-202-V2MvyKU] Resetting offset for partition test-topic-0000202-3wtSyrE-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.529 [pool-76-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-211-HlvOMkY-71, groupId=sub-211-HlvOMkY] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-211-HlvOMkY-71-fe4f25c7-5d70-4d58-8094-5cf15502ad23', protocol='range'}
13:47:26.529 [pool-76-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-211-HlvOMkY-71, groupId=sub-211-HlvOMkY] Finished assignment for group at generation 1: {consumer-sub-211-HlvOMkY-71-fe4f25c7-5d70-4d58-8094-5cf15502ad23=Assignment(partitions=[test-topic-0000211-_2UT4Bs-0])}
13:47:26.531 [pool-76-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-211-HlvOMkY-71, groupId=sub-211-HlvOMkY] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-211-HlvOMkY-71-fe4f25c7-5d70-4d58-8094-5cf15502ad23', protocol='range'}
13:47:26.531 [pool-75-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-208-4UCS5aM-70, groupId=sub-208-4UCS5aM] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-208-4UCS5aM-70-aa8ad4bb-f6ba-4549-a1f8-36a428f68cd7', protocol='range'}
13:47:26.531 [pool-76-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-211-HlvOMkY-71, groupId=sub-211-HlvOMkY] Notifying assignor about the new Assignment(partitions=[test-topic-0000211-_2UT4Bs-0])
13:47:26.531 [pool-76-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-211-HlvOMkY-71, groupId=sub-211-HlvOMkY] Adding newly assigned partitions: test-topic-0000211-_2UT4Bs-0
13:47:26.531 [pool-75-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-208-4UCS5aM-70, groupId=sub-208-4UCS5aM] Finished assignment for group at generation 1: {consumer-sub-208-4UCS5aM-70-aa8ad4bb-f6ba-4549-a1f8-36a428f68cd7=Assignment(partitions=[test-topic-0000208-zYqakMk-0])}
13:47:26.532 [pool-76-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-211-HlvOMkY-71, groupId=sub-211-HlvOMkY] Found no committed offset for partition test-topic-0000211-_2UT4Bs-0
13:47:26.533 [pool-75-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-208-4UCS5aM-70, groupId=sub-208-4UCS5aM] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-208-4UCS5aM-70-aa8ad4bb-f6ba-4549-a1f8-36a428f68cd7', protocol='range'}
13:47:26.533 [pool-75-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-208-4UCS5aM-70, groupId=sub-208-4UCS5aM] Notifying assignor about the new Assignment(partitions=[test-topic-0000208-zYqakMk-0])
13:47:26.533 [pool-75-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-208-4UCS5aM-70, groupId=sub-208-4UCS5aM] Adding newly assigned partitions: test-topic-0000208-zYqakMk-0
13:47:26.533 [pool-76-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-211-HlvOMkY-71, groupId=sub-211-HlvOMkY] Resetting offset for partition test-topic-0000211-_2UT4Bs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.534 [pool-75-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-208-4UCS5aM-70, groupId=sub-208-4UCS5aM] Found no committed offset for partition test-topic-0000208-zYqakMk-0
13:47:26.536 [pool-75-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-208-4UCS5aM-70, groupId=sub-208-4UCS5aM] Resetting offset for partition test-topic-0000208-zYqakMk-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.538 [pool-77-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-214-Bg60hV0-72, groupId=sub-214-Bg60hV0] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-214-Bg60hV0-72-f001fc31-3671-4897-a6d3-27a30b7e62ff', protocol='range'}
13:47:26.539 [pool-77-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-214-Bg60hV0-72, groupId=sub-214-Bg60hV0] Finished assignment for group at generation 1: {consumer-sub-214-Bg60hV0-72-f001fc31-3671-4897-a6d3-27a30b7e62ff=Assignment(partitions=[test-topic-0000214-MyxBVy0-0])}
13:47:26.540 [pool-77-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-214-Bg60hV0-72, groupId=sub-214-Bg60hV0] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-214-Bg60hV0-72-f001fc31-3671-4897-a6d3-27a30b7e62ff', protocol='range'}
13:47:26.540 [pool-77-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-214-Bg60hV0-72, groupId=sub-214-Bg60hV0] Notifying assignor about the new Assignment(partitions=[test-topic-0000214-MyxBVy0-0])
13:47:26.540 [pool-77-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-214-Bg60hV0-72, groupId=sub-214-Bg60hV0] Adding newly assigned partitions: test-topic-0000214-MyxBVy0-0
13:47:26.541 [pool-77-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-214-Bg60hV0-72, groupId=sub-214-Bg60hV0] Found no committed offset for partition test-topic-0000214-MyxBVy0-0
13:47:26.543 [pool-77-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-214-Bg60hV0-72, groupId=sub-214-Bg60hV0] Resetting offset for partition test-topic-0000214-MyxBVy0-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.548 [pool-79-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-220-1KuraSw-74, groupId=sub-220-1KuraSw] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-220-1KuraSw-74-2ff2f1ee-1656-446c-a611-280d79b9030c', protocol='range'}
13:47:26.549 [pool-79-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-220-1KuraSw-74, groupId=sub-220-1KuraSw] Finished assignment for group at generation 1: {consumer-sub-220-1KuraSw-74-2ff2f1ee-1656-446c-a611-280d79b9030c=Assignment(partitions=[test-topic-0000220-uOm4gtE-0])}
13:47:26.549 [pool-78-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-217-MqEbvWM-73, groupId=sub-217-MqEbvWM] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-217-MqEbvWM-73-a570aa6d-0402-45fe-a538-cc041722790c', protocol='range'}
13:47:26.549 [pool-78-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-217-MqEbvWM-73, groupId=sub-217-MqEbvWM] Finished assignment for group at generation 1: {consumer-sub-217-MqEbvWM-73-a570aa6d-0402-45fe-a538-cc041722790c=Assignment(partitions=[test-topic-0000217-XNxlkQ4-0])}
13:47:26.550 [pool-79-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-220-1KuraSw-74, groupId=sub-220-1KuraSw] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-220-1KuraSw-74-2ff2f1ee-1656-446c-a611-280d79b9030c', protocol='range'}
13:47:26.551 [pool-79-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-220-1KuraSw-74, groupId=sub-220-1KuraSw] Notifying assignor about the new Assignment(partitions=[test-topic-0000220-uOm4gtE-0])
13:47:26.551 [pool-79-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-220-1KuraSw-74, groupId=sub-220-1KuraSw] Adding newly assigned partitions: test-topic-0000220-uOm4gtE-0
13:47:26.551 [pool-79-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-220-1KuraSw-74, groupId=sub-220-1KuraSw] Found no committed offset for partition test-topic-0000220-uOm4gtE-0
13:47:26.551 [pool-78-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-217-MqEbvWM-73, groupId=sub-217-MqEbvWM] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-217-MqEbvWM-73-a570aa6d-0402-45fe-a538-cc041722790c', protocol='range'}
13:47:26.551 [pool-78-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-217-MqEbvWM-73, groupId=sub-217-MqEbvWM] Notifying assignor about the new Assignment(partitions=[test-topic-0000217-XNxlkQ4-0])
13:47:26.552 [pool-78-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-217-MqEbvWM-73, groupId=sub-217-MqEbvWM] Adding newly assigned partitions: test-topic-0000217-XNxlkQ4-0
13:47:26.552 [pool-78-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-217-MqEbvWM-73, groupId=sub-217-MqEbvWM] Found no committed offset for partition test-topic-0000217-XNxlkQ4-0
13:47:26.553 [pool-79-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-220-1KuraSw-74, groupId=sub-220-1KuraSw] Resetting offset for partition test-topic-0000220-uOm4gtE-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.554 [pool-78-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-217-MqEbvWM-73, groupId=sub-217-MqEbvWM] Resetting offset for partition test-topic-0000217-XNxlkQ4-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.556 [pool-80-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-223-CpU6oas-75, groupId=sub-223-CpU6oas] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-223-CpU6oas-75-5ccd6af0-062a-4413-9451-d6a5a446cede', protocol='range'}
13:47:26.556 [pool-80-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-223-CpU6oas-75, groupId=sub-223-CpU6oas] Finished assignment for group at generation 1: {consumer-sub-223-CpU6oas-75-5ccd6af0-062a-4413-9451-d6a5a446cede=Assignment(partitions=[test-topic-0000223-NBqvIZI-0])}
13:47:26.558 [pool-80-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-223-CpU6oas-75, groupId=sub-223-CpU6oas] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-223-CpU6oas-75-5ccd6af0-062a-4413-9451-d6a5a446cede', protocol='range'}
13:47:26.558 [pool-80-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-223-CpU6oas-75, groupId=sub-223-CpU6oas] Notifying assignor about the new Assignment(partitions=[test-topic-0000223-NBqvIZI-0])
13:47:26.558 [pool-80-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-223-CpU6oas-75, groupId=sub-223-CpU6oas] Adding newly assigned partitions: test-topic-0000223-NBqvIZI-0
13:47:26.559 [pool-80-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-223-CpU6oas-75, groupId=sub-223-CpU6oas] Found no committed offset for partition test-topic-0000223-NBqvIZI-0
13:47:26.559 [pool-81-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-226-mSjBVK8-76, groupId=sub-226-mSjBVK8] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-226-mSjBVK8-76-15eaa6fc-c98a-4663-b146-cf1ec74b533b', protocol='range'}
13:47:26.559 [pool-81-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-226-mSjBVK8-76, groupId=sub-226-mSjBVK8] Finished assignment for group at generation 1: {consumer-sub-226-mSjBVK8-76-15eaa6fc-c98a-4663-b146-cf1ec74b533b=Assignment(partitions=[test-topic-0000226-cmxwGEA-0])}
13:47:26.561 [pool-80-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-223-CpU6oas-75, groupId=sub-223-CpU6oas] Resetting offset for partition test-topic-0000223-NBqvIZI-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.561 [pool-81-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-226-mSjBVK8-76, groupId=sub-226-mSjBVK8] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-226-mSjBVK8-76-15eaa6fc-c98a-4663-b146-cf1ec74b533b', protocol='range'}
13:47:26.561 [pool-81-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-226-mSjBVK8-76, groupId=sub-226-mSjBVK8] Notifying assignor about the new Assignment(partitions=[test-topic-0000226-cmxwGEA-0])
13:47:26.562 [pool-81-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-226-mSjBVK8-76, groupId=sub-226-mSjBVK8] Adding newly assigned partitions: test-topic-0000226-cmxwGEA-0
13:47:26.562 [pool-81-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-226-mSjBVK8-76, groupId=sub-226-mSjBVK8] Found no committed offset for partition test-topic-0000226-cmxwGEA-0
13:47:26.564 [pool-81-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-226-mSjBVK8-76, groupId=sub-226-mSjBVK8] Resetting offset for partition test-topic-0000226-cmxwGEA-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.566 [pool-82-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-229-4OSEzZ0-77, groupId=sub-229-4OSEzZ0] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-229-4OSEzZ0-77-d1f4b258-f440-49c9-acf9-93f414791f3c', protocol='range'}
13:47:26.566 [pool-82-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-229-4OSEzZ0-77, groupId=sub-229-4OSEzZ0] Finished assignment for group at generation 1: {consumer-sub-229-4OSEzZ0-77-d1f4b258-f440-49c9-acf9-93f414791f3c=Assignment(partitions=[test-topic-0000229-0fBbwTU-0])}
13:47:26.568 [pool-82-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-229-4OSEzZ0-77, groupId=sub-229-4OSEzZ0] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-229-4OSEzZ0-77-d1f4b258-f440-49c9-acf9-93f414791f3c', protocol='range'}
13:47:26.568 [pool-82-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-229-4OSEzZ0-77, groupId=sub-229-4OSEzZ0] Notifying assignor about the new Assignment(partitions=[test-topic-0000229-0fBbwTU-0])
13:47:26.568 [pool-82-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-229-4OSEzZ0-77, groupId=sub-229-4OSEzZ0] Adding newly assigned partitions: test-topic-0000229-0fBbwTU-0
13:47:26.568 [pool-83-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-232-IkCxllw-78, groupId=sub-232-IkCxllw] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-232-IkCxllw-78-048784b8-bf8d-4bc3-8938-7d9c1b3115eb', protocol='range'}
13:47:26.568 [pool-82-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-229-4OSEzZ0-77, groupId=sub-229-4OSEzZ0] Found no committed offset for partition test-topic-0000229-0fBbwTU-0
13:47:26.568 [pool-83-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-232-IkCxllw-78, groupId=sub-232-IkCxllw] Finished assignment for group at generation 1: {consumer-sub-232-IkCxllw-78-048784b8-bf8d-4bc3-8938-7d9c1b3115eb=Assignment(partitions=[test-topic-0000232-L-U4evs-0])}
13:47:26.570 [pool-82-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-229-4OSEzZ0-77, groupId=sub-229-4OSEzZ0] Resetting offset for partition test-topic-0000229-0fBbwTU-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.570 [pool-83-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-232-IkCxllw-78, groupId=sub-232-IkCxllw] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-232-IkCxllw-78-048784b8-bf8d-4bc3-8938-7d9c1b3115eb', protocol='range'}
13:47:26.570 [pool-83-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-232-IkCxllw-78, groupId=sub-232-IkCxllw] Notifying assignor about the new Assignment(partitions=[test-topic-0000232-L-U4evs-0])
13:47:26.570 [pool-83-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-232-IkCxllw-78, groupId=sub-232-IkCxllw] Adding newly assigned partitions: test-topic-0000232-L-U4evs-0
13:47:26.571 [pool-83-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-232-IkCxllw-78, groupId=sub-232-IkCxllw] Found no committed offset for partition test-topic-0000232-L-U4evs-0
13:47:26.573 [pool-83-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-232-IkCxllw-78, groupId=sub-232-IkCxllw] Resetting offset for partition test-topic-0000232-L-U4evs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.582 [pool-84-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-235-gPJ0DOc-79, groupId=sub-235-gPJ0DOc] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-235-gPJ0DOc-79-d0ccadd6-d1b9-4767-8c08-ef1cf9555634', protocol='range'}
13:47:26.583 [pool-84-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-235-gPJ0DOc-79, groupId=sub-235-gPJ0DOc] Finished assignment for group at generation 1: {consumer-sub-235-gPJ0DOc-79-d0ccadd6-d1b9-4767-8c08-ef1cf9555634=Assignment(partitions=[test-topic-0000235-daWCzN0-0])}
13:47:26.585 [pool-84-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-235-gPJ0DOc-79, groupId=sub-235-gPJ0DOc] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-235-gPJ0DOc-79-d0ccadd6-d1b9-4767-8c08-ef1cf9555634', protocol='range'}
13:47:26.585 [pool-84-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-235-gPJ0DOc-79, groupId=sub-235-gPJ0DOc] Notifying assignor about the new Assignment(partitions=[test-topic-0000235-daWCzN0-0])
13:47:26.585 [pool-84-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-235-gPJ0DOc-79, groupId=sub-235-gPJ0DOc] Adding newly assigned partitions: test-topic-0000235-daWCzN0-0
13:47:26.585 [pool-84-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-235-gPJ0DOc-79, groupId=sub-235-gPJ0DOc] Found no committed offset for partition test-topic-0000235-daWCzN0-0
13:47:26.585 [pool-86-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-241-5yLlWtg-81, groupId=sub-241-5yLlWtg] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-241-5yLlWtg-81-d29c9c83-a7a6-484a-9079-6d9ca60314c7', protocol='range'}
13:47:26.586 [pool-85-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-238-M6fph0Y-80, groupId=sub-238-M6fph0Y] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-238-M6fph0Y-80-d1a89b48-fa96-4a13-8b3c-067b46fb24a5', protocol='range'}
13:47:26.586 [pool-86-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-241-5yLlWtg-81, groupId=sub-241-5yLlWtg] Finished assignment for group at generation 1: {consumer-sub-241-5yLlWtg-81-d29c9c83-a7a6-484a-9079-6d9ca60314c7=Assignment(partitions=[test-topic-0000241-hsO-5Ig-0])}
13:47:26.586 [pool-85-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-238-M6fph0Y-80, groupId=sub-238-M6fph0Y] Finished assignment for group at generation 1: {consumer-sub-238-M6fph0Y-80-d1a89b48-fa96-4a13-8b3c-067b46fb24a5=Assignment(partitions=[test-topic-0000238-ATNTxQ8-0])}
13:47:26.587 [pool-84-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-235-gPJ0DOc-79, groupId=sub-235-gPJ0DOc] Resetting offset for partition test-topic-0000235-daWCzN0-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.588 [pool-85-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-238-M6fph0Y-80, groupId=sub-238-M6fph0Y] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-238-M6fph0Y-80-d1a89b48-fa96-4a13-8b3c-067b46fb24a5', protocol='range'}
13:47:26.588 [pool-85-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-238-M6fph0Y-80, groupId=sub-238-M6fph0Y] Notifying assignor about the new Assignment(partitions=[test-topic-0000238-ATNTxQ8-0])
13:47:26.588 [pool-85-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-238-M6fph0Y-80, groupId=sub-238-M6fph0Y] Adding newly assigned partitions: test-topic-0000238-ATNTxQ8-0
13:47:26.588 [pool-86-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-241-5yLlWtg-81, groupId=sub-241-5yLlWtg] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-241-5yLlWtg-81-d29c9c83-a7a6-484a-9079-6d9ca60314c7', protocol='range'}
13:47:26.588 [pool-86-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-241-5yLlWtg-81, groupId=sub-241-5yLlWtg] Notifying assignor about the new Assignment(partitions=[test-topic-0000241-hsO-5Ig-0])
13:47:26.588 [pool-86-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-241-5yLlWtg-81, groupId=sub-241-5yLlWtg] Adding newly assigned partitions: test-topic-0000241-hsO-5Ig-0
13:47:26.588 [pool-85-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-238-M6fph0Y-80, groupId=sub-238-M6fph0Y] Found no committed offset for partition test-topic-0000238-ATNTxQ8-0
13:47:26.588 [pool-86-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-241-5yLlWtg-81, groupId=sub-241-5yLlWtg] Found no committed offset for partition test-topic-0000241-hsO-5Ig-0
13:47:26.590 [pool-85-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-238-M6fph0Y-80, groupId=sub-238-M6fph0Y] Resetting offset for partition test-topic-0000238-ATNTxQ8-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.590 [pool-86-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-241-5yLlWtg-81, groupId=sub-241-5yLlWtg] Resetting offset for partition test-topic-0000241-hsO-5Ig-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.593 [pool-87-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-244-Rz6I8TE-82, groupId=sub-244-Rz6I8TE] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-244-Rz6I8TE-82-ca019eff-81b3-482a-b392-0ab2bc3c4e50', protocol='range'}
13:47:26.593 [pool-87-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-244-Rz6I8TE-82, groupId=sub-244-Rz6I8TE] Finished assignment for group at generation 1: {consumer-sub-244-Rz6I8TE-82-ca019eff-81b3-482a-b392-0ab2bc3c4e50=Assignment(partitions=[test-topic-0000244-CATmwfU-0])}
13:47:26.595 [pool-87-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-244-Rz6I8TE-82, groupId=sub-244-Rz6I8TE] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-244-Rz6I8TE-82-ca019eff-81b3-482a-b392-0ab2bc3c4e50', protocol='range'}
13:47:26.595 [pool-87-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-244-Rz6I8TE-82, groupId=sub-244-Rz6I8TE] Notifying assignor about the new Assignment(partitions=[test-topic-0000244-CATmwfU-0])
13:47:26.595 [pool-87-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-244-Rz6I8TE-82, groupId=sub-244-Rz6I8TE] Adding newly assigned partitions: test-topic-0000244-CATmwfU-0
13:47:26.595 [pool-87-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-244-Rz6I8TE-82, groupId=sub-244-Rz6I8TE] Found no committed offset for partition test-topic-0000244-CATmwfU-0
13:47:26.597 [pool-87-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-244-Rz6I8TE-82, groupId=sub-244-Rz6I8TE] Resetting offset for partition test-topic-0000244-CATmwfU-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.600 [pool-88-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-247-HNv8ZAY-83, groupId=sub-247-HNv8ZAY] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-247-HNv8ZAY-83-188c0ec5-6e3d-4e3b-90d5-582ab05b219c', protocol='range'}
13:47:26.601 [pool-88-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-247-HNv8ZAY-83, groupId=sub-247-HNv8ZAY] Finished assignment for group at generation 1: {consumer-sub-247-HNv8ZAY-83-188c0ec5-6e3d-4e3b-90d5-582ab05b219c=Assignment(partitions=[test-topic-0000247-9BaoGEM-0])}
13:47:26.603 [pool-88-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-247-HNv8ZAY-83, groupId=sub-247-HNv8ZAY] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-247-HNv8ZAY-83-188c0ec5-6e3d-4e3b-90d5-582ab05b219c', protocol='range'}
13:47:26.603 [pool-88-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-247-HNv8ZAY-83, groupId=sub-247-HNv8ZAY] Notifying assignor about the new Assignment(partitions=[test-topic-0000247-9BaoGEM-0])
13:47:26.603 [pool-88-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-247-HNv8ZAY-83, groupId=sub-247-HNv8ZAY] Adding newly assigned partitions: test-topic-0000247-9BaoGEM-0
13:47:26.603 [pool-88-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-247-HNv8ZAY-83, groupId=sub-247-HNv8ZAY] Found no committed offset for partition test-topic-0000247-9BaoGEM-0
13:47:26.605 [pool-88-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-247-HNv8ZAY-83, groupId=sub-247-HNv8ZAY] Resetting offset for partition test-topic-0000247-9BaoGEM-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.606 [pool-89-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-250-ioeMh9Q-84, groupId=sub-250-ioeMh9Q] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-250-ioeMh9Q-84-e8af7339-16b8-48c2-ae14-d60dc2dbf685', protocol='range'}
13:47:26.606 [pool-89-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-250-ioeMh9Q-84, groupId=sub-250-ioeMh9Q] Finished assignment for group at generation 1: {consumer-sub-250-ioeMh9Q-84-e8af7339-16b8-48c2-ae14-d60dc2dbf685=Assignment(partitions=[test-topic-0000250-hNzff68-0])}
13:47:26.608 [pool-89-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-250-ioeMh9Q-84, groupId=sub-250-ioeMh9Q] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-250-ioeMh9Q-84-e8af7339-16b8-48c2-ae14-d60dc2dbf685', protocol='range'}
13:47:26.608 [pool-89-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-250-ioeMh9Q-84, groupId=sub-250-ioeMh9Q] Notifying assignor about the new Assignment(partitions=[test-topic-0000250-hNzff68-0])
13:47:26.608 [pool-89-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-250-ioeMh9Q-84, groupId=sub-250-ioeMh9Q] Adding newly assigned partitions: test-topic-0000250-hNzff68-0
13:47:26.608 [pool-89-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-250-ioeMh9Q-84, groupId=sub-250-ioeMh9Q] Found no committed offset for partition test-topic-0000250-hNzff68-0
13:47:26.610 [pool-89-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-250-ioeMh9Q-84, groupId=sub-250-ioeMh9Q] Resetting offset for partition test-topic-0000250-hNzff68-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.613 [pool-91-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-256-E_xLa20-86, groupId=sub-256-E_xLa20] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-256-E_xLa20-86-cc3786d2-a15a-478e-ac9f-6e8e8b02aba0', protocol='range'}
13:47:26.613 [pool-91-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-256-E_xLa20-86, groupId=sub-256-E_xLa20] Finished assignment for group at generation 1: {consumer-sub-256-E_xLa20-86-cc3786d2-a15a-478e-ac9f-6e8e8b02aba0=Assignment(partitions=[test-topic-0000256-GzCULh8-0])}
13:47:26.614 [pool-91-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-256-E_xLa20-86, groupId=sub-256-E_xLa20] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-256-E_xLa20-86-cc3786d2-a15a-478e-ac9f-6e8e8b02aba0', protocol='range'}
13:47:26.615 [pool-91-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-256-E_xLa20-86, groupId=sub-256-E_xLa20] Notifying assignor about the new Assignment(partitions=[test-topic-0000256-GzCULh8-0])
13:47:26.615 [pool-91-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-256-E_xLa20-86, groupId=sub-256-E_xLa20] Adding newly assigned partitions: test-topic-0000256-GzCULh8-0
13:47:26.615 [pool-91-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-256-E_xLa20-86, groupId=sub-256-E_xLa20] Found no committed offset for partition test-topic-0000256-GzCULh8-0
13:47:26.616 [pool-90-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-253-TqYfbUA-85, groupId=sub-253-TqYfbUA] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-253-TqYfbUA-85-ede6a8e9-93f3-4ca5-be10-daed3eccb01d', protocol='range'}
13:47:26.616 [pool-90-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-253-TqYfbUA-85, groupId=sub-253-TqYfbUA] Finished assignment for group at generation 1: {consumer-sub-253-TqYfbUA-85-ede6a8e9-93f3-4ca5-be10-daed3eccb01d=Assignment(partitions=[test-topic-0000253-71qwhGg-0])}
13:47:26.617 [pool-91-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-256-E_xLa20-86, groupId=sub-256-E_xLa20] Resetting offset for partition test-topic-0000256-GzCULh8-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.618 [pool-90-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-253-TqYfbUA-85, groupId=sub-253-TqYfbUA] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-253-TqYfbUA-85-ede6a8e9-93f3-4ca5-be10-daed3eccb01d', protocol='range'}
13:47:26.618 [pool-90-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-253-TqYfbUA-85, groupId=sub-253-TqYfbUA] Notifying assignor about the new Assignment(partitions=[test-topic-0000253-71qwhGg-0])
13:47:26.618 [pool-90-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-253-TqYfbUA-85, groupId=sub-253-TqYfbUA] Adding newly assigned partitions: test-topic-0000253-71qwhGg-0
13:47:26.619 [pool-90-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-253-TqYfbUA-85, groupId=sub-253-TqYfbUA] Found no committed offset for partition test-topic-0000253-71qwhGg-0
13:47:26.621 [pool-90-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-253-TqYfbUA-85, groupId=sub-253-TqYfbUA] Resetting offset for partition test-topic-0000253-71qwhGg-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.629 [pool-93-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-262-ZxwXbos-88, groupId=sub-262-ZxwXbos] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-262-ZxwXbos-88-60890567-6e8b-4ddd-8a35-7f1cb18c5f16', protocol='range'}
13:47:26.629 [pool-93-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-262-ZxwXbos-88, groupId=sub-262-ZxwXbos] Finished assignment for group at generation 1: {consumer-sub-262-ZxwXbos-88-60890567-6e8b-4ddd-8a35-7f1cb18c5f16=Assignment(partitions=[test-topic-0000262-Z_n6Lqs-0])}
13:47:26.631 [pool-93-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-262-ZxwXbos-88, groupId=sub-262-ZxwXbos] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-262-ZxwXbos-88-60890567-6e8b-4ddd-8a35-7f1cb18c5f16', protocol='range'}
13:47:26.631 [pool-93-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-262-ZxwXbos-88, groupId=sub-262-ZxwXbos] Notifying assignor about the new Assignment(partitions=[test-topic-0000262-Z_n6Lqs-0])
13:47:26.631 [pool-93-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-262-ZxwXbos-88, groupId=sub-262-ZxwXbos] Adding newly assigned partitions: test-topic-0000262-Z_n6Lqs-0
13:47:26.632 [pool-93-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-262-ZxwXbos-88, groupId=sub-262-ZxwXbos] Found no committed offset for partition test-topic-0000262-Z_n6Lqs-0
13:47:26.633 [pool-94-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-265-kIY8JGg-89, groupId=sub-265-kIY8JGg] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-265-kIY8JGg-89-cf137a34-2e0d-4fe5-a3b3-8fb0296e0b2c', protocol='range'}
13:47:26.633 [pool-93-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-262-ZxwXbos-88, groupId=sub-262-ZxwXbos] Resetting offset for partition test-topic-0000262-Z_n6Lqs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.633 [pool-94-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-265-kIY8JGg-89, groupId=sub-265-kIY8JGg] Finished assignment for group at generation 1: {consumer-sub-265-kIY8JGg-89-cf137a34-2e0d-4fe5-a3b3-8fb0296e0b2c=Assignment(partitions=[test-topic-0000265-JPOr-Ag-0])}
13:47:26.634 [pool-92-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-259-L0HKRLY-87, groupId=sub-259-L0HKRLY] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-259-L0HKRLY-87-22ae2025-a840-4f97-9dc0-4fbc0b67d32f', protocol='range'}
13:47:26.634 [pool-92-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-259-L0HKRLY-87, groupId=sub-259-L0HKRLY] Finished assignment for group at generation 1: {consumer-sub-259-L0HKRLY-87-22ae2025-a840-4f97-9dc0-4fbc0b67d32f=Assignment(partitions=[test-topic-0000259-x2gdERw-0])}
13:47:26.635 [pool-94-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-265-kIY8JGg-89, groupId=sub-265-kIY8JGg] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-265-kIY8JGg-89-cf137a34-2e0d-4fe5-a3b3-8fb0296e0b2c', protocol='range'}
13:47:26.635 [pool-94-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-265-kIY8JGg-89, groupId=sub-265-kIY8JGg] Notifying assignor about the new Assignment(partitions=[test-topic-0000265-JPOr-Ag-0])
13:47:26.635 [pool-94-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-265-kIY8JGg-89, groupId=sub-265-kIY8JGg] Adding newly assigned partitions: test-topic-0000265-JPOr-Ag-0
13:47:26.635 [pool-92-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-259-L0HKRLY-87, groupId=sub-259-L0HKRLY] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-259-L0HKRLY-87-22ae2025-a840-4f97-9dc0-4fbc0b67d32f', protocol='range'}
13:47:26.635 [pool-92-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-259-L0HKRLY-87, groupId=sub-259-L0HKRLY] Notifying assignor about the new Assignment(partitions=[test-topic-0000259-x2gdERw-0])
13:47:26.635 [pool-92-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-259-L0HKRLY-87, groupId=sub-259-L0HKRLY] Adding newly assigned partitions: test-topic-0000259-x2gdERw-0
13:47:26.635 [pool-94-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-265-kIY8JGg-89, groupId=sub-265-kIY8JGg] Found no committed offset for partition test-topic-0000265-JPOr-Ag-0
13:47:26.636 [pool-92-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-259-L0HKRLY-87, groupId=sub-259-L0HKRLY] Found no committed offset for partition test-topic-0000259-x2gdERw-0
13:47:26.637 [pool-94-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-265-kIY8JGg-89, groupId=sub-265-kIY8JGg] Resetting offset for partition test-topic-0000265-JPOr-Ag-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.637 [pool-95-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-268-K1hWqxg-90, groupId=sub-268-K1hWqxg] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-268-K1hWqxg-90-c4528907-9065-47e8-b45c-cf053c5075af', protocol='range'}
13:47:26.638 [pool-95-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-268-K1hWqxg-90, groupId=sub-268-K1hWqxg] Finished assignment for group at generation 1: {consumer-sub-268-K1hWqxg-90-c4528907-9065-47e8-b45c-cf053c5075af=Assignment(partitions=[test-topic-0000268-xHueAw8-0])}
13:47:26.638 [pool-92-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-259-L0HKRLY-87, groupId=sub-259-L0HKRLY] Resetting offset for partition test-topic-0000259-x2gdERw-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.639 [pool-95-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-268-K1hWqxg-90, groupId=sub-268-K1hWqxg] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-268-K1hWqxg-90-c4528907-9065-47e8-b45c-cf053c5075af', protocol='range'}
13:47:26.639 [pool-95-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-268-K1hWqxg-90, groupId=sub-268-K1hWqxg] Notifying assignor about the new Assignment(partitions=[test-topic-0000268-xHueAw8-0])
13:47:26.639 [pool-95-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-268-K1hWqxg-90, groupId=sub-268-K1hWqxg] Adding newly assigned partitions: test-topic-0000268-xHueAw8-0
13:47:26.640 [pool-95-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-268-K1hWqxg-90, groupId=sub-268-K1hWqxg] Found no committed offset for partition test-topic-0000268-xHueAw8-0
13:47:26.641 [pool-96-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-271-Pnle5Mw-91, groupId=sub-271-Pnle5Mw] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-271-Pnle5Mw-91-0f426a3c-b2f0-416f-a229-0c4e8a41f002', protocol='range'}
13:47:26.641 [pool-96-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-271-Pnle5Mw-91, groupId=sub-271-Pnle5Mw] Finished assignment for group at generation 1: {consumer-sub-271-Pnle5Mw-91-0f426a3c-b2f0-416f-a229-0c4e8a41f002=Assignment(partitions=[test-topic-0000271-ZxlpV1c-0])}
13:47:26.641 [pool-95-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-268-K1hWqxg-90, groupId=sub-268-K1hWqxg] Resetting offset for partition test-topic-0000268-xHueAw8-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.643 [pool-96-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-271-Pnle5Mw-91, groupId=sub-271-Pnle5Mw] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-271-Pnle5Mw-91-0f426a3c-b2f0-416f-a229-0c4e8a41f002', protocol='range'}
13:47:26.643 [pool-96-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-271-Pnle5Mw-91, groupId=sub-271-Pnle5Mw] Notifying assignor about the new Assignment(partitions=[test-topic-0000271-ZxlpV1c-0])
13:47:26.643 [pool-96-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-271-Pnle5Mw-91, groupId=sub-271-Pnle5Mw] Adding newly assigned partitions: test-topic-0000271-ZxlpV1c-0
13:47:26.643 [pool-96-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-271-Pnle5Mw-91, groupId=sub-271-Pnle5Mw] Found no committed offset for partition test-topic-0000271-ZxlpV1c-0
13:47:26.645 [pool-96-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-271-Pnle5Mw-91, groupId=sub-271-Pnle5Mw] Resetting offset for partition test-topic-0000271-ZxlpV1c-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.649 [pool-97-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-274-KCl7VaY-92, groupId=sub-274-KCl7VaY] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-274-KCl7VaY-92-21462b45-169e-45d9-8a12-989df464c399', protocol='range'}
13:47:26.649 [pool-97-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-274-KCl7VaY-92, groupId=sub-274-KCl7VaY] Finished assignment for group at generation 1: {consumer-sub-274-KCl7VaY-92-21462b45-169e-45d9-8a12-989df464c399=Assignment(partitions=[test-topic-0000274-WBszb4M-0])}
13:47:26.650 [pool-98-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-277-Hee5c8Q-93, groupId=sub-277-Hee5c8Q] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-277-Hee5c8Q-93-8a3c9717-9b57-488f-a3a4-8b7b762cc78f', protocol='range'}
13:47:26.651 [pool-98-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-277-Hee5c8Q-93, groupId=sub-277-Hee5c8Q] Finished assignment for group at generation 1: {consumer-sub-277-Hee5c8Q-93-8a3c9717-9b57-488f-a3a4-8b7b762cc78f=Assignment(partitions=[test-topic-0000277-YF-_KfA-0])}
13:47:26.651 [pool-97-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-274-KCl7VaY-92, groupId=sub-274-KCl7VaY] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-274-KCl7VaY-92-21462b45-169e-45d9-8a12-989df464c399', protocol='range'}
13:47:26.651 [pool-97-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-274-KCl7VaY-92, groupId=sub-274-KCl7VaY] Notifying assignor about the new Assignment(partitions=[test-topic-0000274-WBszb4M-0])
13:47:26.651 [pool-97-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-274-KCl7VaY-92, groupId=sub-274-KCl7VaY] Adding newly assigned partitions: test-topic-0000274-WBszb4M-0
13:47:26.651 [pool-97-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-274-KCl7VaY-92, groupId=sub-274-KCl7VaY] Found no committed offset for partition test-topic-0000274-WBszb4M-0
13:47:26.652 [pool-98-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-277-Hee5c8Q-93, groupId=sub-277-Hee5c8Q] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-277-Hee5c8Q-93-8a3c9717-9b57-488f-a3a4-8b7b762cc78f', protocol='range'}
13:47:26.652 [pool-98-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-277-Hee5c8Q-93, groupId=sub-277-Hee5c8Q] Notifying assignor about the new Assignment(partitions=[test-topic-0000277-YF-_KfA-0])
13:47:26.652 [pool-98-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-277-Hee5c8Q-93, groupId=sub-277-Hee5c8Q] Adding newly assigned partitions: test-topic-0000277-YF-_KfA-0
13:47:26.653 [pool-98-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-277-Hee5c8Q-93, groupId=sub-277-Hee5c8Q] Found no committed offset for partition test-topic-0000277-YF-_KfA-0
13:47:26.653 [pool-97-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-274-KCl7VaY-92, groupId=sub-274-KCl7VaY] Resetting offset for partition test-topic-0000274-WBszb4M-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.654 [pool-98-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-277-Hee5c8Q-93, groupId=sub-277-Hee5c8Q] Resetting offset for partition test-topic-0000277-YF-_KfA-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.656 [pool-99-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-280-hP0y3fw-94, groupId=sub-280-hP0y3fw] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-280-hP0y3fw-94-537fd25b-dfe1-42b7-869e-bd68a7ec44e1', protocol='range'}
13:47:26.656 [pool-99-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-280-hP0y3fw-94, groupId=sub-280-hP0y3fw] Finished assignment for group at generation 1: {consumer-sub-280-hP0y3fw-94-537fd25b-dfe1-42b7-869e-bd68a7ec44e1=Assignment(partitions=[test-topic-0000280-Y3fRkQ4-0])}
13:47:26.658 [pool-99-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-280-hP0y3fw-94, groupId=sub-280-hP0y3fw] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-280-hP0y3fw-94-537fd25b-dfe1-42b7-869e-bd68a7ec44e1', protocol='range'}
13:47:26.658 [pool-99-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-280-hP0y3fw-94, groupId=sub-280-hP0y3fw] Notifying assignor about the new Assignment(partitions=[test-topic-0000280-Y3fRkQ4-0])
13:47:26.658 [pool-99-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-280-hP0y3fw-94, groupId=sub-280-hP0y3fw] Adding newly assigned partitions: test-topic-0000280-Y3fRkQ4-0
13:47:26.658 [pool-99-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-280-hP0y3fw-94, groupId=sub-280-hP0y3fw] Found no committed offset for partition test-topic-0000280-Y3fRkQ4-0
13:47:26.660 [pool-100-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-283-gmHp3L8-95, groupId=sub-283-gmHp3L8] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-283-gmHp3L8-95-4daacc85-50bd-4071-8d32-a365a85cd6fa', protocol='range'}
13:47:26.660 [pool-99-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-280-hP0y3fw-94, groupId=sub-280-hP0y3fw] Resetting offset for partition test-topic-0000280-Y3fRkQ4-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.660 [pool-100-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-283-gmHp3L8-95, groupId=sub-283-gmHp3L8] Finished assignment for group at generation 1: {consumer-sub-283-gmHp3L8-95-4daacc85-50bd-4071-8d32-a365a85cd6fa=Assignment(partitions=[test-topic-0000283-MdXNdPY-0])}
13:47:26.662 [pool-100-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-283-gmHp3L8-95, groupId=sub-283-gmHp3L8] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-283-gmHp3L8-95-4daacc85-50bd-4071-8d32-a365a85cd6fa', protocol='range'}
13:47:26.662 [pool-100-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-283-gmHp3L8-95, groupId=sub-283-gmHp3L8] Notifying assignor about the new Assignment(partitions=[test-topic-0000283-MdXNdPY-0])
13:47:26.663 [pool-100-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-283-gmHp3L8-95, groupId=sub-283-gmHp3L8] Adding newly assigned partitions: test-topic-0000283-MdXNdPY-0
13:47:26.663 [pool-100-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-283-gmHp3L8-95, groupId=sub-283-gmHp3L8] Found no committed offset for partition test-topic-0000283-MdXNdPY-0
13:47:26.665 [pool-100-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-283-gmHp3L8-95, groupId=sub-283-gmHp3L8] Resetting offset for partition test-topic-0000283-MdXNdPY-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.229:9092 (id: 2 rack: null)], epoch=0}}.
13:47:26.669 [pool-101-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-286-P82WLeA-96, groupId=sub-286-P82WLeA] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-286-P82WLeA-96-57b8524d-1b78-4194-b9ba-da58afa866e7', protocol='range'}
13:47:26.669 [pool-101-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-286-P82WLeA-96, groupId=sub-286-P82WLeA] Finished assignment for group at generation 1: {consumer-sub-286-P82WLeA-96-57b8524d-1b78-4194-b9ba-da58afa866e7=Assignment(partitions=[test-topic-0000286-yTITFyk-0])}
13:47:26.671 [pool-101-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-286-P82WLeA-96, groupId=sub-286-P82WLeA] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-286-P82WLeA-96-57b8524d-1b78-4194-b9ba-da58afa866e7', protocol='range'}
13:47:26.671 [pool-101-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-286-P82WLeA-96, groupId=sub-286-P82WLeA] Notifying assignor about the new Assignment(partitions=[test-topic-0000286-yTITFyk-0])
13:47:26.671 [pool-101-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-286-P82WLeA-96, groupId=sub-286-P82WLeA] Adding newly assigned partitions: test-topic-0000286-yTITFyk-0
13:47:26.671 [pool-101-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-286-P82WLeA-96, groupId=sub-286-P82WLeA] Found no committed offset for partition test-topic-0000286-yTITFyk-0
13:47:26.673 [pool-101-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-286-P82WLeA-96, groupId=sub-286-P82WLeA] Resetting offset for partition test-topic-0000286-yTITFyk-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.674 [pool-102-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-289-1YbAHy4-97, groupId=sub-289-1YbAHy4] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-289-1YbAHy4-97-41bbc808-089b-4b83-a29d-7037a11e21b4', protocol='range'}
13:47:26.674 [pool-102-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-289-1YbAHy4-97, groupId=sub-289-1YbAHy4] Finished assignment for group at generation 1: {consumer-sub-289-1YbAHy4-97-41bbc808-089b-4b83-a29d-7037a11e21b4=Assignment(partitions=[test-topic-0000289-gnsmK30-0])}
13:47:26.675 [pool-102-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-289-1YbAHy4-97, groupId=sub-289-1YbAHy4] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-289-1YbAHy4-97-41bbc808-089b-4b83-a29d-7037a11e21b4', protocol='range'}
13:47:26.676 [pool-102-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-289-1YbAHy4-97, groupId=sub-289-1YbAHy4] Notifying assignor about the new Assignment(partitions=[test-topic-0000289-gnsmK30-0])
13:47:26.676 [pool-102-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-289-1YbAHy4-97, groupId=sub-289-1YbAHy4] Adding newly assigned partitions: test-topic-0000289-gnsmK30-0
13:47:26.676 [pool-102-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-289-1YbAHy4-97, groupId=sub-289-1YbAHy4] Found no committed offset for partition test-topic-0000289-gnsmK30-0
13:47:26.678 [pool-102-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-289-1YbAHy4-97, groupId=sub-289-1YbAHy4] Resetting offset for partition test-topic-0000289-gnsmK30-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.678 [pool-103-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-292-7XbbrSY-98, groupId=sub-292-7XbbrSY] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-292-7XbbrSY-98-11471e5d-bb7e-4a18-a817-168eca2a6a2e', protocol='range'}
13:47:26.678 [pool-103-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-292-7XbbrSY-98, groupId=sub-292-7XbbrSY] Finished assignment for group at generation 1: {consumer-sub-292-7XbbrSY-98-11471e5d-bb7e-4a18-a817-168eca2a6a2e=Assignment(partitions=[test-topic-0000292-3RYn65c-0])}
13:47:26.680 [pool-103-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-292-7XbbrSY-98, groupId=sub-292-7XbbrSY] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-292-7XbbrSY-98-11471e5d-bb7e-4a18-a817-168eca2a6a2e', protocol='range'}
13:47:26.680 [pool-103-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-292-7XbbrSY-98, groupId=sub-292-7XbbrSY] Notifying assignor about the new Assignment(partitions=[test-topic-0000292-3RYn65c-0])
13:47:26.680 [pool-103-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-292-7XbbrSY-98, groupId=sub-292-7XbbrSY] Adding newly assigned partitions: test-topic-0000292-3RYn65c-0
13:47:26.680 [pool-103-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-292-7XbbrSY-98, groupId=sub-292-7XbbrSY] Found no committed offset for partition test-topic-0000292-3RYn65c-0
13:47:26.682 [pool-103-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-292-7XbbrSY-98, groupId=sub-292-7XbbrSY] Resetting offset for partition test-topic-0000292-3RYn65c-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.687 [pool-104-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-295-aNnwtyo-99, groupId=sub-295-aNnwtyo] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-295-aNnwtyo-99-d2f9e3f6-5893-4c6d-99c7-315fa5a2be0f', protocol='range'}
13:47:26.687 [pool-105-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-298-sz-MSPk-100, groupId=sub-298-sz-MSPk] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-298-sz-MSPk-100-fe99ab57-3f3b-4580-928e-35f4819cc7b8', protocol='range'}
13:47:26.687 [pool-104-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-295-aNnwtyo-99, groupId=sub-295-aNnwtyo] Finished assignment for group at generation 1: {consumer-sub-295-aNnwtyo-99-d2f9e3f6-5893-4c6d-99c7-315fa5a2be0f=Assignment(partitions=[test-topic-0000295-5y9DMI4-0])}
13:47:26.688 [pool-105-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-298-sz-MSPk-100, groupId=sub-298-sz-MSPk] Finished assignment for group at generation 1: {consumer-sub-298-sz-MSPk-100-fe99ab57-3f3b-4580-928e-35f4819cc7b8=Assignment(partitions=[test-topic-0000298-cXyRfIQ-0])}
13:47:26.689 [pool-105-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-298-sz-MSPk-100, groupId=sub-298-sz-MSPk] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-298-sz-MSPk-100-fe99ab57-3f3b-4580-928e-35f4819cc7b8', protocol='range'}
13:47:26.689 [pool-104-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-295-aNnwtyo-99, groupId=sub-295-aNnwtyo] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-295-aNnwtyo-99-d2f9e3f6-5893-4c6d-99c7-315fa5a2be0f', protocol='range'}
13:47:26.689 [pool-105-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-298-sz-MSPk-100, groupId=sub-298-sz-MSPk] Notifying assignor about the new Assignment(partitions=[test-topic-0000298-cXyRfIQ-0])
13:47:26.689 [pool-104-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-295-aNnwtyo-99, groupId=sub-295-aNnwtyo] Notifying assignor about the new Assignment(partitions=[test-topic-0000295-5y9DMI4-0])
13:47:26.689 [pool-105-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-298-sz-MSPk-100, groupId=sub-298-sz-MSPk] Adding newly assigned partitions: test-topic-0000298-cXyRfIQ-0
13:47:26.689 [pool-104-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-295-aNnwtyo-99, groupId=sub-295-aNnwtyo] Adding newly assigned partitions: test-topic-0000295-5y9DMI4-0
13:47:26.690 [pool-104-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-295-aNnwtyo-99, groupId=sub-295-aNnwtyo] Found no committed offset for partition test-topic-0000295-5y9DMI4-0
13:47:26.690 [pool-106-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-301-kp9JcWM-101, groupId=sub-301-kp9JcWM] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-301-kp9JcWM-101-2ad10c95-d5eb-43e5-9c4a-b66cc6f0f1ba', protocol='range'}
13:47:26.690 [pool-106-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-301-kp9JcWM-101, groupId=sub-301-kp9JcWM] Finished assignment for group at generation 1: {consumer-sub-301-kp9JcWM-101-2ad10c95-d5eb-43e5-9c4a-b66cc6f0f1ba=Assignment(partitions=[test-topic-0000301-j62eWV0-0])}
13:47:26.690 [pool-105-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-298-sz-MSPk-100, groupId=sub-298-sz-MSPk] Found no committed offset for partition test-topic-0000298-cXyRfIQ-0
13:47:26.691 [pool-104-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-295-aNnwtyo-99, groupId=sub-295-aNnwtyo] Resetting offset for partition test-topic-0000295-5y9DMI4-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:26.692 [pool-106-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-301-kp9JcWM-101, groupId=sub-301-kp9JcWM] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-301-kp9JcWM-101-2ad10c95-d5eb-43e5-9c4a-b66cc6f0f1ba', protocol='range'}
13:47:26.692 [pool-106-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-301-kp9JcWM-101, groupId=sub-301-kp9JcWM] Notifying assignor about the new Assignment(partitions=[test-topic-0000301-j62eWV0-0])
13:47:26.692 [pool-106-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-301-kp9JcWM-101, groupId=sub-301-kp9JcWM] Adding newly assigned partitions: test-topic-0000301-j62eWV0-0
13:47:26.692 [pool-105-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-298-sz-MSPk-100, groupId=sub-298-sz-MSPk] Resetting offset for partition test-topic-0000298-cXyRfIQ-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:26.692 [pool-106-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-301-kp9JcWM-101, groupId=sub-301-kp9JcWM] Found no committed offset for partition test-topic-0000301-j62eWV0-0
13:47:26.694 [pool-106-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-301-kp9JcWM-101, groupId=sub-301-kp9JcWM] Resetting offset for partition test-topic-0000301-j62eWV0-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.52:9092 (id: 1 rack: null)], epoch=0}}.
13:47:27.152 [main] INFO WorkloadGenerator - Waiting for topics to be ready -- Sent: 927, Received: 457, Expected: 0
13:47:27.152 [main] INFO WorkloadGenerator - All consumers are ready!
13:47:27.153 [main] INFO WorkloadGenerator - [BenchmarkStart] Starting benchmark Kafka-tpc-h-tpc-h-q6-5000-300-2024-06-02-13-47-21 at 1717336047153
13:47:27.181 [qtp435803541-29] INFO WorkerHandler - Start load publish-rate: 3333333.3333333335 msg/s -- payload-size: 0 -- producer index: 0
13:47:27.182 [qtp435803541-29] INFO LocalWorker - Number of commands 1667 | Commands per batch 17 | Batches per producer 99
13:47:27.206 [throughput-worker-3-1] INFO LocalWorker - Number of batches 8 | Start 0 | Max 1667
13:47:27.244 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Launched 136 completable futures. Awaiting...
13:47:27.246 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Finished TPC-H producer 0-0 after sending 136 messages. Shutting down.
13:47:27.246 [throughput-worker-3-1] INFO LocalWorker - Number of batches 8 | Start 0 | Max 1667
13:47:27.266 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Launched 136 completable futures. Awaiting...
13:47:27.268 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Finished TPC-H producer 0-1 after sending 136 messages. Shutting down.
13:47:27.269 [throughput-worker-3-1] INFO LocalWorker - Number of batches 8 | Start 0 | Max 1667
13:47:27.290 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Launched 136 completable futures. Awaiting...
13:47:27.292 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Finished TPC-H producer 0-2 after sending 136 messages. Shutting down.
13:47:27.293 [throughput-worker-3-1] INFO LocalWorker - Number of batches 8 | Start 0 | Max 1667
13:47:27.312 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Launched 136 completable futures. Awaiting...
13:47:27.314 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Finished TPC-H producer 0-3 after sending 136 messages. Shutting down.
13:47:27.315 [throughput-worker-3-1] INFO LocalWorker - Number of batches 8 | Start 0 | Max 1667
13:47:27.331 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Launched 136 completable futures. Awaiting...
13:47:27.333 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Finished TPC-H producer 0-4 after sending 136 messages. Shutting down.
13:47:27.333 [throughput-worker-3-1] INFO LocalWorker - Number of batches 8 | Start 0 | Max 1667
13:47:27.350 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Launched 136 completable futures. Awaiting...
13:47:27.351 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Finished TPC-H producer 0-5 after sending 136 messages. Shutting down.
13:47:27.352 [throughput-worker-3-1] INFO LocalWorker - Number of batches 8 | Start 0 | Max 1667
13:47:27.365 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Launched 136 completable futures. Awaiting...
13:47:27.367 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Finished TPC-H producer 0-6 after sending 136 messages. Shutting down.
13:47:27.367 [throughput-worker-3-1] INFO LocalWorker - Number of batches 8 | Start 0 | Max 1667
13:47:27.376 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Launched 136 completable futures. Awaiting...
13:47:27.378 [throughput-worker-3-1] INFO LocalWorker - [TpcHBenchmark] Finished TPC-H producer 0-7 after sending 136 messages. Shutting down.
13:47:28.265 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000-H7fWDVE-1, groupId=sub-000-H7fWDVE] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-000-H7fWDVE-1-23576a1d-c3dc-422d-a9fa-c634917e05a4', protocol='range'}
13:47:28.274 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000-H7fWDVE-1, groupId=sub-000-H7fWDVE] Finished assignment for group at generation 1: {consumer-sub-000-H7fWDVE-1-23576a1d-c3dc-422d-a9fa-c634917e05a4=Assignment(partitions=[test-topic-0000000-FGdYwL4-0])}
13:47:28.283 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000-H7fWDVE-1, groupId=sub-000-H7fWDVE] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-000-H7fWDVE-1-23576a1d-c3dc-422d-a9fa-c634917e05a4', protocol='range'}
13:47:28.284 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000-H7fWDVE-1, groupId=sub-000-H7fWDVE] Notifying assignor about the new Assignment(partitions=[test-topic-0000000-FGdYwL4-0])
13:47:28.287 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000-H7fWDVE-1, groupId=sub-000-H7fWDVE] Adding newly assigned partitions: test-topic-0000000-FGdYwL4-0
13:47:28.295 [pool-6-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000-H7fWDVE-1, groupId=sub-000-H7fWDVE] Found no committed offset for partition test-topic-0000000-FGdYwL4-0
13:47:28.319 [pool-6-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-000-H7fWDVE-1, groupId=sub-000-H7fWDVE] Resetting offset for partition test-topic-0000000-FGdYwL4-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.216:9092 (id: 0 rack: null)], epoch=0}}.
13:47:38.760 [main] INFO WorkloadGenerator - Pub rate   350.3 msg/s /  0.2 MB/s | Pub err     0.0 err/s | Cons rate   140.2 msg/s /  0.0 MB/s | Backlog:  3.0 K | Pub Latency (ms) avg:  2.2 - 50%:  1.9 - 99%:  6.7 - 99.9%: 25.0 - Max: 142.6 | Pub Delay Latency (us) avg: 669654.9 - 50%: 117268.0 - 99%: 9831999.0 - 99.9%: 10297599.0 - Max: 10417151.0
13:47:40.553 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 10
13:47:40.823 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 20
13:47:41.026 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 30
13:47:41.412 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 40
13:47:42.082 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 50
13:47:43.259 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 60
13:47:44.513 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 70
13:47:45.114 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 80
13:47:45.611 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 90
13:47:46.270 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 100
13:47:47.018 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 110
13:47:47.773 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 120
13:47:49.265 [main] INFO WorkloadGenerator - Pub rate   204.4 msg/s /  0.1 MB/s | Pub err     0.0 err/s | Cons rate   386.9 msg/s /  0.2 MB/s | Backlog:  0.9 K | Pub Latency (ms) avg:  3.0 - 50%:  2.7 - 99%: 13.8 - 99.9%: 40.4 - Max: 48.5 | Pub Delay Latency (us) avg: 16470643.6 - 50%: 16513599.0 - 99%: 21569279.0 - 99.9%: 21663615.0 - Max: 21668351.0
13:47:49.292 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 130
13:47:49.578 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 140
13:47:50.368 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 150
13:47:51.373 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 160
13:47:51.753 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 170
13:47:52.102 [pool-6-thread-1] INFO LocalWorker - TPC-H progress: 180
13:47:59.522 [main] INFO WorkloadGenerator - Pub rate    79.7 msg/s /  0.0 MB/s | Pub err     0.0 err/s | Cons rate   147.4 msg/s /  0.1 MB/s | Backlog:  0.2 K | Pub Latency (ms) avg:  2.6 - 50%:  2.6 - 99%:  5.4 - 99.9%: 10.8 - Max: 10.8 | Pub Delay Latency (us) avg: 23377145.8 - 50%: 23408383.0 - 99%: 25163263.0 - 99.9%: 25203711.0 - Max: 25203711.0
13:48:09.720 [main] INFO WorkloadGenerator - Pub rate     0.0 msg/s /  0.0 MB/s | Pub err     0.0 err/s | Cons rate     0.0 msg/s /  0.0 MB/s | Backlog:  0.2 K | Pub Latency (ms) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0 | Pub Delay Latency (us) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0
13:48:19.919 [main] INFO WorkloadGenerator - Pub rate     0.0 msg/s /  0.0 MB/s | Pub err     0.0 err/s | Cons rate     0.0 msg/s /  0.0 MB/s | Backlog:  0.2 K | Pub Latency (ms) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0 | Pub Delay Latency (us) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0
13:48:30.123 [main] INFO WorkloadGenerator - Pub rate     0.0 msg/s /  0.0 MB/s | Pub err     0.0 err/s | Cons rate     0.0 msg/s /  0.0 MB/s | Backlog:  0.2 K | Pub Latency (ms) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0 | Pub Delay Latency (us) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0
13:48:40.320 [main] INFO WorkloadGenerator - Pub rate     0.0 msg/s /  0.0 MB/s | Pub err     0.0 err/s | Cons rate     0.0 msg/s /  0.0 MB/s | Backlog:  0.2 K | Pub Latency (ms) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0 | Pub Delay Latency (us) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0
13:48:50.521 [main] INFO WorkloadGenerator - Pub rate     0.0 msg/s /  0.0 MB/s | Pub err     0.0 err/s | Cons rate     0.0 msg/s /  0.0 MB/s | Backlog:  0.2 K | Pub Latency (ms) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0 | Pub Delay Latency (us) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0

09:46:24.833 [main] INFO log - Logging initialized @3680ms to org.eclipse.jetty.util.log.Slf4jLog
09:46:24.917 [main] INFO Server - jetty-9.4.42.v20210604; built: 2021-06-04T17:33:38.939Z; git: 5cd5e6d2375eeab146813b0de9f19eda6ab6e6cb; jvm 11.0.23+9-LTS
09:46:24.952 [main] INFO ContextHandler - Started o.e.j.s.ServletContextHandler@26d10f2e{/,null,AVAILABLE}
09:46:24.971 [main] INFO AbstractConnector - Started ServerConnector@1f130eaf{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
09:46:24.971 [main] INFO Server - Started @3822ms
09:46:24.974 [main] INFO PrometheusMetricsProvider - Started Prometheus stats endpoint at 0.0.0.0:8081
09:46:25.015 [main] INFO BenchmarkWorker - Starting benchmark with config: {
  "httpPort" : 8080,
  "statsPort" : 8081
}
09:46:25.062 [main] INFO Javalin - 
 _________________________________________
|        _                  _ _           |
|       | | __ ___   ____ _| (_)_ __      |
|    _  | |/ _` \ \ / / _` | | | '_ \     |
|   | |_| | (_| |\ V / (_| | | | | | |    |
|    \___/ \__,_| \_/ \__,_|_|_|_| |_|    |
|_________________________________________|
|                                         |
|    https://javalin.io/documentation     |
|_________________________________________|
09:46:25.065 [main] INFO Javalin - Starting Javalin ...
09:46:25.072 [main] INFO Server - jetty-9.4.42.v20210604; built: 2021-06-04T17:33:38.939Z; git: 5cd5e6d2375eeab146813b0de9f19eda6ab6e6cb; jvm 11.0.23+9-LTS
09:46:25.085 [main] INFO session - DefaultSessionIdManager workerName=node0
09:46:25.085 [main] INFO session - No SessionScavenger set, using defaults
09:46:25.086 [main] INFO session - node0 Scavenging every 600000ms
09:46:25.087 [main] INFO ContextHandler - Started i.j.e.j.@77128dab{/,null,AVAILABLE}
09:46:25.087 [main] INFO ContextHandler - Started o.e.j.s.ServletContextHandler@69ce2f62{/,null,AVAILABLE}
09:46:25.089 [main] INFO AbstractConnector - Started ServerConnector@70807224{HTTP/1.1, (http/1.1)}{0.0.0.0:8080}
09:46:25.089 [main] INFO Server - Started @3939ms
09:46:25.089 [main] INFO EmbeddedServer - Jetty is listening on: [http://localhost:8080]
09:46:25.089 [main] INFO Javalin - Javalin has started \o/
09:46:25.158 [main] INFO InstanceWorkerStats - Instance worker stats initialized.
09:47:12.606 [main] INFO Benchmark - Using default worker file workers.yaml!
09:47:12.614 [main] INFO Benchmark - Reading workers list from workers.yaml
09:47:12.661 [main] INFO Benchmark - Starting benchmark with config: {
  "drivers" : [ "driver-kafka/kafka-experiment.yaml" ],
  "workers" : [ "http://10.0.0.234:8080", "http://10.0.0.41:8080", "http://10.0.0.65:8080" ],
  "workersFile" : "/opt/benchmark/workers.yaml",
  "tpcHFiles" : [ "workloads/tpc-h-q6-100-50.yaml" ],
  "workloads" : [ "workloads/tpc-h-base-long.yaml" ],
  "output" : null
}
09:47:12.677 [main] INFO Benchmark - Workloads: {
  "tpc-h-base-long" : {
    "name" : "tpc-h",
    "topics" : 0,
    "partitionsPerTopic" : 1,
    "keyDistributor" : "NO_KEY",
    "messageSize" : 0,
    "useRandomizedPayloads" : false,
    "randomBytesRatio" : 0.0,
    "randomizedPayloadPoolSize" : 0,
    "payloadFile" : "",
    "subscriptionsPerTopic" : 1,
    "producersPerTopic" : 1,
    "consumerPerSubscription" : 1,
    "producerRate" : 10000000,
    "consumerBacklogSizeGB" : 0,
    "backlogDrainRatio" : 1.0,
    "testDurationMinutes" : 10,
    "warmupDurationMinutes" : 1
  }
}
09:47:12.687 [main] INFO Benchmark - TPC-H arguments: [ {
  "queryId" : "tpc-h-q6-100-50",
  "query" : "ForecastingRevenueChange",
  "sourceDataS3FolderUri" : "s3://tpc-h-chunks/chunks-by-file-size/70mb",
  "numberOfChunks" : 100,
  "numberOfWorkers" : 50
} ]
09:47:12.701 [main] INFO AdaptiveRateLimitedTaskProcessor - Initialising with 1 max concurrent tasks
09:47:12.726 [main] INFO InstanceWorkerStats - Instance worker stats initialized.
09:47:13.595 [main] INFO DistributedWorkersEnsemble - Workers list - producers: [http://10.0.0.234:8080,http://10.0.0.41:8080,http://10.0.0.65:8080]
09:47:13.596 [main] INFO DistributedWorkersEnsemble - Workers list - consumers: http://10.0.0.234:8080,http://10.0.0.41:8080,http://10.0.0.65:8080
09:47:13.598 [main] INFO Benchmark - --------------- WORKLOAD : tpc-h --- DRIVER : Kafka---------------
09:47:14.219 [qtp641030345-24] INFO LocalWorker - Driver: {
  "name" : "Kafka",
  "driverClass" : "io.openmessaging.benchmark.driver.kafka.KafkaBenchmarkDriver"
}
09:47:14.245 [qtp641030345-24] INFO AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 1200000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

09:47:14.296 [qtp641030345-24] INFO AppInfoParser - Kafka version: 3.6.1
09:47:14.296 [qtp641030345-24] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:14.296 [qtp641030345-24] INFO AppInfoParser - Kafka startTimeMs: 1718272034295
09:47:14.310 [main] INFO LocalWorker - Driver: {
  "name" : "Kafka",
  "driverClass" : "io.openmessaging.benchmark.driver.kafka.KafkaBenchmarkDriver"
}
09:47:14.332 [main] INFO AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 1200000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

09:47:14.384 [main] INFO AppInfoParser - Kafka version: 3.6.1
09:47:14.384 [main] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:14.385 [main] INFO AppInfoParser - Kafka startTimeMs: 1718272034383
09:47:14.400 [qtp641030345-29] INFO WorkerHandler - Received create topics request for topics: {
  "numberOfTopics" : 54,
  "numberOfPartitionsPerTopic" : 1
}
09:47:15.338 [qtp641030345-29] INFO LocalWorker - Created 54 topics in 937.176293 ms
09:47:15.350 [main] INFO WorkloadGenerator - Created 54 topics in 959.146461 ms
09:47:15.382 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-001-NP6cX5I-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-001-NP6cX5I
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.420 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.420 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.420 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035420
09:47:15.421 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Subscribed to topic(s): test-topic-0000001-a9CcqOE
09:47:15.423 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-004-xN56quE-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-004-xN56quE
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.431 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.431 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.431 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035431
09:47:15.431 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-004-xN56quE-2, groupId=sub-004-xN56quE] Subscribed to topic(s): test-topic-0000004-usTouY0
09:47:15.432 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-007-dAdh6Jk-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-007-dAdh6Jk
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.440 [pool-10-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.440 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.441 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.441 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035440
09:47:15.441 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-007-dAdh6Jk-3, groupId=sub-007-dAdh6Jk] Subscribed to topic(s): test-topic-0000007-yYH8VWg
09:47:15.441 [pool-11-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-004-xN56quE-2, groupId=sub-004-xN56quE] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.442 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-010-1Ocz0_o-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-010-1Ocz0_o
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.447 [pool-12-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-007-dAdh6Jk-3, groupId=sub-007-dAdh6Jk] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.449 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.449 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.449 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035448
09:47:15.449 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-010-1Ocz0_o-4, groupId=sub-010-1Ocz0_o] Subscribed to topic(s): test-topic-0000010-bCuPDwQ
09:47:15.451 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-013-cUISDQs-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-013-cUISDQs
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.455 [pool-13-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-010-1Ocz0_o-4, groupId=sub-010-1Ocz0_o] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.457 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.457 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.457 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035457
09:47:15.458 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-013-cUISDQs-5, groupId=sub-013-cUISDQs] Subscribed to topic(s): test-topic-0000013-Dlw5Sh4
09:47:15.459 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-016-BHkZggo-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-016-BHkZggo
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.463 [pool-14-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-013-cUISDQs-5, groupId=sub-013-cUISDQs] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.465 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.465 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.465 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035464
09:47:15.465 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-016-BHkZggo-6, groupId=sub-016-BHkZggo] Subscribed to topic(s): test-topic-0000016-hqYF7mM
09:47:15.466 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-019-TVYlklQ-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-019-TVYlklQ
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.470 [pool-15-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-016-BHkZggo-6, groupId=sub-016-BHkZggo] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.472 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.472 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.472 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035472
09:47:15.472 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-019-TVYlklQ-7, groupId=sub-019-TVYlklQ] Subscribed to topic(s): test-topic-0000019-ZVQUIRs
09:47:15.473 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-022-1m8YQwM-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-022-1m8YQwM
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.478 [pool-16-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-019-TVYlklQ-7, groupId=sub-019-TVYlklQ] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.479 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.479 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.479 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035479
09:47:15.479 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-022-1m8YQwM-8, groupId=sub-022-1m8YQwM] Subscribed to topic(s): test-topic-0000022-wCQ2bmI
09:47:15.480 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-025-2CjJ6bE-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-025-2CjJ6bE
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.485 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.485 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.485 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035485
09:47:15.486 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-025-2CjJ6bE-9, groupId=sub-025-2CjJ6bE] Subscribed to topic(s): test-topic-0000025-rG-WKrY
09:47:15.486 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-028-5tQIPxo-10
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-028-5tQIPxo
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.489 [pool-17-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-022-1m8YQwM-8, groupId=sub-022-1m8YQwM] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.490 [pool-18-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-025-2CjJ6bE-9, groupId=sub-025-2CjJ6bE] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.491 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.491 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.492 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035491
09:47:15.492 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Subscribed to topic(s): test-topic-0000028-4bCG-f8
09:47:15.493 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-031-N0T6Bp4-11
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-031-N0T6Bp4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.496 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.496 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.496 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035496
09:47:15.496 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-031-N0T6Bp4-11, groupId=sub-031-N0T6Bp4] Subscribed to topic(s): test-topic-0000031-QijEomI
09:47:15.497 [pool-19-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.497 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-034-pHRQrs0-12
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-034-pHRQrs0
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.501 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.501 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.501 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035501
09:47:15.501 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-034-pHRQrs0-12, groupId=sub-034-pHRQrs0] Subscribed to topic(s): test-topic-0000034-M4nrWXk
09:47:15.501 [pool-20-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-031-N0T6Bp4-11, groupId=sub-031-N0T6Bp4] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.503 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-037-tgno4IA-13
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-037-tgno4IA
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.506 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.506 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.506 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035506
09:47:15.507 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-037-tgno4IA-13, groupId=sub-037-tgno4IA] Subscribed to topic(s): test-topic-0000037-EqpJAsk
09:47:15.507 [pool-21-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-034-pHRQrs0-12, groupId=sub-034-pHRQrs0] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.508 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-040-Uts6bzM-14
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-040-Uts6bzM
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.511 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.511 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.511 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035511
09:47:15.511 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-040-Uts6bzM-14, groupId=sub-040-Uts6bzM] Subscribed to topic(s): test-topic-0000040-J1sFQIs
09:47:15.512 [pool-22-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-037-tgno4IA-13, groupId=sub-037-tgno4IA] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.513 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-043-6i3GEpQ-15
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-043-6i3GEpQ
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.516 [pool-23-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-040-Uts6bzM-14, groupId=sub-040-Uts6bzM] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.518 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.518 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.518 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035518
09:47:15.518 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-043-6i3GEpQ-15, groupId=sub-043-6i3GEpQ] Subscribed to topic(s): test-topic-0000043-4BDcIic
09:47:15.519 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-046-PlCtiIk-16
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-046-PlCtiIk
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.522 [pool-24-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-043-6i3GEpQ-15, groupId=sub-043-6i3GEpQ] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.526 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.526 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.526 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035526
09:47:15.526 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Subscribed to topic(s): test-topic-0000046-RG8GLes
09:47:15.527 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-049-Czrc7Rg-17
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-049-Czrc7Rg
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.531 [pool-25-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.532 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.532 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.532 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035532
09:47:15.533 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-049-Czrc7Rg-17, groupId=sub-049-Czrc7Rg] Subscribed to topic(s): test-topic-0000049-1BQUeek
09:47:15.534 [qtp641030345-28] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-052-QKzaf_k-18
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-052-QKzaf_k
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:15.537 [pool-26-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-049-Czrc7Rg-17, groupId=sub-049-Czrc7Rg] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.538 [qtp641030345-28] INFO AppInfoParser - Kafka version: 3.6.1
09:47:15.538 [qtp641030345-28] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:15.538 [qtp641030345-28] INFO AppInfoParser - Kafka startTimeMs: 1718272035538
09:47:15.538 [qtp641030345-28] INFO KafkaConsumer - [Consumer clientId=consumer-sub-052-QKzaf_k-18, groupId=sub-052-QKzaf_k] Subscribed to topic(s): test-topic-0000052-VtHfpgI
09:47:15.540 [qtp641030345-28] INFO LocalWorker - Created 18 consumers in 167.325534 ms
09:47:15.545 [pool-27-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-052-QKzaf_k-18, groupId=sub-052-QKzaf_k] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:15.694 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Discovered group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.696 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] (Re-)joining group
09:47:15.709 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-tgno4IA-13, groupId=sub-037-tgno4IA] Discovered group coordinator 10.0.0.122:9092 (id: 2147483645 rack: null)
09:47:15.710 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-tgno4IA-13, groupId=sub-037-tgno4IA] (Re-)joining group
09:47:15.711 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
09:47:15.711 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Requesting disconnect from last known coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.711 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
09:47:15.711 [pool-19-thread-1] INFO NetworkClient - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Client requested disconnect from node 2147483646
09:47:15.714 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Discovered group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.714 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
09:47:15.715 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Requesting disconnect from last known coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.727 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-tgno4IA-13, groupId=sub-037-tgno4IA] Request joining group due to: need to re-join with the given member-id: consumer-sub-037-tgno4IA-13-4d2e7b70-c76d-4330-a61e-5b5640e01ed4
09:47:15.727 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-tgno4IA-13, groupId=sub-037-tgno4IA] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.727 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-tgno4IA-13, groupId=sub-037-tgno4IA] (Re-)joining group
09:47:15.729 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Discovered group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.729 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] (Re-)joining group
09:47:15.730 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Discovered group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.731 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] (Re-)joining group
09:47:15.732 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
09:47:15.732 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Requesting disconnect from last known coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.732 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
09:47:15.732 [pool-25-thread-1] INFO NetworkClient - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Client requested disconnect from node 2147483646
09:47:15.734 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
09:47:15.734 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Requesting disconnect from last known coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.734 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
09:47:15.734 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Discovered group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.734 [pool-10-thread-1] INFO NetworkClient - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Client requested disconnect from node 2147483646
09:47:15.734 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
09:47:15.734 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Requesting disconnect from last known coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.734 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-Czrc7Rg-17, groupId=sub-049-Czrc7Rg] Discovered group coordinator 10.0.0.122:9092 (id: 2147483645 rack: null)
09:47:15.735 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-xN56quE-2, groupId=sub-004-xN56quE] Discovered group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.735 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-Czrc7Rg-17, groupId=sub-049-Czrc7Rg] (Re-)joining group
09:47:15.735 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-xN56quE-2, groupId=sub-004-xN56quE] (Re-)joining group
09:47:15.735 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Discovered group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.736 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
09:47:15.736 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Requesting disconnect from last known coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.738 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-Czrc7Rg-17, groupId=sub-049-Czrc7Rg] Request joining group due to: need to re-join with the given member-id: consumer-sub-049-Czrc7Rg-17-eafefb24-f3d2-4dbb-823d-1d358b921243
09:47:15.738 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-Czrc7Rg-17, groupId=sub-049-Czrc7Rg] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.738 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-Czrc7Rg-17, groupId=sub-049-Czrc7Rg] (Re-)joining group
09:47:15.742 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-QKzaf_k-18, groupId=sub-052-QKzaf_k] Discovered group coordinator 10.0.0.40:9092 (id: 2147483647 rack: null)
09:47:15.742 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-QKzaf_k-18, groupId=sub-052-QKzaf_k] (Re-)joining group
09:47:15.744 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-dAdh6Jk-3, groupId=sub-007-dAdh6Jk] Discovered group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.745 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-QKzaf_k-18, groupId=sub-052-QKzaf_k] Request joining group due to: need to re-join with the given member-id: consumer-sub-052-QKzaf_k-18-3568a76e-3be2-4b40-b7c9-8084867d2b6a
09:47:15.745 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-dAdh6Jk-3, groupId=sub-007-dAdh6Jk] (Re-)joining group
09:47:15.745 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-QKzaf_k-18, groupId=sub-052-QKzaf_k] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.745 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-QKzaf_k-18, groupId=sub-052-QKzaf_k] (Re-)joining group
09:47:15.746 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-xN56quE-2, groupId=sub-004-xN56quE] Request joining group due to: need to re-join with the given member-id: consumer-sub-004-xN56quE-2-211a7a74-b915-4f6e-bff4-3eda46bba633
09:47:15.746 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-xN56quE-2, groupId=sub-004-xN56quE] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.746 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-xN56quE-2, groupId=sub-004-xN56quE] (Re-)joining group
09:47:15.747 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-dAdh6Jk-3, groupId=sub-007-dAdh6Jk] Request joining group due to: need to re-join with the given member-id: consumer-sub-007-dAdh6Jk-3-e1b4b353-aa7c-4b92-8048-454c318451ee
09:47:15.748 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-dAdh6Jk-3, groupId=sub-007-dAdh6Jk] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.748 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-dAdh6Jk-3, groupId=sub-007-dAdh6Jk] (Re-)joining group
09:47:15.752 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-1Ocz0_o-4, groupId=sub-010-1Ocz0_o] Discovered group coordinator 10.0.0.40:9092 (id: 2147483647 rack: null)
09:47:15.752 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-1Ocz0_o-4, groupId=sub-010-1Ocz0_o] (Re-)joining group
09:47:15.754 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-1Ocz0_o-4, groupId=sub-010-1Ocz0_o] Request joining group due to: need to re-join with the given member-id: consumer-sub-010-1Ocz0_o-4-fb64c78f-6f18-417f-8be7-87e0976f79a1
09:47:15.755 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-1Ocz0_o-4, groupId=sub-010-1Ocz0_o] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.755 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-1Ocz0_o-4, groupId=sub-010-1Ocz0_o] (Re-)joining group
09:47:15.759 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-cUISDQs-5, groupId=sub-013-cUISDQs] Discovered group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.760 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-cUISDQs-5, groupId=sub-013-cUISDQs] (Re-)joining group
09:47:15.762 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-cUISDQs-5, groupId=sub-013-cUISDQs] Request joining group due to: need to re-join with the given member-id: consumer-sub-013-cUISDQs-5-9468fb3f-4b40-4b3b-9a5b-b3978f2c5476
09:47:15.763 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-cUISDQs-5, groupId=sub-013-cUISDQs] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.763 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-cUISDQs-5, groupId=sub-013-cUISDQs] (Re-)joining group
09:47:15.767 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-BHkZggo-6, groupId=sub-016-BHkZggo] Discovered group coordinator 10.0.0.40:9092 (id: 2147483647 rack: null)
09:47:15.767 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-BHkZggo-6, groupId=sub-016-BHkZggo] (Re-)joining group
09:47:15.769 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-BHkZggo-6, groupId=sub-016-BHkZggo] Request joining group due to: need to re-join with the given member-id: consumer-sub-016-BHkZggo-6-06336074-abee-4362-97db-1d0a16e21a1f
09:47:15.769 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-BHkZggo-6, groupId=sub-016-BHkZggo] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.770 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-BHkZggo-6, groupId=sub-016-BHkZggo] (Re-)joining group
09:47:15.775 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-TVYlklQ-7, groupId=sub-019-TVYlklQ] Discovered group coordinator 10.0.0.40:9092 (id: 2147483647 rack: null)
09:47:15.775 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-TVYlklQ-7, groupId=sub-019-TVYlklQ] (Re-)joining group
09:47:15.777 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-TVYlklQ-7, groupId=sub-019-TVYlklQ] Request joining group due to: need to re-join with the given member-id: consumer-sub-019-TVYlklQ-7-ee33af2b-2679-453e-ba51-e581d995cf64
09:47:15.777 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-TVYlklQ-7, groupId=sub-019-TVYlklQ] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.777 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-TVYlklQ-7, groupId=sub-019-TVYlklQ] (Re-)joining group
09:47:15.782 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-1m8YQwM-8, groupId=sub-022-1m8YQwM] Discovered group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.782 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-1m8YQwM-8, groupId=sub-022-1m8YQwM] (Re-)joining group
09:47:15.784 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-1m8YQwM-8, groupId=sub-022-1m8YQwM] Request joining group due to: need to re-join with the given member-id: consumer-sub-022-1m8YQwM-8-a502fac7-1a58-4c7a-aa80-ac5406f9b383
09:47:15.785 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-1m8YQwM-8, groupId=sub-022-1m8YQwM] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.785 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-1m8YQwM-8, groupId=sub-022-1m8YQwM] (Re-)joining group
09:47:15.788 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-2CjJ6bE-9, groupId=sub-025-2CjJ6bE] Discovered group coordinator 10.0.0.122:9092 (id: 2147483645 rack: null)
09:47:15.789 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-2CjJ6bE-9, groupId=sub-025-2CjJ6bE] (Re-)joining group
09:47:15.792 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-2CjJ6bE-9, groupId=sub-025-2CjJ6bE] Request joining group due to: need to re-join with the given member-id: consumer-sub-025-2CjJ6bE-9-4758eef4-f1f5-4a8d-907c-6be74e376be7
09:47:15.792 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-2CjJ6bE-9, groupId=sub-025-2CjJ6bE] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.792 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-2CjJ6bE-9, groupId=sub-025-2CjJ6bE] (Re-)joining group
09:47:15.794 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Discovered group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.794 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
09:47:15.794 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] (Re-)joining group
09:47:15.796 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Request joining group due to: need to re-join with the given member-id: consumer-sub-028-5tQIPxo-10-eadd295a-43bf-4a69-b9c8-36d822c30be5
09:47:15.796 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.796 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] (Re-)joining group
09:47:15.798 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-N0T6Bp4-11, groupId=sub-031-N0T6Bp4] Discovered group coordinator 10.0.0.122:9092 (id: 2147483645 rack: null)
09:47:15.799 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-N0T6Bp4-11, groupId=sub-031-N0T6Bp4] (Re-)joining group
09:47:15.801 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-N0T6Bp4-11, groupId=sub-031-N0T6Bp4] Request joining group due to: need to re-join with the given member-id: consumer-sub-031-N0T6Bp4-11-521e31cd-0cd1-462e-962c-3bd01ea05f83
09:47:15.801 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-N0T6Bp4-11, groupId=sub-031-N0T6Bp4] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.801 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-N0T6Bp4-11, groupId=sub-031-N0T6Bp4] (Re-)joining group
09:47:15.805 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-pHRQrs0-12, groupId=sub-034-pHRQrs0] Discovered group coordinator 10.0.0.40:9092 (id: 2147483647 rack: null)
09:47:15.806 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-pHRQrs0-12, groupId=sub-034-pHRQrs0] (Re-)joining group
09:47:15.808 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-pHRQrs0-12, groupId=sub-034-pHRQrs0] Request joining group due to: need to re-join with the given member-id: consumer-sub-034-pHRQrs0-12-2dee4590-1edd-41c3-bdc8-80d6884bc5cd
09:47:15.808 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-pHRQrs0-12, groupId=sub-034-pHRQrs0] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.808 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-pHRQrs0-12, groupId=sub-034-pHRQrs0] (Re-)joining group
09:47:15.815 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040-Uts6bzM-14, groupId=sub-040-Uts6bzM] Discovered group coordinator 10.0.0.122:9092 (id: 2147483645 rack: null)
09:47:15.815 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040-Uts6bzM-14, groupId=sub-040-Uts6bzM] (Re-)joining group
09:47:15.818 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040-Uts6bzM-14, groupId=sub-040-Uts6bzM] Request joining group due to: need to re-join with the given member-id: consumer-sub-040-Uts6bzM-14-295c31a2-8d70-46f8-9fa2-2b8efb80abaa
09:47:15.819 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040-Uts6bzM-14, groupId=sub-040-Uts6bzM] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.819 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040-Uts6bzM-14, groupId=sub-040-Uts6bzM] (Re-)joining group
09:47:15.820 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-6i3GEpQ-15, groupId=sub-043-6i3GEpQ] Discovered group coordinator 10.0.0.122:9092 (id: 2147483645 rack: null)
09:47:15.821 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-6i3GEpQ-15, groupId=sub-043-6i3GEpQ] (Re-)joining group
09:47:15.823 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-6i3GEpQ-15, groupId=sub-043-6i3GEpQ] Request joining group due to: need to re-join with the given member-id: consumer-sub-043-6i3GEpQ-15-dd9694ad-ed89-4d8c-a014-415f444e1c96
09:47:15.823 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-6i3GEpQ-15, groupId=sub-043-6i3GEpQ] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.823 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-6i3GEpQ-15, groupId=sub-043-6i3GEpQ] (Re-)joining group
09:47:15.829 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Discovered group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.830 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
09:47:15.830 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] (Re-)joining group
09:47:15.831 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Discovered group coordinator 10.0.0.162:9092 (id: 2147483646 rack: null)
09:47:15.832 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
09:47:15.832 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Request joining group due to: need to re-join with the given member-id: consumer-sub-046-PlCtiIk-16-10c96e96-3e01-407d-81cf-9f2eea724e11
09:47:15.832 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.832 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] (Re-)joining group
09:47:15.832 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] (Re-)joining group
09:47:15.833 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Request joining group due to: need to re-join with the given member-id: consumer-sub-001-NP6cX5I-1-a1aa6ee7-7f7b-4a47-bc39-2b3022f99ae2
09:47:15.834 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:15.834 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] (Re-)joining group
09:47:15.978 [main] INFO WorkloadGenerator - Created 54 external consumers in 620.546179 ms
09:47:15.988 [main] INFO ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-sub-000--4kasEA-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 1200000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sub-000--4kasEA
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 10485760
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:47:16.025 [main] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.026 [main] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.026 [main] INFO AppInfoParser - Kafka startTimeMs: 1718272036025
09:47:16.027 [main] INFO KafkaConsumer - [Consumer clientId=consumer-sub-000--4kasEA-1, groupId=sub-000--4kasEA] Subscribed to topic(s): test-topic-0000000-8Eex934
09:47:16.031 [main] INFO LocalWorker - Created 1 consumers in 52.309099 ms
09:47:16.042 [pool-10-thread-1] INFO Metadata - [Consumer clientId=consumer-sub-000--4kasEA-1, groupId=sub-000--4kasEA] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.043 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000--4kasEA-1, groupId=sub-000--4kasEA] Discovered group coordinator 10.0.0.122:9092 (id: 2147483645 rack: null)
09:47:16.044 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000--4kasEA-1, groupId=sub-000--4kasEA] (Re-)joining group
09:47:16.050 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.052 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-1] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.059 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000--4kasEA-1, groupId=sub-000--4kasEA] Request joining group due to: need to re-join with the given member-id: consumer-sub-000--4kasEA-1-bed19782-6c84-4272-b8f5-e9f98fd3c33a
09:47:16.060 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000--4kasEA-1, groupId=sub-000--4kasEA] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
09:47:16.060 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000--4kasEA-1, groupId=sub-000--4kasEA] (Re-)joining group
09:47:16.060 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
09:47:16.071 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.071 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.071 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.071 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036071
09:47:16.072 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.073 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-2] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.073 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-2] Instantiated an idempotent producer.
09:47:16.075 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.075 [kafka-producer-network-thread | producer-1] INFO Metadata - [Producer clientId=producer-1] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.075 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.075 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.075 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036075
09:47:16.076 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.076 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-3] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.076 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-3] Instantiated an idempotent producer.
09:47:16.078 [kafka-producer-network-thread | producer-2] INFO Metadata - [Producer clientId=producer-2] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.078 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.078 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.078 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.078 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036078
09:47:16.079 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.079 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-4] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.079 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-4] Instantiated an idempotent producer.
09:47:16.081 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.082 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.082 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.082 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036082
09:47:16.082 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.083 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-5] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.083 [kafka-producer-network-thread | producer-3] INFO Metadata - [Producer clientId=producer-3] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.083 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-5] Instantiated an idempotent producer.
09:47:16.083 [kafka-producer-network-thread | producer-3] INFO TransactionManager - [Producer clientId=producer-3] ProducerId set to 0 with epoch 0
09:47:16.084 [kafka-producer-network-thread | producer-4] INFO Metadata - [Producer clientId=producer-4] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.085 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.086 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.086 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.086 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036085
09:47:16.086 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.087 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-6] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.087 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-6] Instantiated an idempotent producer.
09:47:16.088 [kafka-producer-network-thread | producer-5] INFO Metadata - [Producer clientId=producer-5] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.088 [kafka-producer-network-thread | producer-5] INFO TransactionManager - [Producer clientId=producer-5] ProducerId set to 1 with epoch 0
09:47:16.089 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.089 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.089 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.089 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036089
09:47:16.090 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.090 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-7] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.090 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-7] Instantiated an idempotent producer.
09:47:16.091 [kafka-producer-network-thread | producer-6] INFO Metadata - [Producer clientId=producer-6] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.092 [kafka-producer-network-thread | producer-6] INFO TransactionManager - [Producer clientId=producer-6] ProducerId set to 2000 with epoch 0
09:47:16.092 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.092 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.092 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.092 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036092
09:47:16.093 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.093 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-8] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.094 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-8] Instantiated an idempotent producer.
09:47:16.094 [kafka-producer-network-thread | producer-7] INFO Metadata - [Producer clientId=producer-7] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.095 [kafka-producer-network-thread | producer-7] INFO TransactionManager - [Producer clientId=producer-7] ProducerId set to 2001 with epoch 0
09:47:16.095 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.095 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.095 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.095 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036095
09:47:16.096 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.097 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-9] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.097 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-9] Instantiated an idempotent producer.
09:47:16.097 [kafka-producer-network-thread | producer-8] INFO Metadata - [Producer clientId=producer-8] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.098 [kafka-producer-network-thread | producer-8] INFO TransactionManager - [Producer clientId=producer-8] ProducerId set to 2002 with epoch 0
09:47:16.099 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.099 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.099 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.099 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036099
09:47:16.100 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.100 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-10] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.100 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-10] Instantiated an idempotent producer.
09:47:16.101 [kafka-producer-network-thread | producer-9] INFO Metadata - [Producer clientId=producer-9] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.101 [kafka-producer-network-thread | producer-9] INFO TransactionManager - [Producer clientId=producer-9] ProducerId set to 2003 with epoch 0
09:47:16.102 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.102 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.102 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.102 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036102
09:47:16.103 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.103 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-11] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.103 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-11] Instantiated an idempotent producer.
09:47:16.104 [kafka-producer-network-thread | producer-10] INFO Metadata - [Producer clientId=producer-10] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.104 [kafka-producer-network-thread | producer-10] INFO TransactionManager - [Producer clientId=producer-10] ProducerId set to 1004 with epoch 0
09:47:16.105 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.105 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.105 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.105 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036105
09:47:16.106 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.106 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-12] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.106 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-12] Instantiated an idempotent producer.
09:47:16.107 [kafka-producer-network-thread | producer-11] INFO Metadata - [Producer clientId=producer-11] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.108 [kafka-producer-network-thread | producer-11] INFO TransactionManager - [Producer clientId=producer-11] ProducerId set to 1005 with epoch 0
09:47:16.108 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.108 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.108 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.108 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036108
09:47:16.108 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-13
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.109 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-13] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.109 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-13] Instantiated an idempotent producer.
09:47:16.111 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.111 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.111 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.111 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036111
09:47:16.112 [kafka-producer-network-thread | producer-12] INFO Metadata - [Producer clientId=producer-12] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.112 [kafka-producer-network-thread | producer-12] INFO TransactionManager - [Producer clientId=producer-12] ProducerId set to 2005 with epoch 0
09:47:16.112 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-14
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.113 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-14] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.113 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-14] Instantiated an idempotent producer.
09:47:16.114 [kafka-producer-network-thread | producer-13] INFO Metadata - [Producer clientId=producer-13] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.115 [kafka-producer-network-thread | producer-13] INFO TransactionManager - [Producer clientId=producer-13] ProducerId set to 1006 with epoch 0
09:47:16.116 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.116 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.116 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.116 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036116
09:47:16.117 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-15
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.117 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-15] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.117 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-15] Instantiated an idempotent producer.
09:47:16.118 [kafka-producer-network-thread | producer-14] INFO Metadata - [Producer clientId=producer-14] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.118 [kafka-producer-network-thread | producer-14] INFO TransactionManager - [Producer clientId=producer-14] ProducerId set to 1007 with epoch 0
09:47:16.119 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.119 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.119 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.119 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036119
09:47:16.120 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-16
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.122 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-16] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.122 [kafka-producer-network-thread | producer-15] INFO Metadata - [Producer clientId=producer-15] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.122 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-16] Instantiated an idempotent producer.
09:47:16.122 [kafka-producer-network-thread | producer-15] INFO TransactionManager - [Producer clientId=producer-15] ProducerId set to 2007 with epoch 0
09:47:16.123 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.124 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.124 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.124 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036124
09:47:16.125 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-17
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.125 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-17] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.125 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-17] Instantiated an idempotent producer.
09:47:16.126 [kafka-producer-network-thread | producer-16] INFO Metadata - [Producer clientId=producer-16] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.126 [kafka-producer-network-thread | producer-16] INFO TransactionManager - [Producer clientId=producer-16] ProducerId set to 8 with epoch 0
09:47:16.127 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.128 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.128 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.128 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036127
09:47:16.128 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-18
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.129 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-18] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.129 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-18] Instantiated an idempotent producer.
09:47:16.130 [kafka-producer-network-thread | producer-17] INFO Metadata - [Producer clientId=producer-17] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.130 [kafka-producer-network-thread | producer-17] INFO TransactionManager - [Producer clientId=producer-17] ProducerId set to 2009 with epoch 0
09:47:16.132 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.132 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.132 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.132 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036132
09:47:16.132 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-19
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.133 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-19] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.133 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-19] Instantiated an idempotent producer.
09:47:16.134 [kafka-producer-network-thread | producer-18] INFO Metadata - [Producer clientId=producer-18] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.134 [kafka-producer-network-thread | producer-18] INFO TransactionManager - [Producer clientId=producer-18] ProducerId set to 9 with epoch 0
09:47:16.135 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.135 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.135 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.135 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036135
09:47:16.135 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-20
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.136 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-20] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.136 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-20] Instantiated an idempotent producer.
09:47:16.137 [kafka-producer-network-thread | producer-19] INFO Metadata - [Producer clientId=producer-19] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.137 [kafka-producer-network-thread | producer-19] INFO TransactionManager - [Producer clientId=producer-19] ProducerId set to 1012 with epoch 0
09:47:16.138 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.138 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.138 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.138 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036138
09:47:16.138 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-21
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.139 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-21] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.139 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-21] Instantiated an idempotent producer.
09:47:16.140 [kafka-producer-network-thread | producer-20] INFO Metadata - [Producer clientId=producer-20] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.140 [kafka-producer-network-thread | producer-20] INFO TransactionManager - [Producer clientId=producer-20] ProducerId set to 2012 with epoch 0
09:47:16.141 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.142 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.142 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.142 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036142
09:47:16.142 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-22
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.143 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-22] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.143 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-22] Instantiated an idempotent producer.
09:47:16.145 [kafka-producer-network-thread | producer-21] INFO Metadata - [Producer clientId=producer-21] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.145 [kafka-producer-network-thread | producer-21] INFO TransactionManager - [Producer clientId=producer-21] ProducerId set to 11 with epoch 0
09:47:16.150 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.151 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.151 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.151 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036151
09:47:16.152 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-23
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.152 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-23] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.153 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-23] Instantiated an idempotent producer.
09:47:16.153 [kafka-producer-network-thread | producer-22] INFO Metadata - [Producer clientId=producer-22] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.153 [kafka-producer-network-thread | producer-22] INFO TransactionManager - [Producer clientId=producer-22] ProducerId set to 12 with epoch 0
09:47:16.158 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.159 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.159 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.159 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036159
09:47:16.160 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-24
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.163 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-24] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.163 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-24] Instantiated an idempotent producer.
09:47:16.164 [kafka-producer-network-thread | producer-23] INFO Metadata - [Producer clientId=producer-23] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.165 [kafka-producer-network-thread | producer-23] INFO TransactionManager - [Producer clientId=producer-23] ProducerId set to 14 with epoch 0
09:47:16.165 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.165 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.165 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.165 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036165
09:47:16.166 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-25
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.167 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-25] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.167 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-25] Instantiated an idempotent producer.
09:47:16.168 [kafka-producer-network-thread | producer-24] INFO Metadata - [Producer clientId=producer-24] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.168 [kafka-producer-network-thread | producer-24] INFO TransactionManager - [Producer clientId=producer-24] ProducerId set to 15 with epoch 0
09:47:16.170 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.170 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.170 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.170 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036170
09:47:16.171 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-26
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.172 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-26] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.172 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-26] Instantiated an idempotent producer.
09:47:16.172 [kafka-producer-network-thread | producer-25] INFO Metadata - [Producer clientId=producer-25] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.173 [kafka-producer-network-thread | producer-25] INFO TransactionManager - [Producer clientId=producer-25] ProducerId set to 16 with epoch 0
09:47:16.174 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.174 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.174 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.175 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036174
09:47:16.177 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-27
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.178 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-27] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.178 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-27] Instantiated an idempotent producer.
09:47:16.179 [kafka-producer-network-thread | producer-26] INFO Metadata - [Producer clientId=producer-26] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.180 [kafka-producer-network-thread | producer-26] INFO TransactionManager - [Producer clientId=producer-26] ProducerId set to 2023 with epoch 0
09:47:16.180 [kafka-producer-network-thread | producer-1] INFO TransactionManager - [Producer clientId=producer-1] ProducerId set to 1018 with epoch 0
09:47:16.180 [kafka-producer-network-thread | producer-2] INFO TransactionManager - [Producer clientId=producer-2] ProducerId set to 2025 with epoch 0
09:47:16.186 [kafka-producer-network-thread | producer-4] INFO TransactionManager - [Producer clientId=producer-4] ProducerId set to 1022 with epoch 0
09:47:16.187 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.187 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.187 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.187 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036187
09:47:16.188 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-28
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.188 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-28] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.188 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-28] Instantiated an idempotent producer.
09:47:16.190 [kafka-producer-network-thread | producer-27] INFO Metadata - [Producer clientId=producer-27] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.190 [kafka-producer-network-thread | producer-27] INFO TransactionManager - [Producer clientId=producer-27] ProducerId set to 2027 with epoch 0
09:47:16.191 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.191 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.191 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.191 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036191
09:47:16.192 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-29
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.193 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-29] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.193 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-29] Instantiated an idempotent producer.
09:47:16.193 [kafka-producer-network-thread | producer-28] INFO Metadata - [Producer clientId=producer-28] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.194 [kafka-producer-network-thread | producer-28] INFO TransactionManager - [Producer clientId=producer-28] ProducerId set to 21 with epoch 0
09:47:16.195 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.195 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.195 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.195 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036195
09:47:16.196 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-30
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.197 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-30] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.197 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-30] Instantiated an idempotent producer.
09:47:16.199 [kafka-producer-network-thread | producer-29] INFO Metadata - [Producer clientId=producer-29] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.200 [kafka-producer-network-thread | producer-29] INFO TransactionManager - [Producer clientId=producer-29] ProducerId set to 2031 with epoch 0
09:47:16.201 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.201 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.201 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.202 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036201
09:47:16.202 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-31
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.203 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-31] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.203 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-31] Instantiated an idempotent producer.
09:47:16.204 [kafka-producer-network-thread | producer-30] INFO Metadata - [Producer clientId=producer-30] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.204 [kafka-producer-network-thread | producer-30] INFO TransactionManager - [Producer clientId=producer-30] ProducerId set to 1025 with epoch 0
09:47:16.205 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.206 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.206 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.206 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036205
09:47:16.206 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-32
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.207 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-32] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.207 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-32] Instantiated an idempotent producer.
09:47:16.209 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.209 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.209 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.211 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036209
09:47:16.212 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-33
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.212 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-33] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.212 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-33] Instantiated an idempotent producer.
09:47:16.212 [kafka-producer-network-thread | producer-31] INFO Metadata - [Producer clientId=producer-31] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.213 [kafka-producer-network-thread | producer-31] INFO TransactionManager - [Producer clientId=producer-31] ProducerId set to 2033 with epoch 0
09:47:16.215 [kafka-producer-network-thread | producer-32] INFO Metadata - [Producer clientId=producer-32] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.215 [kafka-producer-network-thread | producer-32] INFO TransactionManager - [Producer clientId=producer-32] ProducerId set to 2035 with epoch 0
09:47:16.216 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.217 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.217 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.217 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036217
09:47:16.218 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-34
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.218 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-34] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.219 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-34] Instantiated an idempotent producer.
09:47:16.219 [kafka-producer-network-thread | producer-33] INFO Metadata - [Producer clientId=producer-33] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.219 [kafka-producer-network-thread | producer-33] INFO TransactionManager - [Producer clientId=producer-33] ProducerId set to 2037 with epoch 0
09:47:16.221 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.221 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.222 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.222 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036221
09:47:16.222 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-35
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.223 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-35] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.223 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-35] Instantiated an idempotent producer.
09:47:16.224 [kafka-producer-network-thread | producer-34] INFO Metadata - [Producer clientId=producer-34] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.224 [kafka-producer-network-thread | producer-34] INFO TransactionManager - [Producer clientId=producer-34] ProducerId set to 27 with epoch 0
09:47:16.228 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.228 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.228 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.228 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036228
09:47:16.229 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-36
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.230 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-36] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.230 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-36] Instantiated an idempotent producer.
09:47:16.232 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.232 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.232 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.232 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036232
09:47:16.233 [kafka-producer-network-thread | producer-35] INFO Metadata - [Producer clientId=producer-35] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.233 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-37
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.233 [kafka-producer-network-thread | producer-35] INFO TransactionManager - [Producer clientId=producer-35] ProducerId set to 1027 with epoch 0
09:47:16.234 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-37] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.234 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-37] Instantiated an idempotent producer.
09:47:16.235 [kafka-producer-network-thread | producer-36] INFO Metadata - [Producer clientId=producer-36] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.235 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.235 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.235 [kafka-producer-network-thread | producer-36] INFO TransactionManager - [Producer clientId=producer-36] ProducerId set to 29 with epoch 0
09:47:16.235 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.235 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036235
09:47:16.236 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-38
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.236 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-38] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.236 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-38] Instantiated an idempotent producer.
09:47:16.238 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.238 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.239 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.239 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036238
09:47:16.239 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-39
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.240 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-39] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.240 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-39] Instantiated an idempotent producer.
09:47:16.241 [kafka-producer-network-thread | producer-38] INFO Metadata - [Producer clientId=producer-38] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.241 [kafka-producer-network-thread | producer-38] INFO TransactionManager - [Producer clientId=producer-38] ProducerId set to 1028 with epoch 0
09:47:16.241 [kafka-producer-network-thread | producer-37] INFO Metadata - [Producer clientId=producer-37] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.242 [kafka-producer-network-thread | producer-37] INFO TransactionManager - [Producer clientId=producer-37] ProducerId set to 31 with epoch 0
09:47:16.242 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.242 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.242 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.242 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036242
09:47:16.244 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-40
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.244 [kafka-producer-network-thread | producer-39] INFO Metadata - [Producer clientId=producer-39] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.244 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-40] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.244 [kafka-producer-network-thread | producer-39] INFO TransactionManager - [Producer clientId=producer-39] ProducerId set to 1030 with epoch 0
09:47:16.244 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-40] Instantiated an idempotent producer.
09:47:16.247 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.247 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.247 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.247 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036247
09:47:16.248 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-41
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.248 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-41] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.248 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-41] Instantiated an idempotent producer.
09:47:16.251 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.251 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.251 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.252 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036251
09:47:16.252 [kafka-producer-network-thread | producer-40] INFO Metadata - [Producer clientId=producer-40] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.252 [kafka-producer-network-thread | producer-40] INFO TransactionManager - [Producer clientId=producer-40] ProducerId set to 32 with epoch 0
09:47:16.252 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-42
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.253 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-42] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.253 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-42] Instantiated an idempotent producer.
09:47:16.256 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.257 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.257 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.257 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036257
09:47:16.258 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-43
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.258 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-43] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.258 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-43] Instantiated an idempotent producer.
09:47:16.258 [kafka-producer-network-thread | producer-42] INFO Metadata - [Producer clientId=producer-42] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.259 [kafka-producer-network-thread | producer-42] INFO TransactionManager - [Producer clientId=producer-42] ProducerId set to 36 with epoch 0
09:47:16.262 [kafka-producer-network-thread | producer-41] INFO Metadata - [Producer clientId=producer-41] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.263 [kafka-producer-network-thread | producer-41] INFO TransactionManager - [Producer clientId=producer-41] ProducerId set to 34 with epoch 0
09:47:16.263 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.263 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.263 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.264 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036263
09:47:16.264 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-44
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.265 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-44] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.265 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-44] Instantiated an idempotent producer.
09:47:16.266 [kafka-producer-network-thread | producer-43] INFO Metadata - [Producer clientId=producer-43] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.266 [kafka-producer-network-thread | producer-43] INFO TransactionManager - [Producer clientId=producer-43] ProducerId set to 1033 with epoch 0
09:47:16.268 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.268 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.268 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.268 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036268
09:47:16.269 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-45
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.269 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-45] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.269 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-45] Instantiated an idempotent producer.
09:47:16.271 [kafka-producer-network-thread | producer-44] INFO Metadata - [Producer clientId=producer-44] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.271 [kafka-producer-network-thread | producer-44] INFO TransactionManager - [Producer clientId=producer-44] ProducerId set to 2048 with epoch 0
09:47:16.272 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.272 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.272 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.272 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036272
09:47:16.273 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-46
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.273 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-46] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.273 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-46] Instantiated an idempotent producer.
09:47:16.274 [kafka-producer-network-thread | producer-45] INFO Metadata - [Producer clientId=producer-45] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.275 [kafka-producer-network-thread | producer-45] INFO TransactionManager - [Producer clientId=producer-45] ProducerId set to 40 with epoch 0
09:47:16.275 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.276 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.276 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.276 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036276
09:47:16.276 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-47
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.277 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-47] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.277 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-47] Instantiated an idempotent producer.
09:47:16.280 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.280 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.280 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.280 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036280
09:47:16.281 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-48
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.281 [kafka-producer-network-thread | producer-46] INFO Metadata - [Producer clientId=producer-46] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.282 [kafka-producer-network-thread | producer-46] INFO TransactionManager - [Producer clientId=producer-46] ProducerId set to 1035 with epoch 0
09:47:16.282 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-48] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.282 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-48] Instantiated an idempotent producer.
09:47:16.283 [kafka-producer-network-thread | producer-47] INFO Metadata - [Producer clientId=producer-47] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.283 [kafka-producer-network-thread | producer-47] INFO TransactionManager - [Producer clientId=producer-47] ProducerId set to 2050 with epoch 0
09:47:16.284 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.284 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.284 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.284 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036284
09:47:16.285 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-49
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.285 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-49] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.285 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-49] Instantiated an idempotent producer.
09:47:16.287 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.287 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.287 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.288 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036287
09:47:16.288 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-50
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.289 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-50] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.289 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-50] Instantiated an idempotent producer.
09:47:16.290 [kafka-producer-network-thread | producer-48] INFO Metadata - [Producer clientId=producer-48] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.291 [kafka-producer-network-thread | producer-48] INFO TransactionManager - [Producer clientId=producer-48] ProducerId set to 45 with epoch 0
09:47:16.292 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.292 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.292 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.292 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036292
09:47:16.293 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-51
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.293 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-51] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.294 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-51] Instantiated an idempotent producer.
09:47:16.296 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.296 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.296 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.296 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036296
09:47:16.296 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-52
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.297 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-52] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.297 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-52] Instantiated an idempotent producer.
09:47:16.301 [kafka-producer-network-thread | producer-50] INFO Metadata - [Producer clientId=producer-50] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.301 [kafka-producer-network-thread | producer-51] INFO Metadata - [Producer clientId=producer-51] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.302 [kafka-producer-network-thread | producer-50] INFO TransactionManager - [Producer clientId=producer-50] ProducerId set to 1037 with epoch 0
09:47:16.302 [kafka-producer-network-thread | producer-51] INFO TransactionManager - [Producer clientId=producer-51] ProducerId set to 1038 with epoch 0
09:47:16.302 [kafka-producer-network-thread | producer-49] INFO Metadata - [Producer clientId=producer-49] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.302 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.302 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.302 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.302 [kafka-producer-network-thread | producer-49] INFO TransactionManager - [Producer clientId=producer-49] ProducerId set to 47 with epoch 0
09:47:16.302 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036302
09:47:16.303 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-53
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.304 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-53] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.304 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-53] Instantiated an idempotent producer.
09:47:16.305 [kafka-producer-network-thread | producer-52] INFO Metadata - [Producer clientId=producer-52] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.305 [kafka-producer-network-thread | producer-52] INFO TransactionManager - [Producer clientId=producer-52] ProducerId set to 1042 with epoch 0
09:47:16.308 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.308 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.308 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.308 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036308
09:47:16.308 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-54
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.309 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-54] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.309 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-54] Instantiated an idempotent producer.
09:47:16.310 [kafka-producer-network-thread | producer-53] INFO Metadata - [Producer clientId=producer-53] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.310 [kafka-producer-network-thread | producer-53] INFO TransactionManager - [Producer clientId=producer-53] ProducerId set to 1044 with epoch 0
09:47:16.311 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.311 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.312 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.312 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036311
09:47:16.312 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-55
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.313 [kafka-producer-network-thread | producer-54] INFO Metadata - [Producer clientId=producer-54] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.314 [kafka-producer-network-thread | producer-54] INFO TransactionManager - [Producer clientId=producer-54] ProducerId set to 48 with epoch 0
09:47:16.316 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-55] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.316 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-55] Instantiated an idempotent producer.
09:47:16.319 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.319 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.319 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.319 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036319
09:47:16.319 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-56
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.320 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-56] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.320 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-56] Instantiated an idempotent producer.
09:47:16.321 [kafka-producer-network-thread | producer-55] INFO Metadata - [Producer clientId=producer-55] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.322 [kafka-producer-network-thread | producer-55] INFO TransactionManager - [Producer clientId=producer-55] ProducerId set to 1047 with epoch 0
09:47:16.322 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.322 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.322 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.322 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036322
09:47:16.323 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-57
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.323 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-57] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.324 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-57] Instantiated an idempotent producer.
09:47:16.328 [kafka-producer-network-thread | producer-56] INFO Metadata - [Producer clientId=producer-56] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.330 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.330 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.330 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.331 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036330
09:47:16.331 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-58
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.332 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-58] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.332 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-58] Instantiated an idempotent producer.
09:47:16.333 [kafka-producer-network-thread | producer-57] INFO Metadata - [Producer clientId=producer-57] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.334 [kafka-producer-network-thread | producer-57] INFO TransactionManager - [Producer clientId=producer-57] ProducerId set to 1048 with epoch 0
09:47:16.334 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.334 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.334 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.334 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036334
09:47:16.335 [qtp641030345-29] INFO ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 1048576
	bootstrap.servers = [10.0.0.40:9092, 10.0.0.162:9092, 10.0.0.122:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-59
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 1200000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:47:16.336 [qtp641030345-29] WARN KafkaProducer - [Producer clientId=producer-59] delivery.timeout.ms should be equal to or larger than linger.ms + request.timeout.ms. Setting it to 1200001.
09:47:16.336 [qtp641030345-29] INFO KafkaProducer - [Producer clientId=producer-59] Instantiated an idempotent producer.
09:47:16.337 [kafka-producer-network-thread | producer-58] INFO Metadata - [Producer clientId=producer-58] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.337 [kafka-producer-network-thread | producer-58] INFO TransactionManager - [Producer clientId=producer-58] ProducerId set to 51 with epoch 0
09:47:16.338 [qtp641030345-29] INFO ProducerConfig - These configurations '[default.api.timeout.ms]' were supplied but are not used yet.
09:47:16.338 [qtp641030345-29] INFO AppInfoParser - Kafka version: 3.6.1
09:47:16.338 [qtp641030345-29] INFO AppInfoParser - Kafka commitId: 5e3c2b738d253ff5
09:47:16.338 [qtp641030345-29] INFO AppInfoParser - Kafka startTimeMs: 1718272036338
09:47:16.340 [kafka-producer-network-thread | producer-56] INFO TransactionManager - [Producer clientId=producer-56] ProducerId set to 2062 with epoch 0
09:47:16.340 [kafka-producer-network-thread | producer-59] INFO Metadata - [Producer clientId=producer-59] Cluster ID: 4UY7e71FRwqvOzxdVZaQIA
09:47:16.341 [kafka-producer-network-thread | producer-59] INFO TransactionManager - [Producer clientId=producer-59] ProducerId set to 1050 with epoch 0
09:47:16.341 [qtp641030345-29] INFO LocalWorker - Created 59 producers in 301.807275 ms from {
  "topics" : [ "test-topic-0000000-8Eex934", "test-topic-0000001-a9CcqOE", "test-topic-0000002-znnJLzw", "test-topic-0000003-DnBEPIs", "test-topic-0000004-usTouY0", "test-topic-0000005-zjXqMDM", "test-topic-0000006-VvQACQk", "test-topic-0000007-yYH8VWg", "test-topic-0000008-Sxnin3E", "test-topic-0000009-LUIe8yI", "test-topic-0000010-bCuPDwQ", "test-topic-0000011--9uyhDs", "test-topic-0000012-Gyo43RA", "test-topic-0000013-Dlw5Sh4", "test-topic-0000014-614Lp8s", "test-topic-0000015-XDQ72r0", "test-topic-0000016-hqYF7mM", "test-topic-0000017-egkTscg", "test-topic-0000018-0eMDS1I", "test-topic-0000019-ZVQUIRs", "test-topic-0000020-TQHW9Pc", "test-topic-0000021-ftzk43c", "test-topic-0000022-wCQ2bmI", "test-topic-0000023-1k1lynQ", "test-topic-0000024-E_mM628", "test-topic-0000025-rG-WKrY", "test-topic-0000026-fjKWmxU", "test-topic-0000027-M7-89V8", "test-topic-0000028-4bCG-f8", "test-topic-0000029-l3LcHMQ", "test-topic-0000030-deSHzVw", "test-topic-0000031-QijEomI", "test-topic-0000032-o1hyh_M", "test-topic-0000033-WsZ8gRI", "test-topic-0000034-M4nrWXk", "test-topic-0000035-jk5ZMXw", "test-topic-0000036-qWWpdyY", "test-topic-0000037-EqpJAsk", "test-topic-0000038-6ybgQ4U", "test-topic-0000039-57G_sJw", "test-topic-0000040-J1sFQIs", "test-topic-0000041-X_elTkI", "test-topic-0000042-80tdPus", "test-topic-0000043-4BDcIic", "test-topic-0000044-tSJ2gKg", "test-topic-0000045-6G6k7os", "test-topic-0000046-RG8GLes", "test-topic-0000047-UP2rxHY", "test-topic-0000048-z_TaFxY", "test-topic-0000049-1BQUeek", "test-topic-0000050-_eDZehg", "test-topic-0000051-0oBroVU", "test-topic-0000052-VtHfpgI", "test-topic-0000053-UZ0LF1A" ],
  "producerIndex" : 0,
  "isTpcH" : true
}
09:47:16.393 [main] INFO WorkloadGenerator - Created 54 producers in 362.000498 ms
09:47:16.393 [main] INFO WorkloadGenerator - Waiting for consumers to be ready...
09:47:16.654 [main] INFO WorkloadGenerator - Waiting for topics to be ready -- Sent: 140, Received: 0, Expected: 0
09:47:16.654 [main] INFO WorkloadGenerator - All consumers are ready!
09:47:16.655 [main] INFO WorkloadGenerator - [BenchmarkStart] Starting benchmark Kafka-tpc-h-tpc-h-q6-100-50-2024-06-13-09-47-14 at 1718272036655
09:47:16.682 [qtp641030345-28] INFO WorkerHandler - Start load publish-rate: 3333333.3333333335 msg/s -- payload-size: 0 -- producer index: 0
09:47:16.683 [qtp641030345-28] INFO AdaptiveRateLimitedTaskProcessor - Initialising with 2048 max concurrent tasks
09:47:16.684 [qtp641030345-28] INFO LocalWorker - Number of commands 34 | Commands per batch 2 | Batches per producer 17
09:47:16.688 [tpc-h-worker-2-8] INFO LocalWorker - [TpcHBenchmark] No work for TPC-H producer 0-7. Shutting down.
09:47:16.688 [tpc-h-worker-2-7] INFO LocalWorker - [TpcHBenchmark] No work for TPC-H producer 0-6. Shutting down.
09:47:16.721 [tpc-h-worker-2-1] INFO LocalWorker - Number of batches 3 | Start 0 | Max 34
09:47:16.721 [tpc-h-worker-2-3] INFO LocalWorker - Number of batches 3 | Start 0 | Max 34
09:47:16.721 [tpc-h-worker-2-6] INFO LocalWorker - Number of batches 2 | Start 0 | Max 34
09:47:16.721 [tpc-h-worker-2-4] INFO LocalWorker - Number of batches 3 | Start 0 | Max 34
09:47:16.721 [tpc-h-worker-2-2] INFO LocalWorker - Number of batches 3 | Start 0 | Max 34
09:47:16.721 [tpc-h-worker-2-5] INFO LocalWorker - Number of batches 3 | Start 0 | Max 34
09:47:16.736 [tpc-h-worker-2-5] INFO LocalWorker - [TpcHBenchmark] Launched 6 completable futures. Awaiting...
09:47:16.739 [tpc-h-worker-2-6] INFO LocalWorker - [TpcHBenchmark] Launched 4 completable futures. Awaiting...
09:47:16.739 [tpc-h-worker-2-4] INFO LocalWorker - [TpcHBenchmark] Launched 6 completable futures. Awaiting...
09:47:16.741 [tpc-h-worker-2-1] INFO LocalWorker - [TpcHBenchmark] Launched 6 completable futures. Awaiting...
09:47:16.742 [tpc-h-worker-2-2] INFO LocalWorker - [TpcHBenchmark] Launched 6 completable futures. Awaiting...
09:47:16.742 [tpc-h-worker-2-3] INFO LocalWorker - [TpcHBenchmark] Launched 6 completable futures. Awaiting...
09:47:16.744 [tpc-h-worker-2-5] INFO LocalWorker - [TpcHBenchmark] Finished TPC-H producer 0-4 after sending 6 messages. Shutting down.
09:47:16.745 [tpc-h-worker-2-1] INFO LocalWorker - [TpcHBenchmark] Finished TPC-H producer 0-0 after sending 6 messages. Shutting down.
09:47:16.746 [tpc-h-worker-2-2] INFO LocalWorker - [TpcHBenchmark] Finished TPC-H producer 0-1 after sending 6 messages. Shutting down.
09:47:16.746 [tpc-h-worker-2-6] INFO LocalWorker - [TpcHBenchmark] Finished TPC-H producer 0-5 after sending 4 messages. Shutting down.
09:47:16.747 [tpc-h-worker-2-4] INFO LocalWorker - [TpcHBenchmark] Finished TPC-H producer 0-3 after sending 6 messages. Shutting down.
09:47:16.748 [tpc-h-worker-2-3] INFO LocalWorker - [TpcHBenchmark] Finished TPC-H producer 0-2 after sending 6 messages. Shutting down.
09:47:18.741 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-tgno4IA-13, groupId=sub-037-tgno4IA] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-037-tgno4IA-13-4d2e7b70-c76d-4330-a61e-5b5640e01ed4', protocol='range'}
09:47:18.743 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-Czrc7Rg-17, groupId=sub-049-Czrc7Rg] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-049-Czrc7Rg-17-eafefb24-f3d2-4dbb-823d-1d358b921243', protocol='range'}
09:47:18.748 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-Czrc7Rg-17, groupId=sub-049-Czrc7Rg] Finished assignment for group at generation 1: {consumer-sub-049-Czrc7Rg-17-eafefb24-f3d2-4dbb-823d-1d358b921243=Assignment(partitions=[test-topic-0000049-1BQUeek-0])}
09:47:18.748 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-tgno4IA-13, groupId=sub-037-tgno4IA] Finished assignment for group at generation 1: {consumer-sub-037-tgno4IA-13-4d2e7b70-c76d-4330-a61e-5b5640e01ed4=Assignment(partitions=[test-topic-0000037-EqpJAsk-0])}
09:47:18.757 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-QKzaf_k-18, groupId=sub-052-QKzaf_k] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-052-QKzaf_k-18-3568a76e-3be2-4b40-b7c9-8084867d2b6a', protocol='range'}
09:47:18.757 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-QKzaf_k-18, groupId=sub-052-QKzaf_k] Finished assignment for group at generation 1: {consumer-sub-052-QKzaf_k-18-3568a76e-3be2-4b40-b7c9-8084867d2b6a=Assignment(partitions=[test-topic-0000052-VtHfpgI-0])}
09:47:18.759 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-1Ocz0_o-4, groupId=sub-010-1Ocz0_o] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-010-1Ocz0_o-4-fb64c78f-6f18-417f-8be7-87e0976f79a1', protocol='range'}
09:47:18.759 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-1Ocz0_o-4, groupId=sub-010-1Ocz0_o] Finished assignment for group at generation 1: {consumer-sub-010-1Ocz0_o-4-fb64c78f-6f18-417f-8be7-87e0976f79a1=Assignment(partitions=[test-topic-0000010-bCuPDwQ-0])}
09:47:18.764 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-xN56quE-2, groupId=sub-004-xN56quE] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-004-xN56quE-2-211a7a74-b915-4f6e-bff4-3eda46bba633', protocol='range'}
09:47:18.765 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-xN56quE-2, groupId=sub-004-xN56quE] Finished assignment for group at generation 1: {consumer-sub-004-xN56quE-2-211a7a74-b915-4f6e-bff4-3eda46bba633=Assignment(partitions=[test-topic-0000004-usTouY0-0])}
09:47:18.766 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-dAdh6Jk-3, groupId=sub-007-dAdh6Jk] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-007-dAdh6Jk-3-e1b4b353-aa7c-4b92-8048-454c318451ee', protocol='range'}
09:47:18.766 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-dAdh6Jk-3, groupId=sub-007-dAdh6Jk] Finished assignment for group at generation 1: {consumer-sub-007-dAdh6Jk-3-e1b4b353-aa7c-4b92-8048-454c318451ee=Assignment(partitions=[test-topic-0000007-yYH8VWg-0])}
09:47:18.767 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-cUISDQs-5, groupId=sub-013-cUISDQs] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-013-cUISDQs-5-9468fb3f-4b40-4b3b-9a5b-b3978f2c5476', protocol='range'}
09:47:18.768 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-cUISDQs-5, groupId=sub-013-cUISDQs] Finished assignment for group at generation 1: {consumer-sub-013-cUISDQs-5-9468fb3f-4b40-4b3b-9a5b-b3978f2c5476=Assignment(partitions=[test-topic-0000013-Dlw5Sh4-0])}
09:47:18.771 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-BHkZggo-6, groupId=sub-016-BHkZggo] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-016-BHkZggo-6-06336074-abee-4362-97db-1d0a16e21a1f', protocol='range'}
09:47:18.771 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-BHkZggo-6, groupId=sub-016-BHkZggo] Finished assignment for group at generation 1: {consumer-sub-016-BHkZggo-6-06336074-abee-4362-97db-1d0a16e21a1f=Assignment(partitions=[test-topic-0000016-hqYF7mM-0])}
09:47:18.777 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-Czrc7Rg-17, groupId=sub-049-Czrc7Rg] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-049-Czrc7Rg-17-eafefb24-f3d2-4dbb-823d-1d358b921243', protocol='range'}
09:47:18.777 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-tgno4IA-13, groupId=sub-037-tgno4IA] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-037-tgno4IA-13-4d2e7b70-c76d-4330-a61e-5b5640e01ed4', protocol='range'}
09:47:18.777 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-tgno4IA-13, groupId=sub-037-tgno4IA] Notifying assignor about the new Assignment(partitions=[test-topic-0000037-EqpJAsk-0])
09:47:18.777 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-Czrc7Rg-17, groupId=sub-049-Czrc7Rg] Notifying assignor about the new Assignment(partitions=[test-topic-0000049-1BQUeek-0])
09:47:18.778 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-BHkZggo-6, groupId=sub-016-BHkZggo] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-016-BHkZggo-6-06336074-abee-4362-97db-1d0a16e21a1f', protocol='range'}
09:47:18.778 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-1Ocz0_o-4, groupId=sub-010-1Ocz0_o] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-010-1Ocz0_o-4-fb64c78f-6f18-417f-8be7-87e0976f79a1', protocol='range'}
09:47:18.778 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-BHkZggo-6, groupId=sub-016-BHkZggo] Notifying assignor about the new Assignment(partitions=[test-topic-0000016-hqYF7mM-0])
09:47:18.778 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-1Ocz0_o-4, groupId=sub-010-1Ocz0_o] Notifying assignor about the new Assignment(partitions=[test-topic-0000010-bCuPDwQ-0])
09:47:18.778 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-QKzaf_k-18, groupId=sub-052-QKzaf_k] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-052-QKzaf_k-18-3568a76e-3be2-4b40-b7c9-8084867d2b6a', protocol='range'}
09:47:18.778 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-QKzaf_k-18, groupId=sub-052-QKzaf_k] Notifying assignor about the new Assignment(partitions=[test-topic-0000052-VtHfpgI-0])
09:47:18.778 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-TVYlklQ-7, groupId=sub-019-TVYlklQ] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-019-TVYlklQ-7-ee33af2b-2679-453e-ba51-e581d995cf64', protocol='range'}
09:47:18.779 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-TVYlklQ-7, groupId=sub-019-TVYlklQ] Finished assignment for group at generation 1: {consumer-sub-019-TVYlklQ-7-ee33af2b-2679-453e-ba51-e581d995cf64=Assignment(partitions=[test-topic-0000019-ZVQUIRs-0])}
09:47:18.780 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-1Ocz0_o-4, groupId=sub-010-1Ocz0_o] Adding newly assigned partitions: test-topic-0000010-bCuPDwQ-0
09:47:18.780 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-BHkZggo-6, groupId=sub-016-BHkZggo] Adding newly assigned partitions: test-topic-0000016-hqYF7mM-0
09:47:18.780 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-tgno4IA-13, groupId=sub-037-tgno4IA] Adding newly assigned partitions: test-topic-0000037-EqpJAsk-0
09:47:18.780 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-Czrc7Rg-17, groupId=sub-049-Czrc7Rg] Adding newly assigned partitions: test-topic-0000049-1BQUeek-0
09:47:18.780 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-QKzaf_k-18, groupId=sub-052-QKzaf_k] Adding newly assigned partitions: test-topic-0000052-VtHfpgI-0
09:47:18.782 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-TVYlklQ-7, groupId=sub-019-TVYlklQ] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-019-TVYlklQ-7-ee33af2b-2679-453e-ba51-e581d995cf64', protocol='range'}
09:47:18.782 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-TVYlklQ-7, groupId=sub-019-TVYlklQ] Notifying assignor about the new Assignment(partitions=[test-topic-0000019-ZVQUIRs-0])
09:47:18.782 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-TVYlklQ-7, groupId=sub-019-TVYlklQ] Adding newly assigned partitions: test-topic-0000019-ZVQUIRs-0
09:47:18.786 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-dAdh6Jk-3, groupId=sub-007-dAdh6Jk] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-007-dAdh6Jk-3-e1b4b353-aa7c-4b92-8048-454c318451ee', protocol='range'}
09:47:18.786 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-xN56quE-2, groupId=sub-004-xN56quE] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-004-xN56quE-2-211a7a74-b915-4f6e-bff4-3eda46bba633', protocol='range'}
09:47:18.786 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-dAdh6Jk-3, groupId=sub-007-dAdh6Jk] Notifying assignor about the new Assignment(partitions=[test-topic-0000007-yYH8VWg-0])
09:47:18.786 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-xN56quE-2, groupId=sub-004-xN56quE] Notifying assignor about the new Assignment(partitions=[test-topic-0000004-usTouY0-0])
09:47:18.786 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-dAdh6Jk-3, groupId=sub-007-dAdh6Jk] Adding newly assigned partitions: test-topic-0000007-yYH8VWg-0
09:47:18.786 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-xN56quE-2, groupId=sub-004-xN56quE] Adding newly assigned partitions: test-topic-0000004-usTouY0-0
09:47:18.786 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-cUISDQs-5, groupId=sub-013-cUISDQs] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-013-cUISDQs-5-9468fb3f-4b40-4b3b-9a5b-b3978f2c5476', protocol='range'}
09:47:18.786 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-cUISDQs-5, groupId=sub-013-cUISDQs] Notifying assignor about the new Assignment(partitions=[test-topic-0000013-Dlw5Sh4-0])
09:47:18.786 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-cUISDQs-5, groupId=sub-013-cUISDQs] Adding newly assigned partitions: test-topic-0000013-Dlw5Sh4-0
09:47:18.787 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-1m8YQwM-8, groupId=sub-022-1m8YQwM] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-022-1m8YQwM-8-a502fac7-1a58-4c7a-aa80-ac5406f9b383', protocol='range'}
09:47:18.787 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-1m8YQwM-8, groupId=sub-022-1m8YQwM] Finished assignment for group at generation 1: {consumer-sub-022-1m8YQwM-8-a502fac7-1a58-4c7a-aa80-ac5406f9b383=Assignment(partitions=[test-topic-0000022-wCQ2bmI-0])}
09:47:18.793 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-1m8YQwM-8, groupId=sub-022-1m8YQwM] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-022-1m8YQwM-8-a502fac7-1a58-4c7a-aa80-ac5406f9b383', protocol='range'}
09:47:18.793 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-1m8YQwM-8, groupId=sub-022-1m8YQwM] Notifying assignor about the new Assignment(partitions=[test-topic-0000022-wCQ2bmI-0])
09:47:18.793 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-1m8YQwM-8, groupId=sub-022-1m8YQwM] Adding newly assigned partitions: test-topic-0000022-wCQ2bmI-0
09:47:18.794 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-2CjJ6bE-9, groupId=sub-025-2CjJ6bE] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-025-2CjJ6bE-9-4758eef4-f1f5-4a8d-907c-6be74e376be7', protocol='range'}
09:47:18.794 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-2CjJ6bE-9, groupId=sub-025-2CjJ6bE] Finished assignment for group at generation 1: {consumer-sub-025-2CjJ6bE-9-4758eef4-f1f5-4a8d-907c-6be74e376be7=Assignment(partitions=[test-topic-0000025-rG-WKrY-0])}
09:47:18.794 [pool-22-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-037-tgno4IA-13, groupId=sub-037-tgno4IA] Found no committed offset for partition test-topic-0000037-EqpJAsk-0
09:47:18.794 [pool-13-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-010-1Ocz0_o-4, groupId=sub-010-1Ocz0_o] Found no committed offset for partition test-topic-0000010-bCuPDwQ-0
09:47:18.794 [pool-26-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-049-Czrc7Rg-17, groupId=sub-049-Czrc7Rg] Found no committed offset for partition test-topic-0000049-1BQUeek-0
09:47:18.794 [pool-27-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-052-QKzaf_k-18, groupId=sub-052-QKzaf_k] Found no committed offset for partition test-topic-0000052-VtHfpgI-0
09:47:18.794 [pool-16-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-019-TVYlklQ-7, groupId=sub-019-TVYlklQ] Found no committed offset for partition test-topic-0000019-ZVQUIRs-0
09:47:18.795 [pool-15-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-016-BHkZggo-6, groupId=sub-016-BHkZggo] Found no committed offset for partition test-topic-0000016-hqYF7mM-0
09:47:18.796 [pool-14-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-013-cUISDQs-5, groupId=sub-013-cUISDQs] Found no committed offset for partition test-topic-0000013-Dlw5Sh4-0
09:47:18.796 [pool-12-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-007-dAdh6Jk-3, groupId=sub-007-dAdh6Jk] Found no committed offset for partition test-topic-0000007-yYH8VWg-0
09:47:18.796 [pool-11-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-004-xN56quE-2, groupId=sub-004-xN56quE] Found no committed offset for partition test-topic-0000004-usTouY0-0
09:47:18.797 [pool-17-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-022-1m8YQwM-8, groupId=sub-022-1m8YQwM] Found no committed offset for partition test-topic-0000022-wCQ2bmI-0
09:47:18.798 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-2CjJ6bE-9, groupId=sub-025-2CjJ6bE] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-025-2CjJ6bE-9-4758eef4-f1f5-4a8d-907c-6be74e376be7', protocol='range'}
09:47:18.798 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-2CjJ6bE-9, groupId=sub-025-2CjJ6bE] Notifying assignor about the new Assignment(partitions=[test-topic-0000025-rG-WKrY-0])
09:47:18.798 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-028-5tQIPxo-10-eadd295a-43bf-4a69-b9c8-36d822c30be5', protocol='range'}
09:47:18.798 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-2CjJ6bE-9, groupId=sub-025-2CjJ6bE] Adding newly assigned partitions: test-topic-0000025-rG-WKrY-0
09:47:18.799 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Finished assignment for group at generation 1: {consumer-sub-028-5tQIPxo-10-eadd295a-43bf-4a69-b9c8-36d822c30be5=Assignment(partitions=[test-topic-0000028-4bCG-f8-0])}
09:47:18.799 [pool-18-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-025-2CjJ6bE-9, groupId=sub-025-2CjJ6bE] Found no committed offset for partition test-topic-0000025-rG-WKrY-0
09:47:18.808 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-028-5tQIPxo-10-eadd295a-43bf-4a69-b9c8-36d822c30be5', protocol='range'}
09:47:18.808 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Notifying assignor about the new Assignment(partitions=[test-topic-0000028-4bCG-f8-0])
09:47:18.808 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Adding newly assigned partitions: test-topic-0000028-4bCG-f8-0
09:47:18.808 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-N0T6Bp4-11, groupId=sub-031-N0T6Bp4] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-031-N0T6Bp4-11-521e31cd-0cd1-462e-962c-3bd01ea05f83', protocol='range'}
09:47:18.808 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-N0T6Bp4-11, groupId=sub-031-N0T6Bp4] Finished assignment for group at generation 1: {consumer-sub-031-N0T6Bp4-11-521e31cd-0cd1-462e-962c-3bd01ea05f83=Assignment(partitions=[test-topic-0000031-QijEomI-0])}
09:47:18.809 [pool-19-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Found no committed offset for partition test-topic-0000028-4bCG-f8-0
09:47:18.809 [pool-13-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-010-1Ocz0_o-4, groupId=sub-010-1Ocz0_o] Resetting offset for partition test-topic-0000010-bCuPDwQ-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.162:9092 (id: 1 rack: null)], epoch=0}}.
09:47:18.809 [pool-22-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-037-tgno4IA-13, groupId=sub-037-tgno4IA] Resetting offset for partition test-topic-0000037-EqpJAsk-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.40:9092 (id: 0 rack: null)], epoch=0}}.
09:47:18.809 [pool-14-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-013-cUISDQs-5, groupId=sub-013-cUISDQs] Resetting offset for partition test-topic-0000013-Dlw5Sh4-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.40:9092 (id: 0 rack: null)], epoch=0}}.
09:47:18.809 [pool-16-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-019-TVYlklQ-7, groupId=sub-019-TVYlklQ] Resetting offset for partition test-topic-0000019-ZVQUIRs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.162:9092 (id: 1 rack: null)], epoch=0}}.
09:47:18.809 [pool-12-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-007-dAdh6Jk-3, groupId=sub-007-dAdh6Jk] Resetting offset for partition test-topic-0000007-yYH8VWg-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.122:9092 (id: 2 rack: null)], epoch=0}}.
09:47:18.809 [pool-18-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-025-2CjJ6bE-9, groupId=sub-025-2CjJ6bE] Resetting offset for partition test-topic-0000025-rG-WKrY-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.162:9092 (id: 1 rack: null)], epoch=0}}.
09:47:18.809 [pool-26-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-049-Czrc7Rg-17, groupId=sub-049-Czrc7Rg] Resetting offset for partition test-topic-0000049-1BQUeek-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.162:9092 (id: 1 rack: null)], epoch=0}}.
09:47:18.809 [pool-17-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-022-1m8YQwM-8, groupId=sub-022-1m8YQwM] Resetting offset for partition test-topic-0000022-wCQ2bmI-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.40:9092 (id: 0 rack: null)], epoch=0}}.
09:47:18.809 [pool-11-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-004-xN56quE-2, groupId=sub-004-xN56quE] Resetting offset for partition test-topic-0000004-usTouY0-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.162:9092 (id: 1 rack: null)], epoch=0}}.
09:47:18.809 [pool-27-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-052-QKzaf_k-18, groupId=sub-052-QKzaf_k] Resetting offset for partition test-topic-0000052-VtHfpgI-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.122:9092 (id: 2 rack: null)], epoch=0}}.
09:47:18.810 [pool-15-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-016-BHkZggo-6, groupId=sub-016-BHkZggo] Resetting offset for partition test-topic-0000016-hqYF7mM-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.40:9092 (id: 0 rack: null)], epoch=0}}.
09:47:18.810 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-pHRQrs0-12, groupId=sub-034-pHRQrs0] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-034-pHRQrs0-12-2dee4590-1edd-41c3-bdc8-80d6884bc5cd', protocol='range'}
09:47:18.810 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-pHRQrs0-12, groupId=sub-034-pHRQrs0] Finished assignment for group at generation 1: {consumer-sub-034-pHRQrs0-12-2dee4590-1edd-41c3-bdc8-80d6884bc5cd=Assignment(partitions=[test-topic-0000034-M4nrWXk-0])}
09:47:18.812 [pool-19-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-028-5tQIPxo-10, groupId=sub-028-5tQIPxo] Resetting offset for partition test-topic-0000028-4bCG-f8-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.122:9092 (id: 2 rack: null)], epoch=0}}.
09:47:18.812 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-N0T6Bp4-11, groupId=sub-031-N0T6Bp4] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-031-N0T6Bp4-11-521e31cd-0cd1-462e-962c-3bd01ea05f83', protocol='range'}
09:47:18.813 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-N0T6Bp4-11, groupId=sub-031-N0T6Bp4] Notifying assignor about the new Assignment(partitions=[test-topic-0000031-QijEomI-0])
09:47:18.813 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-N0T6Bp4-11, groupId=sub-031-N0T6Bp4] Adding newly assigned partitions: test-topic-0000031-QijEomI-0
09:47:18.814 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-pHRQrs0-12, groupId=sub-034-pHRQrs0] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-034-pHRQrs0-12-2dee4590-1edd-41c3-bdc8-80d6884bc5cd', protocol='range'}
09:47:18.814 [pool-20-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-031-N0T6Bp4-11, groupId=sub-031-N0T6Bp4] Found no committed offset for partition test-topic-0000031-QijEomI-0
09:47:18.814 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-pHRQrs0-12, groupId=sub-034-pHRQrs0] Notifying assignor about the new Assignment(partitions=[test-topic-0000034-M4nrWXk-0])
09:47:18.814 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-pHRQrs0-12, groupId=sub-034-pHRQrs0] Adding newly assigned partitions: test-topic-0000034-M4nrWXk-0
09:47:18.815 [pool-21-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-034-pHRQrs0-12, groupId=sub-034-pHRQrs0] Found no committed offset for partition test-topic-0000034-M4nrWXk-0
09:47:18.816 [pool-20-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-031-N0T6Bp4-11, groupId=sub-031-N0T6Bp4] Resetting offset for partition test-topic-0000031-QijEomI-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.40:9092 (id: 0 rack: null)], epoch=0}}.
09:47:18.821 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040-Uts6bzM-14, groupId=sub-040-Uts6bzM] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-040-Uts6bzM-14-295c31a2-8d70-46f8-9fa2-2b8efb80abaa', protocol='range'}
09:47:18.821 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040-Uts6bzM-14, groupId=sub-040-Uts6bzM] Finished assignment for group at generation 1: {consumer-sub-040-Uts6bzM-14-295c31a2-8d70-46f8-9fa2-2b8efb80abaa=Assignment(partitions=[test-topic-0000040-J1sFQIs-0])}
09:47:18.825 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040-Uts6bzM-14, groupId=sub-040-Uts6bzM] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-040-Uts6bzM-14-295c31a2-8d70-46f8-9fa2-2b8efb80abaa', protocol='range'}
09:47:18.825 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-6i3GEpQ-15, groupId=sub-043-6i3GEpQ] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-043-6i3GEpQ-15-dd9694ad-ed89-4d8c-a014-415f444e1c96', protocol='range'}
09:47:18.825 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040-Uts6bzM-14, groupId=sub-040-Uts6bzM] Notifying assignor about the new Assignment(partitions=[test-topic-0000040-J1sFQIs-0])
09:47:18.825 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040-Uts6bzM-14, groupId=sub-040-Uts6bzM] Adding newly assigned partitions: test-topic-0000040-J1sFQIs-0
09:47:18.825 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-6i3GEpQ-15, groupId=sub-043-6i3GEpQ] Finished assignment for group at generation 1: {consumer-sub-043-6i3GEpQ-15-dd9694ad-ed89-4d8c-a014-415f444e1c96=Assignment(partitions=[test-topic-0000043-4BDcIic-0])}
09:47:18.826 [pool-23-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-040-Uts6bzM-14, groupId=sub-040-Uts6bzM] Found no committed offset for partition test-topic-0000040-J1sFQIs-0
09:47:18.826 [pool-21-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-034-pHRQrs0-12, groupId=sub-034-pHRQrs0] Resetting offset for partition test-topic-0000034-M4nrWXk-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.40:9092 (id: 0 rack: null)], epoch=0}}.
09:47:18.829 [pool-23-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-040-Uts6bzM-14, groupId=sub-040-Uts6bzM] Resetting offset for partition test-topic-0000040-J1sFQIs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.40:9092 (id: 0 rack: null)], epoch=0}}.
09:47:18.830 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-6i3GEpQ-15, groupId=sub-043-6i3GEpQ] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-043-6i3GEpQ-15-dd9694ad-ed89-4d8c-a014-415f444e1c96', protocol='range'}
09:47:18.830 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-6i3GEpQ-15, groupId=sub-043-6i3GEpQ] Notifying assignor about the new Assignment(partitions=[test-topic-0000043-4BDcIic-0])
09:47:18.830 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-6i3GEpQ-15, groupId=sub-043-6i3GEpQ] Adding newly assigned partitions: test-topic-0000043-4BDcIic-0
09:47:18.831 [pool-24-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-043-6i3GEpQ-15, groupId=sub-043-6i3GEpQ] Found no committed offset for partition test-topic-0000043-4BDcIic-0
09:47:18.833 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-046-PlCtiIk-16-10c96e96-3e01-407d-81cf-9f2eea724e11', protocol='range'}
09:47:18.834 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Finished assignment for group at generation 1: {consumer-sub-046-PlCtiIk-16-10c96e96-3e01-407d-81cf-9f2eea724e11=Assignment(partitions=[test-topic-0000046-RG8GLes-0])}
09:47:18.836 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-001-NP6cX5I-1-a1aa6ee7-7f7b-4a47-bc39-2b3022f99ae2', protocol='range'}
09:47:18.836 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Finished assignment for group at generation 1: {consumer-sub-001-NP6cX5I-1-a1aa6ee7-7f7b-4a47-bc39-2b3022f99ae2=Assignment(partitions=[test-topic-0000001-a9CcqOE-0])}
09:47:18.848 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-046-PlCtiIk-16-10c96e96-3e01-407d-81cf-9f2eea724e11', protocol='range'}
09:47:18.849 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Notifying assignor about the new Assignment(partitions=[test-topic-0000046-RG8GLes-0])
09:47:18.849 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Adding newly assigned partitions: test-topic-0000046-RG8GLes-0
09:47:18.849 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-001-NP6cX5I-1-a1aa6ee7-7f7b-4a47-bc39-2b3022f99ae2', protocol='range'}
09:47:18.849 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Notifying assignor about the new Assignment(partitions=[test-topic-0000001-a9CcqOE-0])
09:47:18.849 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Adding newly assigned partitions: test-topic-0000001-a9CcqOE-0
09:47:18.850 [pool-25-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Found no committed offset for partition test-topic-0000046-RG8GLes-0
09:47:18.850 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Found no committed offset for partition test-topic-0000001-a9CcqOE-0
09:47:18.852 [pool-24-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-043-6i3GEpQ-15, groupId=sub-043-6i3GEpQ] Resetting offset for partition test-topic-0000043-4BDcIic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.40:9092 (id: 0 rack: null)], epoch=0}}.
09:47:18.854 [pool-25-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-046-PlCtiIk-16, groupId=sub-046-PlCtiIk] Resetting offset for partition test-topic-0000046-RG8GLes-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.122:9092 (id: 2 rack: null)], epoch=0}}.
09:47:18.855 [pool-10-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-001-NP6cX5I-1, groupId=sub-001-NP6cX5I] Resetting offset for partition test-topic-0000001-a9CcqOE-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.40:9092 (id: 0 rack: null)], epoch=0}}.
09:47:18.863 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:18.863 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:18.904 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_7.csv is false. -1
09:47:18.904 [pool-7-thread-1] INFO S3Client - Interrupting processor iteration. 1
09:47:19.004 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_7.csv is false. -1
09:47:19.004 [pool-7-thread-1] INFO S3Client - Interrupting processor iteration. 1
09:47:19.062 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000--4kasEA-1, groupId=sub-000--4kasEA] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sub-000--4kasEA-1-bed19782-6c84-4272-b8f5-e9f98fd3c33a', protocol='range'}
09:47:19.069 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000--4kasEA-1, groupId=sub-000--4kasEA] Finished assignment for group at generation 1: {consumer-sub-000--4kasEA-1-bed19782-6c84-4272-b8f5-e9f98fd3c33a=Assignment(partitions=[test-topic-0000000-8Eex934-0])}
09:47:19.078 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.078 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.078 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.078 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000--4kasEA-1, groupId=sub-000--4kasEA] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sub-000--4kasEA-1-bed19782-6c84-4272-b8f5-e9f98fd3c33a', protocol='range'}
09:47:19.079 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000--4kasEA-1, groupId=sub-000--4kasEA] Notifying assignor about the new Assignment(partitions=[test-topic-0000000-8Eex934-0])
09:47:19.081 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000--4kasEA-1, groupId=sub-000--4kasEA] Adding newly assigned partitions: test-topic-0000000-8Eex934-0
09:47:19.081 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.082 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.082 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.085 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.086 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.086 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.088 [pool-10-thread-1] INFO ConsumerCoordinator - [Consumer clientId=consumer-sub-000--4kasEA-1, groupId=sub-000--4kasEA] Found no committed offset for partition test-topic-0000000-8Eex934-0
09:47:19.088 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.089 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.089 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.091 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.092 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.092 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.094 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.095 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.095 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.097 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.097 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.098 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.099 [pool-10-thread-1] INFO SubscriptionState - [Consumer clientId=consumer-sub-000--4kasEA-1, groupId=sub-000--4kasEA] Resetting offset for partition test-topic-0000000-8Eex934-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.0.0.162:9092 (id: 1 rack: null)], epoch=0}}.
09:47:19.099 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.100 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.100 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.102 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.102 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.102 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.105 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.105 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_7.csv is false. -1
09:47:19.105 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_31.csv is false. -1
09:47:19.105 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_32.csv is false. -1
09:47:19.105 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_25.csv is false. -1
09:47:19.105 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_26.csv is false. -1
09:47:19.105 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_27.csv is false. -1
09:47:19.105 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_28.csv is false. -1
09:47:19.105 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_19.csv is false. -1
09:47:19.105 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:19.105 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:19.105 [pool-7-thread-1] INFO S3Client - Interrupting processor iteration. 10
09:47:19.105 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.105 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.107 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.107 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.107 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.109 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.109 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.109 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.111 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.111 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.111 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.113 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.113 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.113 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.115 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.116 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.116 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.118 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.118 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.118 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.120 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.120 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.120 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.122 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.122 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.122 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.124 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.124 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.124 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.127 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.127 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.135 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.135 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.140 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.140 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.140 [ForkJoinPool.commonPool-worker-9] INFO LocalWorker - Finishing task.
09:47:19.142 [ForkJoinPool.commonPool-worker-9] INFO LocalWorker - Finishing task.
09:47:19.142 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.142 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.143 [ForkJoinPool.commonPool-worker-3] INFO LocalWorker - Finishing task.
09:47:19.143 [ForkJoinPool.commonPool-worker-9] INFO LocalWorker - Finishing task.
09:47:19.144 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.144 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.145 [ForkJoinPool.commonPool-worker-7] INFO LocalWorker - Finishing task.
09:47:19.146 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.146 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.147 [ForkJoinPool.commonPool-worker-7] INFO LocalWorker - Finishing task.
09:47:19.148 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.148 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.150 [ForkJoinPool.commonPool-worker-7] INFO LocalWorker - Finishing task.
09:47:19.150 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.150 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.152 [ForkJoinPool.commonPool-worker-7] INFO LocalWorker - Finishing task.
09:47:19.152 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.152 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.154 [ForkJoinPool.commonPool-worker-7] INFO LocalWorker - Finishing task.
09:47:19.155 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.155 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.156 [ForkJoinPool.commonPool-worker-5] INFO LocalWorker - Finishing task.
09:47:19.157 [ForkJoinPool.commonPool-worker-11] INFO LocalWorker - Finishing task.
09:47:19.157 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.157 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.159 [ForkJoinPool.commonPool-worker-11] INFO LocalWorker - Finishing task.
09:47:19.160 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.160 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.162 [ForkJoinPool.commonPool-worker-11] INFO LocalWorker - Finishing task.
09:47:19.162 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.162 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.164 [ForkJoinPool.commonPool-worker-11] INFO LocalWorker - Finishing task.
09:47:19.164 [pool-10-thread-1] INFO LocalWorker - Starting new task.
09:47:19.164 [pool-10-thread-1] INFO LocalWorker - Started new task!
09:47:19.166 [ForkJoinPool.commonPool-worker-11] INFO LocalWorker - Finishing task.
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_7.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_31.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_32.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_25.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_26.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_27.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_28.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_19.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:19.205 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:19.206 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:19.206 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:19.206 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:19.206 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:19.206 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:19.206 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:19.206 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:19.206 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:19.206 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:19.206 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:19.206 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:19.206 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:19.206 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:19.206 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:19.206 [pool-7-thread-1] INFO S3Client - Interrupting processor iteration. 34
09:47:19.306 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_7.csv is false. -1
09:47:19.306 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_31.csv is false. -1
09:47:19.306 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_32.csv is false. -1
09:47:19.306 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_25.csv is false. -1
09:47:19.306 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_26.csv is false. -1
09:47:19.306 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_27.csv is false. -1
09:47:19.306 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_28.csv is false. -1
09:47:19.306 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_19.csv is false. -1
09:47:19.306 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:19.306 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:19.306 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:19.306 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:19.306 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:19.306 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:19.306 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:19.307 [pool-7-thread-1] INFO S3Client - Interrupting processor iteration. 34
09:47:19.408 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_7.csv is false. -1
09:47:19.408 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_31.csv is false. -1
09:47:19.408 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_32.csv is false. -1
09:47:19.408 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_25.csv is false. -1
09:47:19.408 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_26.csv is false. -1
09:47:19.408 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_27.csv is false. -1
09:47:19.408 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_28.csv is false. -1
09:47:19.408 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_19.csv is false. -1
09:47:19.408 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:19.408 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:19.408 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:19.408 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:19.408 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:19.408 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:19.408 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:19.409 [pool-7-thread-1] INFO S3Client - Interrupting processor iteration. 34
09:47:19.510 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_7.csv is false. -1
09:47:19.510 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_31.csv is false. -1
09:47:19.510 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_32.csv is false. -1
09:47:19.510 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_25.csv is false. -1
09:47:19.510 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_26.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_27.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_28.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_19.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:19.511 [pool-7-thread-1] INFO S3Client - Interrupting processor iteration. 34
09:47:19.611 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_7.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_31.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_32.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_25.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_26.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_27.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_28.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_19.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:19.617 [pool-7-thread-1] INFO S3Client - Interrupting processor iteration. 34
09:47:19.719 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_7.csv is false. -1
09:47:19.719 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_31.csv is false. -1
09:47:19.719 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_32.csv is false. -1
09:47:19.719 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_25.csv is false. -1
09:47:19.719 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_26.csv is false. -1
09:47:19.719 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_27.csv is false. -1
09:47:19.719 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_28.csv is false. -1
09:47:19.719 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_19.csv is false. -1
09:47:19.719 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:19.720 [pool-7-thread-1] INFO S3Client - Interrupting processor iteration. 34
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_7.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_31.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_32.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_25.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_26.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_27.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_28.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_19.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:19.821 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:19.822 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:19.822 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:19.822 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:19.822 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:19.822 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:19.822 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:19.822 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:19.822 [pool-7-thread-1] INFO S3Client - Interrupting processor iteration. 34
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_7.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_31.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_32.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_25.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_26.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_27.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_28.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_19.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:19.922 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:19.931 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:19.931 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:19.931 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:19.931 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:19.931 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:19.932 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:19.932 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:19.932 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:19.932 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:19.932 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:19.933 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:19.933 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:19.933 [pool-7-thread-1] INFO S3Client - Interrupting processor iteration. 34
09:47:20.037 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: ".30"
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: ".30"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:638) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:58) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:20.163 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_26.csv is false. -1
09:47:20.164 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: ".56"
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: ".56"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:638) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:58) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:20.271 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: "2-12-22"
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: "2-12-22"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:652) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:58) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:20.399 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:20.399 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:20.399 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:20.399 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:20.399 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:20.399 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:20.399 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:20.399 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:20.399 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:20.399 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:20.399 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:20.399 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:20.399 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:20.400 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:20.400 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:20.400 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:20.400 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:20.400 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:20.400 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:20.400 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:20.400 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:20.400 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:20.400 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:20.400 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:20.400 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:20.400 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:20.471 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:20.480 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:20.480 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:20.480 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:20.480 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:20.480 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:20.480 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:20.481 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:20.481 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:20.481 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:20.481 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:20.481 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:20.481 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:20.481 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:20.481 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:20.482 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:20.482 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:20.482 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:20.482 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:20.482 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:20.482 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:20.482 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:20.482 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:20.482 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:20.483 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:20.483 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:20.512 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:20.512 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:20.512 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:20.512 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:20.512 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:20.512 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:20.513 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:20.513 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:20.513 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:20.513 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:20.513 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:20.513 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:20.513 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:20.513 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:20.513 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:20.514 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:20.514 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:20.514 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:20.514 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:20.514 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:20.514 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:20.514 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:20.514 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:20.514 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:20.514 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:20.515 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:20.601 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:20.602 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:20.602 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:20.602 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:20.602 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:20.602 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:20.602 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:20.602 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:20.682 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:20.682 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:20.682 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:20.682 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:20.683 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:20.683 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:20.683 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:20.683 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:20.683 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:20.683 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:20.683 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:20.689 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:20.690 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:20.690 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:20.690 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:20.690 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:20.690 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:20.690 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:20.690 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:20.690 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:20.690 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:20.690 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:20.690 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:20.690 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:20.691 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:20.691 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:20.773 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:20.774 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:20.840 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:20.842 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:20.842 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:20.842 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:20.842 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:20.842 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:20.842 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:20.842 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:20.842 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:20.842 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:20.936 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:20.937 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:20.937 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:20.937 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:20.937 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:20.937 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:20.937 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:20.937 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:20.937 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:20.937 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:20.937 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:20.937 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:20.938 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:20.938 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:20.938 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:20.938 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:20.938 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:20.938 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:20.938 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:20.938 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:20.938 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:20.938 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:20.938 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:20.939 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:20.939 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:20.939 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:21.070 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:21.070 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:21.071 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:21.071 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:21.071 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:21.071 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:21.071 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:21.071 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:21.071 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:21.071 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:21.071 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:21.071 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:21.072 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:21.072 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:21.072 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:21.072 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:21.072 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:21.072 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:21.072 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:21.072 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:21.072 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:21.072 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:21.072 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:21.072 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:21.072 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:21.072 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:21.231 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_20.csv is false. -1
09:47:21.231 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_8.csv is false. -1
09:47:21.231 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:21.231 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:21.233 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:21.233 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:21.233 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:21.233 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:21.233 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:21.233 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:21.233 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:21.233 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:21.234 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:21.234 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:21.234 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:21.234 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:21.234 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:21.234 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:21.234 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:21.234 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:21.234 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:21.234 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:21.234 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:21.235 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:21.235 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:21.235 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:21.302 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_9.csv is false. -1
09:47:21.303 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_10.csv is false. -1
09:47:21.303 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_13.csv is false. -1
09:47:21.303 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_1.csv is false. -1
09:47:21.303 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:21.303 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:21.303 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:21.303 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:21.303 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:21.303 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:21.303 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_22.csv is false. -1
09:47:21.303 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_23.csv is false. -1
09:47:21.304 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:21.304 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:21.304 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:21.304 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:21.304 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:21.304 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:21.304 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:21.304 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:21.304 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:21.304 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_24.csv is false. -1
09:47:21.304 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_17.csv is false. -1
09:47:21.304 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_18.csv is false. -1
09:47:21.380 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: "0.01"
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: "0.01"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:652) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:59) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:21.850 [sdk-async-response-0-5] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272041850
09:47:21.908 [sdk-async-response-0-0] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272041908
09:47:22.038 [sdk-async-response-0-2] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272042038
09:47:22.263 [sdk-async-response-0-5] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272042263
09:47:22.263 [sdk-async-response-0-0] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272042263
09:47:22.350 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: "0.10"
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: "0.10"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:652) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:59) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:22.633 [sdk-async-response-0-4] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272042633
09:47:22.691 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_2.csv is false. -1
09:47:22.691 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:22.691 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:22.691 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:22.691 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:22.691 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:22.707 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:22.707 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:22.707 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_6.csv is false. -1
09:47:22.707 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_14.csv is false. -1
09:47:22.707 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_15.csv is false. -1
09:47:22.707 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_16.csv is false. -1
09:47:22.707 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_11.csv is false. -1
09:47:22.707 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_12.csv is false. -1
09:47:22.707 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_34.csv is false. -1
09:47:22.710 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: "8-05-31"
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: "8-05-31"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:652) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:58) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:22.901 [sdk-async-response-0-6] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272042901
09:47:22.924 [sdk-async-response-0-1] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272042924
09:47:23.127 [sdk-async-response-0-6] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272043126
09:47:23.205 [sdk-async-response-0-2] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272043205
09:47:23.293 [sdk-async-response-0-5] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272043293
09:47:23.378 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: "nusual requ"
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: "nusual requ"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:652) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:58) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:23.862 [sdk-async-response-0-5] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272043862
09:47:24.050 [sdk-async-response-0-6] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272044050
09:47:24.059 [sdk-async-response-0-2] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272044059
09:47:24.104 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:24.104 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:24.105 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:24.105 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:24.105 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:24.197 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:24.203 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:24.204 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: ".18"
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: ".18"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:638) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:58) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:24.464 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_3.csv is false. -1
09:47:24.466 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:24.466 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:24.466 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_33.csv is false. -1
09:47:24.466 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_21.csv is false. -1
09:47:24.569 [sdk-async-response-0-4] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272044569
09:47:24.614 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_4.csv is false. -1
09:47:24.615 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_5.csv is false. -1
09:47:24.619 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: "7-10-12"
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: "7-10-12"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:652) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:58) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:24.910 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_29.csv is false. -1
09:47:24.911 [pool-7-thread-1] INFO S3Client - Buffer s3://tpc-h-chunks/chunks-by-file-size/70mb/chunk_30.csv is false. -1
09:47:24.912 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: "0.10"
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: "0.10"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:652) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:59) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:25.687 [sdk-async-response-0-4] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272045687
09:47:25.731 [sdk-async-response-0-3] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272045731
09:47:25.750 [sdk-async-response-0-1] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272045750
09:47:25.757 [sdk-async-response-0-2] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272045757
09:47:25.757 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: "-10-15"
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: "-10-15"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:652) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:58) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:25.810 [sdk-async-response-0-6] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272045810
09:47:25.813 [sdk-async-response-0-0] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272045813
09:47:25.814 [sdk-async-response-0-7] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272045814
09:47:26.036 [sdk-async-response-0-1] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272046036
09:47:26.174 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: "0.06"
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: "0.06"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:652) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:59) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:26.563 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: "ep quietly "
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: "ep quietly "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:652) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:58) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:26.644 [sdk-async-response-0-5] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272046644
09:47:26.849 [sdk-async-response-0-3] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272046849
09:47:26.960 [sdk-async-response-0-1] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272046960
09:47:27.127 [sdk-async-response-0-0] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272047127
09:47:27.212 [sdk-async-response-0-1] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272047211
09:47:27.256 [sdk-async-response-0-7] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272047256
09:47:27.274 [sdk-async-response-0-4] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272047274
09:47:27.289 [sdk-async-response-0-5] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272047289
09:47:27.298 [sdk-async-response-0-3] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272047298
09:47:27.310 [sdk-async-response-0-2] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272047310
09:47:27.312 [sdk-async-response-0-6] INFO S3Client - Finished reading chunks, launching chunk processor... 1718272047312
09:47:27.320 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: "1995-01-30"
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: "1995-01-30"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:652) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:59) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:27.331 [main] INFO WorkloadGenerator - Pub rate    10.0 msg/s /  0.0 MB/s | Pub err     0.0 err/s | Cons rate    27.4 msg/s /  0.0 MB/s | Backlog:  0.0 K | Pub Latency (ms) avg:  8.4 - 50%:  8.2 - 99%: 13.8 - 99.9%: 14.0 - Max: 14.0 | Pub Delay Latency (us) avg: 5647.0 - 50%: 5557.0 - 99%: 11423.0 - 99.9%: 11665.0 - Max: 11665.0
09:47:27.643 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: ""
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:662) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:58) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:27.975 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: ""
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:662) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:58) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:28.345 [pool-7-thread-1] ERROR S3Client - Exception occurred!
java.lang.RuntimeException: Failed to parse row: For input string: "0.04"
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:76) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.readTpcHRowFromLine(TpcHDataParser.java:52) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at io.openmessaging.tpch.client.S3Client.lambda$startRowProcessor$0(S3Client.java:104) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.NumberFormatException: For input string: "0.04"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:652) ~[?:?]
	at java.lang.Integer.parseInt(Integer.java:770) ~[?:?]
	at io.openmessaging.tpch.algorithm.TpcHDataParser.parseCsvRow(TpcHDataParser.java:59) ~[io.openmessaging.benchmark-tpc-h-driver-0.0.1-SNAPSHOT.jar:?]
	... 8 more
09:47:37.585 [main] INFO WorkloadGenerator - Pub rate     0.0 msg/s /  0.0 MB/s | Pub err     0.0 err/s | Cons rate     0.0 msg/s /  0.0 MB/s | Backlog:  0.0 K | Pub Latency (ms) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0 | Pub Delay Latency (us) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0
09:47:47.800 [main] INFO WorkloadGenerator - Pub rate     0.0 msg/s /  0.0 MB/s | Pub err     0.0 err/s | Cons rate     0.0 msg/s /  0.0 MB/s | Backlog:  0.0 K | Pub Latency (ms) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0 | Pub Delay Latency (us) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0
09:47:58.029 [main] INFO WorkloadGenerator - Pub rate     0.0 msg/s /  0.0 MB/s | Pub err     0.0 err/s | Cons rate     0.0 msg/s /  0.0 MB/s | Backlog:  0.0 K | Pub Latency (ms) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0 | Pub Delay Latency (us) avg:  0.0 - 50%:  0.0 - 99%:  0.0 - 99.9%:  0.0 - Max:  0.0
